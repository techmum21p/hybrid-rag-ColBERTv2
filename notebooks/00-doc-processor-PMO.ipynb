{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ef7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Local RAG Chatbot with Image Understanding\\n===================================================\\n\\n‚úÖ No Cloud Dependencies (runs 100% locally)\\n‚úÖ No RAGatouille (direct Jina ColBERT v2 implementation)\\n‚úÖ PyMuPDF4LLM for PDF conversion\\n‚úÖ Image extraction and analysis with LLaVA vision model\\n‚úÖ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\\n‚úÖ Markdown-aware semantic chunking\\n‚úÖ SQLite database for storage\\n\\nRequirements:\\n- Ollama (for LLMs: llama3.2:3b, llava:7b)\\n- Mac Mini M4 or similar (16GB RAM recommended)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Local RAG Chatbot with Image Understanding\n",
    "===================================================\n",
    "\n",
    "‚úÖ No Cloud Dependencies (runs 100% locally)\n",
    "‚úÖ No RAGatouille (direct Jina ColBERT v2 implementation)\n",
    "‚úÖ PyMuPDF4LLM for PDF conversion\n",
    "‚úÖ Image extraction and analysis with LLaVA vision model\n",
    "‚úÖ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\n",
    "‚úÖ Markdown-aware semantic chunking\n",
    "‚úÖ SQLite database for storage\n",
    "\n",
    "Requirements:\n",
    "- Ollama (for LLMs: llama3.2:3b, llava:7b)\n",
    "- Mac Mini M4 or similar (16GB RAM recommended)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f723da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Suppress tokenizers parallelism warning when forking\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress deprecation warnings from transformers/sentence-transformers\n",
    "warnings.filterwarnings('ignore', message='.*torch_dtype.*deprecated.*')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage  # Renamed to avoid conflict with database model\n",
    "\n",
    "# PDF and text processing\n",
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF for image extraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Retrieval\n",
    "import bm25s\n",
    "from bm25s.hf import BM25HF\n",
    "import Stemmer  # PyStemmer for stemming\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "\n",
    "# LLM\n",
    "import requests  # For Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a1fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for local RAG system\"\"\"\n",
    "    # Base directory (set to project root - parent of notebooks folder)\n",
    "    base_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    # Database\n",
    "    db_path: str = None\n",
    "    \n",
    "    # Chunking - FIXED to use CHARACTER counts (more reliable than token counts)\n",
    "    # Optimized for small models (3B params) with limited context windows\n",
    "    min_chunk_size: int = 600   # Minimum 600 characters per chunk\n",
    "    max_chunk_size: int = 800   # Maximum 800 characters per chunk (HARD LIMIT)\n",
    "    chunk_overlap: int = 200    # 200 character overlap between chunks\n",
    "    \n",
    "    # Retrieval - ADAPTIVE strategy for better recall\n",
    "    bm25_top_k: int = 100       # BM25 initial candidates\n",
    "    colbert_top_k: int = 100    # ColBERT initial candidates\n",
    "    \n",
    "    # Adaptive top-k: Adjust based on query complexity\n",
    "    final_top_k_min: int = 5    # Minimum chunks (for simple queries)\n",
    "    final_top_k_max: int = 10   # Maximum chunks (for complex queries)\n",
    "    final_top_k_default: int = 7  # Default for most queries\n",
    "    \n",
    "    # Context window management\n",
    "    max_context_chars: int = 6000  # Max chars to send to LLM (7-8 chunks √ó 800)\n",
    "    \n",
    "    # Models\n",
    "    chat_model: str = \"llama3.2:3b\"\n",
    "    vision_model: str = \"gemma3:4b\"\n",
    "    embedding_model: str = \"jinaai/jina-colbert-v2\"\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_url: str = \"http://localhost:11434\"\n",
    "    ollama_timeout: int = 300  # Increased timeout for slower models\n",
    "    \n",
    "    # Paths (will be set to absolute paths in __post_init__)\n",
    "    bm25_index_path: str = None\n",
    "    colbert_index_path: str = None\n",
    "    images_dir: str = None\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set absolute paths after initialization\"\"\"\n",
    "        if self.db_path is None:\n",
    "            self.db_path = os.path.join(self.base_dir, \"rag_local.db\")\n",
    "        if self.bm25_index_path is None:\n",
    "            self.bm25_index_path = os.path.join(self.base_dir, \"indexes\", \"bm25s\")\n",
    "        if self.colbert_index_path is None:\n",
    "            self.colbert_index_path = os.path.join(self.base_dir, \"indexes\", \"colbert\")\n",
    "        if self.images_dir is None:\n",
    "            self.images_dir = os.path.join(self.base_dir, \"extracted_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    upload_date = Column(DateTime, default=datetime.utcnow)\n",
    "    total_pages = Column(Integer)\n",
    "    status = Column(String(50))\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = 'images'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    page_number = Column(Integer, nullable=False)\n",
    "    image_path = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    image_type = Column(String(50))\n",
    "    ocr_text = Column(Text)\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = 'chunks'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    text = Column(Text, nullable=False)\n",
    "    heading_path = Column(String(500))\n",
    "    token_count = Column(Integer)\n",
    "    has_images = Column(Boolean, default=False)\n",
    "    chunk_metadata = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA CLIENT WITH STREAMING SUPPORT\n",
    "# ============================================================================\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with Ollama API with streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.ollama_url\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        model: str, \n",
    "        prompt: str, \n",
    "        system: str = \"\",\n",
    "        images: List[str] = None,\n",
    "        timeout: int = 300,\n",
    "        stream: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text with Ollama (with optional streaming)\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                # Streaming mode - print tokens as they arrive\n",
    "                response = requests.post(url, json=payload, timeout=timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"response\" in chunk:\n",
    "                            token = chunk[\"response\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "                        \n",
    "                        # Check if done\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "                \n",
    "                print()  # Newline after streaming\n",
    "                return full_response\n",
    "            else:\n",
    "                # Non-streaming mode - wait for complete response\n",
    "                response = requests.post(url, json=payload, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                return result.get(\"response\", \"\")\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            return f\"Error: Request timed out after {timeout} seconds\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def analyze_image(self, image_path: str) -> Dict:\n",
    "        \"\"\"Analyze image using vision model\"\"\"\n",
    "        import base64\n",
    "        \n",
    "        # Read and encode image\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "        # Prompt for image analysis\n",
    "        prompt = \"\"\"Analyze this image and provide:\n",
    "1. A detailed description of what you see\n",
    "2. The type of image (diagram, chart, photo, screenshot, etc.)\n",
    "3. Any text visible in the image (OCR)\n",
    "\n",
    "Format your response as:\n",
    "DESCRIPTION: [your description]\n",
    "TYPE: [image type]\n",
    "TEXT: [any visible text]\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate(\n",
    "                model=self.config.vision_model,\n",
    "                prompt=prompt,\n",
    "                images=[image_data],\n",
    "                timeout=self.config.ollama_timeout\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            description = \"\"\n",
    "            image_type = \"unknown\"\n",
    "            ocr_text = \"\"\n",
    "            \n",
    "            lines = response.split('\\n')\n",
    "            current_section = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('DESCRIPTION:'):\n",
    "                    current_section = 'description'\n",
    "                    description = line.replace('DESCRIPTION:', '').strip()\n",
    "                elif line.startswith('TYPE:'):\n",
    "                    current_section = 'type'\n",
    "                    image_type = line.replace('TYPE:', '').strip()\n",
    "                elif line.startswith('TEXT:'):\n",
    "                    current_section = 'text'\n",
    "                    ocr_text = line.replace('TEXT:', '').strip()\n",
    "                elif current_section == 'description' and line:\n",
    "                    description += ' ' + line\n",
    "                elif current_section == 'text' and line:\n",
    "                    ocr_text += ' ' + line\n",
    "            \n",
    "            # FIXED: Return 'type' key instead of 'image_type' to match what DocumentProcessor expects\n",
    "            return {\n",
    "                'description': description.strip() or response[:200],  # Fallback to first 200 chars\n",
    "                'type': image_type.strip() or 'image',  # Changed from 'image_type' to 'type'\n",
    "                'ocr_text': ocr_text.strip()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è  Error analyzing image: {e}\")\n",
    "            return {\n",
    "                'description': 'Image analysis failed',\n",
    "                'type': 'unknown',  # Changed from 'image_type' to 'type'\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        context: str = None,\n",
    "        stream: bool = True  # Enable streaming by default!\n",
    "    ) -> str:\n",
    "        \"\"\"Chat with context - ULTRA-STRONG grounding to prevent hallucination\"\"\"\n",
    "        \n",
    "        # Use /api/chat endpoint for proper message handling\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "\n",
    "        # ULTRA-STRONG system message - Maximum grounding for small models\n",
    "        # UPDATED: Removed citation requirement for natural conversation flow\n",
    "        if context:\n",
    "            system_msg = f\"\"\"You are a helpful AI assistant that answers questions based on provided documents.\n",
    "\n",
    "üö´ ABSOLUTE RULES:\n",
    "- ONLY use information from the documents provided below\n",
    "- DO NOT use knowledge from your training data\n",
    "- DO NOT make assumptions beyond what's written\n",
    "- If the answer is not in the documents, say: \"I don't have that information in the provided documents.\"\n",
    "\n",
    "‚úÖ HOW TO ANSWER:\n",
    "- Read the documents carefully\n",
    "- Provide clear, direct answers\n",
    "- Use natural language (no need to cite \"Source 1\" etc.)\n",
    "- Be concise but complete\n",
    "\n",
    "üìÑ DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "Answer the user's question naturally and helpfully using only the information above.\"\"\"\n",
    "        else:\n",
    "            system_msg = \"You are a helpful assistant.\"\n",
    "\n",
    "        # CRITICAL FIX: When context is provided (RAG mode), only pass the current query\n",
    "        # This prevents the LLM from getting confused by previous answers based on different contexts\n",
    "        if context:\n",
    "            # For RAG queries: Only send the LATEST user question (not full conversation history)\n",
    "            # This prevents hallucination from mixing old context with new queries\n",
    "            chat_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": messages[-1][\"content\"]}  # Only current question!\n",
    "            ]\n",
    "        else:\n",
    "            # For non-RAG queries: Include full conversation history\n",
    "            chat_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg}\n",
    "            ]\n",
    "            for msg in messages:\n",
    "                chat_messages.append({\n",
    "                    \"role\": msg[\"role\"],\n",
    "                    \"content\": msg[\"content\"]\n",
    "                })\n",
    "\n",
    "        # Call Ollama chat API with STRONGER grounding parameters\n",
    "        payload = {\n",
    "            \"model\": self.config.chat_model,\n",
    "            \"messages\": chat_messages,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.0,  # ZERO temperature for maximum factuality!\n",
    "                \"top_p\": 0.8,  # Reduced for more focused responses\n",
    "                \"top_k\": 20,  # Limit vocabulary to most likely tokens\n",
    "                \"repeat_penalty\": 1.2,  # Increased to prevent repetition\n",
    "                \"num_ctx\": 4096  # Ensure enough context window\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if stream:\n",
    "                response = requests.post(url, json=payload, timeout=self.config.ollama_timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                            token = chunk[\"message\"][\"content\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "\n",
    "                print()\n",
    "                return full_response\n",
    "            else:\n",
    "                response = requests.post(url, json=payload, timeout=self.config.ollama_timeout)\n",
    "                response.raise_for_status()\n",
    "                return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            return f\"Error: Request timed out after {self.config.ollama_timeout} seconds\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75436919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKDOWN-AWARE SEMANTIC CHUNKER - REWRITTEN FOR PROPER SIZE ENFORCEMENT\n",
    "# ============================================================================\n",
    "\n",
    "class MarkdownSemanticChunker:\n",
    "    \"\"\"\n",
    "    Intelligent markdown chunking that STRICTLY respects size limits while maintaining hierarchy.\n",
    "    \n",
    "    Key improvements:\n",
    "    - Uses CHARACTER counts (not misleading token counts)\n",
    "    - HARD enforces max_chunk_size (no more 26K char chunks!)\n",
    "    - Respects markdown hierarchy when possible\n",
    "    - Splits at sentence boundaries for better semantic coherence\n",
    "    - Maintains overlap for context continuity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def chunk_markdown(self, markdown_text: str, doc_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Create semantically meaningful chunks with STRICT size enforcement\"\"\"\n",
    "        sections = self._parse_markdown_hierarchy(markdown_text)\n",
    "        chunks = self._create_chunks_from_sections(sections, doc_context)\n",
    "        return chunks\n",
    "    \n",
    "    def _parse_markdown_hierarchy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse markdown into hierarchical sections\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        heading_stack = []\n",
    "        \n",
    "        for line in lines:\n",
    "            heading_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            \n",
    "            if heading_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                level = len(heading_match.group(1))\n",
    "                title = heading_match.group(2).strip()\n",
    "                \n",
    "                heading_stack = [(lvl, ttl) for lvl, ttl in heading_stack if lvl < level]\n",
    "                heading_stack.append((level, title))\n",
    "                \n",
    "                parent_path = ' > '.join([ttl for _, ttl in heading_stack[:-1]])\n",
    "                full_path = ' > '.join([ttl for _, ttl in heading_stack])\n",
    "                \n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'content': '',\n",
    "                    'parent_path': parent_path,\n",
    "                    'full_path': full_path\n",
    "                }\n",
    "            else:\n",
    "                if current_section is not None:\n",
    "                    current_section['content'] += line + '\\n'\n",
    "                else:\n",
    "                    if not sections or sections[-1]['level'] != 0:\n",
    "                        sections.append({\n",
    "                            'level': 0,\n",
    "                            'title': 'Introduction',\n",
    "                            'content': line + '\\n',\n",
    "                            'parent_path': '',\n",
    "                            'full_path': 'Introduction'\n",
    "                        })\n",
    "                    else:\n",
    "                        sections[-1]['content'] += line + '\\n'\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_chunks_from_sections(self, sections: List[Dict], doc_context: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create chunks with HARD size limits while respecting markdown hierarchy.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Try to keep sections together if they fit\n",
    "        2. If section is too large, split at paragraph boundaries\n",
    "        3. If paragraph is too large, split at sentence boundaries\n",
    "        4. ALWAYS enforce max_chunk_size as HARD limit\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for section in sections:\n",
    "            section_chunks = self._process_section(section, doc_context)\n",
    "            chunks.extend(section_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_section(self, section: Dict, doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Process a single section, splitting if necessary\"\"\"\n",
    "        # Format section with heading\n",
    "        heading_text = self._format_heading(section)\n",
    "        content = section['content'].strip()\n",
    "        \n",
    "        # Calculate sizes\n",
    "        heading_size = len(heading_text)\n",
    "        content_size = len(content)\n",
    "        total_size = heading_size + content_size\n",
    "        \n",
    "        # Case 1: Entire section fits within max size\n",
    "        if total_size <= self.config.max_chunk_size:\n",
    "            return [{\n",
    "                'text': heading_text + content,\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': total_size,\n",
    "                'token_count': self._estimate_tokens(total_size),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'complete_section'\n",
    "            }]\n",
    "        \n",
    "        # Case 2: Section is too large - need to split\n",
    "        # Try splitting at paragraph boundaries first\n",
    "        paragraphs = re.split(r'\\n\\n+', content)\n",
    "        \n",
    "        if len(paragraphs) > 1:\n",
    "            return self._split_by_paragraphs(section, heading_text, paragraphs, doc_context)\n",
    "        else:\n",
    "            # Single large paragraph - split by sentences\n",
    "            return self._split_by_sentences(section, heading_text, content, doc_context)\n",
    "    \n",
    "    def _split_by_paragraphs(\n",
    "        self, \n",
    "        section: Dict, \n",
    "        heading_text: str, \n",
    "        paragraphs: List[str], \n",
    "        doc_context: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Split section by paragraphs, respecting max_chunk_size\"\"\"\n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = 1\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            para_size = len(para) + 2  # +2 for \\n\\n\n",
    "            \n",
    "            # Check if adding this paragraph would exceed max size\n",
    "            if current_size + para_size > self.config.max_chunk_size:\n",
    "                # Save current chunk if it has content beyond heading\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'split_section',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                # Check if paragraph itself is too large\n",
    "                if para_size > self.config.max_chunk_size - len(heading_text):\n",
    "                    # Paragraph is too large - split by sentences\n",
    "                    sentence_chunks = self._split_paragraph_by_sentences(\n",
    "                        section, heading_text, para, doc_context, part_num\n",
    "                    )\n",
    "                    chunks.extend(sentence_chunks)\n",
    "                    part_num += len(sentence_chunks)\n",
    "                    current_text = heading_text\n",
    "                    current_size = len(heading_text)\n",
    "                else:\n",
    "                    # Start new chunk with this paragraph\n",
    "                    current_text = heading_text + para + '\\n\\n'\n",
    "                    current_size = len(current_text)\n",
    "            else:\n",
    "                # Add paragraph to current chunk\n",
    "                current_text += para + '\\n\\n'\n",
    "                current_size += para_size\n",
    "        \n",
    "        # Add final chunk if it has content\n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'split_section',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_by_sentences(\n",
    "        self, \n",
    "        section: Dict, \n",
    "        heading_text: str, \n",
    "        content: str, \n",
    "        doc_context: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Split content by sentences when paragraphs are too large\"\"\"\n",
    "        # Simple sentence splitting (can be improved with nltk if needed)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "        \n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = 1\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            sentence_size = len(sentence) + 1  # +1 for space\n",
    "            \n",
    "            # Check if adding this sentence would exceed max size\n",
    "            if current_size + sentence_size > self.config.max_chunk_size:\n",
    "                # Save current chunk if it has content\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'sentence_split',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                # If sentence itself is too large, truncate it (last resort)\n",
    "                if sentence_size > self.config.max_chunk_size - len(heading_text):\n",
    "                    truncated = sentence[:self.config.max_chunk_size - len(heading_text) - 3] + \"...\"\n",
    "                    chunks.append({\n",
    "                        'text': heading_text + truncated,\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(heading_text + truncated),\n",
    "                        'token_count': self._estimate_tokens(len(heading_text + truncated)),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'truncated',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                    current_text = heading_text\n",
    "                    current_size = len(heading_text)\n",
    "                else:\n",
    "                    # Start new chunk with this sentence\n",
    "                    current_text = heading_text + sentence + ' '\n",
    "                    current_size = len(current_text)\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_text += sentence + ' '\n",
    "                current_size += sentence_size\n",
    "        \n",
    "        # Add final chunk if it has content\n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'sentence_split',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_paragraph_by_sentences(\n",
    "        self,\n",
    "        section: Dict,\n",
    "        heading_text: str,\n",
    "        paragraph: str,\n",
    "        doc_context: str,\n",
    "        start_part_num: int\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Helper to split a single large paragraph by sentences\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', paragraph)\n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = start_part_num\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            sentence_size = len(sentence) + 1\n",
    "            \n",
    "            if current_size + sentence_size > self.config.max_chunk_size:\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'sentence_split',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                current_text = heading_text + sentence + ' '\n",
    "                current_size = len(current_text)\n",
    "            else:\n",
    "                current_text += sentence + ' '\n",
    "                current_size += sentence_size\n",
    "        \n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'sentence_split',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _format_heading(self, section: Dict) -> str:\n",
    "        \"\"\"Format section heading with context\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if section['parent_path']:\n",
    "            parts.append(f\"[Context: {section['parent_path']}]\")\n",
    "        \n",
    "        if section['title'] and section['title'] != 'Introduction':\n",
    "            heading_prefix = '#' * section['level']\n",
    "            parts.append(f\"{heading_prefix} {section['title']}\")\n",
    "        \n",
    "        if parts:\n",
    "            return '\\n\\n'.join(parts) + '\\n\\n'\n",
    "        return ''\n",
    "    \n",
    "    def _estimate_tokens(self, char_count: int) -> int:\n",
    "        \"\"\"Estimate token count from character count (rough approximation)\"\"\"\n",
    "        # Rough estimate: 1 token ‚âà 4 characters for English text\n",
    "        return char_count // 4\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        DEPRECATED: Old method that was broken due to truncation.\n",
    "        Kept for compatibility but now just returns character count.\n",
    "        \"\"\"\n",
    "        return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR WITH IMAGE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF processing with image extraction and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.ollama = ollama_client\n",
    "        self.chunker = MarkdownSemanticChunker(config)\n",
    "        \n",
    "        # Create images directory\n",
    "        os.makedirs(config.images_dir, exist_ok=True)\n",
    "    \n",
    "    def _sanitize_utf8(self, text: str) -> str:\n",
    "        \"\"\"IMPROVED: Robust UTF-8 sanitization to prevent database corruption and LLM errors\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Remove invalid UTF-8 sequences\n",
    "            clean_text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Step 2: Remove null bytes which cause database issues\n",
    "            clean_text = clean_text.replace('\\x00', '')\n",
    "            \n",
    "            # Step 3: Remove problematic control characters (keep newlines, tabs, carriage returns)\n",
    "            clean_text = ''.join(\n",
    "                char for char in clean_text\n",
    "                if char in ['\\n', '\\t', '\\r'] or ord(char) >= 32\n",
    "            )\n",
    "            \n",
    "            # Step 4: Normalize whitespace (optional but helps with consistency)\n",
    "            # Replace multiple spaces with single space\n",
    "            import re\n",
    "            clean_text = re.sub(r' +', ' ', clean_text)\n",
    "            \n",
    "            return clean_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è  UTF-8 sanitization error: {e}\")\n",
    "            # Last resort: keep only printable ASCII\n",
    "            return ''.join(char for char in text if 32 <= ord(char) <= 126 or char in ['\\n', '\\t'])\n",
    "    \n",
    "    def pdf_to_markdown(self, pdf_path: str) -> str:\n",
    "        \"\"\"Convert PDF to Markdown using PyMuPDF4LLM\"\"\"\n",
    "        markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        # Sanitize to remove invalid UTF-8\n",
    "        return self._sanitize_utf8(markdown_text)\n",
    "    \n",
    "    def _group_nearby_rectangles(self, rects: List[fitz.Rect], proximity_threshold: float = 20) -> List[List[int]]:\n",
    "        \"\"\"Group rectangles that are close to each other\"\"\"\n",
    "        if not rects:\n",
    "            return []\n",
    "\n",
    "        # Each rect gets assigned to a group\n",
    "        groups = []\n",
    "        assigned = [False] * len(rects)\n",
    "\n",
    "        for i, rect in enumerate(rects):\n",
    "            if assigned[i]:\n",
    "                continue\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [i]\n",
    "            assigned[i] = True\n",
    "\n",
    "            # Find all rects that should be in this group\n",
    "            changed = True\n",
    "            while changed:\n",
    "                changed = False\n",
    "                for j, other_rect in enumerate(rects):\n",
    "                    if assigned[j]:\n",
    "                        continue\n",
    "\n",
    "                    # Check if this rect is close to any rect in current group\n",
    "                    for group_idx in current_group:\n",
    "                        group_rect = rects[group_idx]\n",
    "\n",
    "                        # Calculate distance between rectangles\n",
    "                        # Expand each rect by proximity_threshold and check for intersection\n",
    "                        expanded_group = fitz.Rect(\n",
    "                            group_rect.x0 - proximity_threshold,\n",
    "                            group_rect.y0 - proximity_threshold,\n",
    "                            group_rect.x1 + proximity_threshold,\n",
    "                            group_rect.y1 + proximity_threshold\n",
    "                        )\n",
    "\n",
    "                        if expanded_group.intersects(other_rect):\n",
    "                            current_group.append(j)\n",
    "                            assigned[j] = True\n",
    "                            changed = True\n",
    "                            break\n",
    "\n",
    "            groups.append(current_group)\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def extract_images_from_pdf(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        document_id: int,\n",
    "        min_image_size: int = 50,  # Minimum width/height in pixels\n",
    "        proximity_threshold: float = 20  # Group images within this distance (points)\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract images from PDF with intelligent grouping.\n",
    "        Groups nearby images together to capture complete diagrams.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            image_list = page.get_images(full=True)\n",
    "\n",
    "            if not image_list:\n",
    "                continue\n",
    "\n",
    "            # Get bounding boxes for all images on this page\n",
    "            image_bboxes = []\n",
    "            for img_info in image_list:\n",
    "                xref = img_info[0]\n",
    "                # Get all instances of this image on the page\n",
    "                rects = page.get_image_rects(xref)\n",
    "                if rects:\n",
    "                    for rect in rects:\n",
    "                        # Check minimum size\n",
    "                        width = rect.width\n",
    "                        height = rect.height\n",
    "                        if width >= min_image_size and height >= min_image_size:\n",
    "                            image_bboxes.append({\n",
    "                                'rect': rect,\n",
    "                                'xref': xref,\n",
    "                                'width': width,\n",
    "                                'height': height\n",
    "                            })\n",
    "\n",
    "            if not image_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Group nearby images\n",
    "            rects_only = [bbox['rect'] for bbox in image_bboxes]\n",
    "            groups = self._group_nearby_rectangles(rects_only, proximity_threshold)\n",
    "\n",
    "            # Process each group\n",
    "            for group_idx, group in enumerate(groups):\n",
    "                if len(group) == 1:\n",
    "                    # Single image - extract normally\n",
    "                    bbox = image_bboxes[group[0]]\n",
    "                    try:\n",
    "                        base_image = doc.extract_image(bbox['xref'])\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        pil_image = PILImage.open(io.BytesIO(image_bytes))\n",
    "\n",
    "                        # Save image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_img{len(images)+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        if pil_image.mode == 'RGBA':\n",
    "                            pil_image = pil_image.convert('RGB')\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': False,\n",
    "                            'bbox': bbox['rect']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Failed to extract single image on page {page_num+1}: {e}\")\n",
    "\n",
    "                else:\n",
    "                    # Multiple images grouped together - capture as screenshot\n",
    "                    # Calculate bounding box that encompasses all images in group\n",
    "                    union_rect = image_bboxes[group[0]]['rect']\n",
    "                    for idx in group[1:]:\n",
    "                        union_rect = union_rect | image_bboxes[idx]['rect']  # Union of rectangles\n",
    "\n",
    "                    # Add some padding\n",
    "                    padding = 5\n",
    "                    union_rect = fitz.Rect(\n",
    "                        max(0, union_rect.x0 - padding),\n",
    "                        max(0, union_rect.y0 - padding),\n",
    "                        min(page.rect.width, union_rect.x1 + padding),\n",
    "                        min(page.rect.height, union_rect.y1 + padding)\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        # Render this region as an image\n",
    "                        mat = fitz.Matrix(2, 2)  # 2x zoom for better quality\n",
    "                        pix = page.get_pixmap(matrix=mat, clip=union_rect)\n",
    "\n",
    "                        # Convert to PIL Image\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        pil_image = PILImage.open(io.BytesIO(img_data))\n",
    "\n",
    "                        # Save composite image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_composite{group_idx+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': True,\n",
    "                            'num_components': len(group),\n",
    "                            'bbox': union_rect\n",
    "                        })\n",
    "\n",
    "                        print(f\"    üìä Grouped {len(group)} images into composite on page {page_num+1}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Failed to create composite image on page {page_num+1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        return images\n",
    "    \n",
    "    def analyze_images(\n",
    "        self, \n",
    "        images: List[Dict],\n",
    "        document_id: int,\n",
    "        db_session\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Analyze images with vision model and save to database\"\"\"\n",
    "        image_ids = []\n",
    "        \n",
    "        for idx, img_info in enumerate(images):\n",
    "            print(f\"    Analyzing image {idx+1} on page {img_info['page_number']}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Analyze with vision model\n",
    "            analysis = self.ollama.analyze_image(img_info['image_path'])\n",
    "            \n",
    "            # Save to database with UTF-8 sanitization\n",
    "            image_record = Image(\n",
    "                document_id=document_id,\n",
    "                page_number=img_info['page_number'],\n",
    "                image_path=img_info['image_path'],\n",
    "                description=self._sanitize_utf8(analysis['description']),\n",
    "                image_type=self._sanitize_utf8(analysis['type']),\n",
    "                ocr_text=self._sanitize_utf8(analysis['ocr_text'])\n",
    "            )\n",
    "            db_session.add(image_record)\n",
    "            db_session.flush()\n",
    "            \n",
    "            image_ids.append(image_record.id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì ({elapsed:.1f}s)\")\n",
    "        \n",
    "        db_session.commit()\n",
    "        return image_ids\n",
    "    \n",
    "    def enrich_chunks_with_images(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        images_data: List[Dict],\n",
    "        db_session\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Add image context (description + OCR text) to relevant chunks for better search accuracy\"\"\"\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_copy = chunk.copy()\n",
    "            \n",
    "            # Find images that might be relevant to this chunk\n",
    "            # Simple heuristic: chunks that mention visual content keywords\n",
    "            relevant_images = []\n",
    "            \n",
    "            for img in images_data:\n",
    "                if any(keyword in chunk['text'].lower() for keyword in \n",
    "                       ['figure', 'image', 'diagram', 'chart', 'screenshot', 'see below', 'shown in']):\n",
    "                    relevant_images.append(img)\n",
    "            \n",
    "            if relevant_images:\n",
    "                # Build comprehensive image context including OCR text\n",
    "                image_context = \"\\n\\n[Images in this section]:\\n\"\n",
    "                image_metadata = []\n",
    "                \n",
    "                for img in relevant_images:\n",
    "                    # Add type and description\n",
    "                    image_context += f\"- {img['type'].capitalize()}: {img['description']}\\n\"\n",
    "                    \n",
    "                    # CRITICAL: Add OCR text if available (makes text in images searchable!)\n",
    "                    if img.get('ocr_text') and img['ocr_text'].strip():\n",
    "                        image_context += f\"  Text visible in image: {img['ocr_text']}\\n\"\n",
    "                    \n",
    "                    image_metadata.append({\n",
    "                        'path': img['image_path'],\n",
    "                        'description': img['description'],\n",
    "                        'type': img['type'],\n",
    "                        'ocr_text': img.get('ocr_text', '')\n",
    "                    })\n",
    "                \n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'] + image_context)\n",
    "                chunk_copy['has_images'] = True\n",
    "                chunk_copy['image_paths'] = [img['image_path'] for img in relevant_images]\n",
    "                chunk_copy['image_metadata'] = image_metadata\n",
    "            else:\n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'])\n",
    "                chunk_copy['has_images'] = False\n",
    "            \n",
    "            enriched_chunks.append(chunk_copy)\n",
    "        \n",
    "        return enriched_chunks\n",
    "    \n",
    "    def process_document(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        db_session\n",
    "    ) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Convert to markdown\n",
    "        print(\"\\n[Step 1/5] Converting PDF to Markdown...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        markdown_text = self.pdf_to_markdown(pdf_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Extracted {len(markdown_text):,} characters\")\n",
    "        \n",
    "        # Create document record\n",
    "        doc = Document(\n",
    "            filename=os.path.basename(pdf_path),\n",
    "            status='processing'\n",
    "        )\n",
    "        db_session.add(doc)\n",
    "        db_session.commit()\n",
    "        \n",
    "        # Step 2: Extract and analyze images\n",
    "        print(\"\\n[Step 2/5] Extracting and analyzing images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        images = self.extract_images_from_pdf(pdf_path, doc.id)\n",
    "        \n",
    "        if images:\n",
    "            image_ids = self.analyze_images(images, doc.id, db_session)\n",
    "            \n",
    "            # Get image data for enrichment\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_record = db_session.query(Image).filter_by(id=img_id).first()\n",
    "                if img_record:\n",
    "                    images_data.append({\n",
    "                        'image_path': img_record.image_path,\n",
    "                        'description': img_record.description,\n",
    "                        'type': img_record.image_type,\n",
    "                        'ocr_text': img_record.ocr_text\n",
    "                    })\n",
    "        else:\n",
    "            images_data = []\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ‚úì Completed in {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Extracted {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"  ‚Ä¢ Vision analysis: ‚úì\")\n",
    "        \n",
    "        # Step 3: Markdown-aware semantic chunking\n",
    "        print(\"\\n[Step 3/5] Markdown-aware semantic chunking...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        doc_context = f\"Document: {os.path.basename(pdf_path)}\\n\\n{markdown_text[:500]}\"\n",
    "        chunks = self.chunker.chunk_markdown(markdown_text, doc_context)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Created {len(chunks)} semantic chunks\")\n",
    "        \n",
    "        # Step 4: Enrich chunks with image context (INCLUDING OCR TEXT!)\n",
    "        print(\"\\n[Step 4/5] Enriching chunks with image context...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        if images_data:\n",
    "            chunks = self.enrich_chunks_with_images(chunks, images_data, db_session)\n",
    "            chunks_with_images = sum(1 for c in chunks if c.get('has_images', False))\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì {elapsed:.2f}s\")\n",
    "            print(f\"  ‚Ä¢ {chunks_with_images} chunks enriched with image context + OCR text\")\n",
    "        else:\n",
    "            # Still sanitize even if no images\n",
    "            for chunk in chunks:\n",
    "                chunk['text'] = self._sanitize_utf8(chunk['text'])\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì {elapsed:.2f}s\")\n",
    "            print(f\"  ‚Ä¢ No images to enrich\")\n",
    "        \n",
    "        # Step 5: Save to database\n",
    "        print(\"\\n[Step 5/5] Saving chunks to database...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_record = Chunk(\n",
    "                document_id=doc.id,\n",
    "                chunk_index=idx,\n",
    "                text=self._sanitize_utf8(chunk['text']),  # Sanitize before saving\n",
    "                heading_path=self._sanitize_utf8(chunk.get('heading_path', '')),  # Sanitize heading too\n",
    "                token_count=chunk.get('token_count', 0),\n",
    "                has_images=chunk.get('has_images', False),\n",
    "                chunk_metadata=self._sanitize_utf8(json.dumps({\n",
    "                    k: v for k, v in chunk.items() \n",
    "                    if k not in ['text', 'heading_path', 'token_count', 'has_images']\n",
    "                })) if chunk else ''\n",
    "            )\n",
    "            db_session.add(chunk_record)\n",
    "        \n",
    "        doc.status = 'indexed'\n",
    "        db_session.commit()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        \n",
    "        return chunks, doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08725b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JINA COLBERT V2 RETRIEVER (NO RAGATOUILLE!)\n",
    "# ============================================================================\n",
    "\n",
    "class JinaColBERTRetriever:\n",
    "    \"\"\"Direct implementation of Jina ColBERT v2 (no RAGatouille dependency)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = SentenceTransformer(\n",
    "            config.embedding_model,\n",
    "            trust_remote_code=True,\n",
    "            device=config.device\n",
    "        )\n",
    "        # Set max sequence length to avoid truncation warnings\n",
    "        self.model.max_seq_length = 512\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Index corpus with ColBERT embeddings\"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        print(f\"  Encoding {len(corpus)} documents...\")\n",
    "        \n",
    "        # Encode corpus (this gives us token-level embeddings)\n",
    "        # Truncate long sequences to avoid errors\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        os.makedirs(self.config.colbert_index_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'embeddings': self.corpus_embeddings,\n",
    "            'corpus': corpus\n",
    "        }, os.path.join(self.config.colbert_index_path, 'index.pt'))\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.config.colbert_index_path, 'index.pt')\n",
    "        data = torch.load(index_file, map_location=self.config.device)\n",
    "        self.corpus_embeddings = data['embeddings']\n",
    "        self.corpus = data['corpus']\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using MaxSim scoring\"\"\"\n",
    "        if not self.corpus or len(self.corpus) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Handle single item corpus\n",
    "        if len(self.corpus) == 1:\n",
    "            return [{\n",
    "                'document_id': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'text': self.corpus[0]\n",
    "            }]\n",
    "        \n",
    "        # Get top-k\n",
    "        k = min(k, len(scores))\n",
    "        top_k_indices = torch.topk(scores, k=k).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'document_id': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'text': self.corpus[idx] if self.corpus else None\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank documents with more accurate scoring\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Encode query and documents\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = self.model.encode(\n",
    "            documents, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, doc_embeddings)\n",
    "        \n",
    "        # Handle single document\n",
    "        if len(documents) == 1:\n",
    "            return [{\n",
    "                'result_index': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'rank': 1,\n",
    "                'text': documents[0]\n",
    "            }]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(sorted_indices[:k]):\n",
    "            results.append({\n",
    "                'result_index': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'rank': rank + 1,\n",
    "                'text': documents[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _maxsim_score(\n",
    "        self, \n",
    "        query_embedding: torch.Tensor, \n",
    "        doc_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MaxSim score between query and documents\n",
    "        \n",
    "        MaxSim: For each query token, find max similarity with all doc tokens,\n",
    "        then average across query tokens\n",
    "        \"\"\"\n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 1:\n",
    "            query_embedding = query_embedding.unsqueeze(0)\n",
    "        if doc_embeddings.dim() == 1:\n",
    "            doc_embeddings = doc_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # For 2D embeddings (single vector per doc), compute cosine similarity directly\n",
    "        if query_embedding.dim() == 2 and doc_embeddings.dim() == 2:\n",
    "            # Normalize embeddings\n",
    "            query_norm = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            doc_norm = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            scores = torch.mm(query_norm, doc_norm.t())\n",
    "            \n",
    "            # Return as 1D tensor\n",
    "            return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()\n",
    "        \n",
    "        # For 3D embeddings (token-level), use mean pooling\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_vec = query_embedding.mean(dim=1)\n",
    "        else:\n",
    "            query_vec = query_embedding\n",
    "            \n",
    "        if doc_embeddings.dim() == 3:\n",
    "            doc_vec = doc_embeddings.mean(dim=1)\n",
    "        else:\n",
    "            doc_vec = doc_embeddings\n",
    "        \n",
    "        # Normalize\n",
    "        query_vec = torch.nn.functional.normalize(query_vec, p=2, dim=-1)\n",
    "        doc_vec = torch.nn.functional.normalize(doc_vec, p=2, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        if query_vec.dim() == 1:\n",
    "            query_vec = query_vec.unsqueeze(0)\n",
    "        if doc_vec.dim() == 1:\n",
    "            doc_vec = doc_vec.unsqueeze(0)\n",
    "            \n",
    "        scores = torch.mm(query_vec, doc_vec.t())\n",
    "        \n",
    "        # Return as 1D tensor\n",
    "        return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUAL INDEXER (BM25s + Jina ColBERT)\n",
    "# ============================================================================\n",
    "\n",
    "class DualIndexer:\n",
    "    \"\"\"Manages BM25s and Jina ColBERT v2 indexes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.bm25_retriever = None\n",
    "        self.colbert_retriever = JinaColBERTRetriever(config)\n",
    "    \n",
    "    def build_bm25_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build BM25s index\"\"\"\n",
    "        print(\"\\n[BM25s] Building lexical search index...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create stemmer\n",
    "        stemmer = Stemmer.Stemmer(\"english\")\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        corpus_tokens = bm25s.tokenize(\n",
    "            corpus, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=stemmer\n",
    "        )\n",
    "        \n",
    "        self.bm25_retriever = bm25s.BM25()\n",
    "        self.bm25_retriever.index(corpus_tokens)\n",
    "        \n",
    "        os.makedirs(self.config.bm25_index_path, exist_ok=True)\n",
    "        self.bm25_retriever.save(self.config.bm25_index_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "    \n",
    "    def build_colbert_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build Jina ColBERT v2 index\"\"\"\n",
    "        print(\"\\n[ColBERT] Building semantic search index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.colbert_retriever.index(corpus)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ‚úì {elapsed:.2f}s\")\n",
    "    \n",
    "    def load_indexes(self) -> None:\n",
    "        \"\"\"Load indexes from disk\"\"\"\n",
    "        self.bm25_retriever = bm25s.BM25.load(self.config.bm25_index_path)\n",
    "        self.colbert_retriever.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID RETRIEVER WITH RRF AND RERANKING\n",
    "# ============================================================================\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Three-stage retrieval: BM25s + ColBERT + ColBERT Reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, indexer: DualIndexer, db_session, corpus_to_chunk_id: List[int] = None):\n",
    "        self.config = config\n",
    "        self.indexer = indexer\n",
    "        self.db_session = db_session\n",
    "        self.stemmer = Stemmer.Stemmer(\"english\")\n",
    "        # CRITICAL: Mapping from corpus index to database chunk ID\n",
    "        self.corpus_to_chunk_id = corpus_to_chunk_id or []\n",
    "    \n",
    "    def retrieve(self, query: str, top_k_final: int = None) -> List[Dict]:\n",
    "        \"\"\"Three-stage hybrid retrieval with detailed scoring\"\"\"\n",
    "        if top_k_final is None:\n",
    "            top_k_final = self.config.final_top_k\n",
    "        \n",
    "        print(f\"\\nüîç Retrieving relevant chunks...\")\n",
    "        \n",
    "        # Get corpus size to adjust k values\n",
    "        corpus_size = len(self.indexer.colbert_retriever.corpus) if self.indexer.colbert_retriever.corpus else 0\n",
    "        \n",
    "        # Adjust k values based on corpus size\n",
    "        bm25_k = min(self.config.bm25_top_k, corpus_size) if corpus_size > 0 else self.config.bm25_top_k\n",
    "        colbert_k = min(self.config.colbert_top_k, corpus_size) if corpus_size > 0 else self.config.colbert_top_k\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Corpus size: {corpus_size}, using k={bm25_k} for retrieval\")\n",
    "        \n",
    "        # Stage 1: BM25s\n",
    "        start = time.time()\n",
    "        bm25_results = self._bm25_search(query, k=bm25_k)\n",
    "        bm25_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ BM25s: {bm25_time:.3f}s ({len(bm25_results)} results)\")\n",
    "        \n",
    "        # Stage 2: ColBERT\n",
    "        start = time.time()\n",
    "        colbert_results = self._colbert_search(query, k=colbert_k)\n",
    "        colbert_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ ColBERT: {colbert_time:.3f}s ({len(colbert_results)} results)\")\n",
    "        \n",
    "        # Fusion\n",
    "        start = time.time()\n",
    "        fused_results = self._reciprocal_rank_fusion(bm25_results, colbert_results)\n",
    "        candidates = fused_results[:min(50, len(fused_results))]\n",
    "        fusion_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Fusion: {fusion_time:.3f}s ({len(candidates)} candidates)\")\n",
    "        \n",
    "        # Fetch chunks - USING THE MAPPING!\n",
    "        start = time.time()\n",
    "        candidate_corpus_indices = [r['corpus_index'] for r in candidates]\n",
    "        candidate_chunks = self._fetch_chunks_from_db(candidate_corpus_indices)\n",
    "        \n",
    "        # PRESERVE INTERMEDIATE SCORES\n",
    "        # Map corpus_index to intermediate scores\n",
    "        score_map = {}\n",
    "        for bm25_result in bm25_results:\n",
    "            idx = bm25_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['bm25_score'] = bm25_result['score']\n",
    "        \n",
    "        for colbert_result in colbert_results:\n",
    "            idx = colbert_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['colbert_score'] = colbert_result['score']\n",
    "        \n",
    "        for fused_result in candidates:\n",
    "            idx = fused_result['corpus_index']\n",
    "            if idx in score_map:\n",
    "                score_map[idx]['rrf_score'] = fused_result['rrf_score']\n",
    "        \n",
    "        # Add intermediate scores to chunks\n",
    "        for i, chunk in enumerate(candidate_chunks):\n",
    "            corpus_idx = candidate_corpus_indices[i]\n",
    "            if corpus_idx in score_map:\n",
    "                chunk['intermediate_scores'] = score_map[corpus_idx]\n",
    "        \n",
    "        fetch_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Fetch: {fetch_time:.3f}s ({len(candidate_chunks)} chunks)\")\n",
    "        \n",
    "        # Stage 3: Rerank\n",
    "        start = time.time()\n",
    "        final_k = min(top_k_final, len(candidate_chunks))\n",
    "        reranked_results = self._colbert_rerank(query, candidate_chunks, top_k=final_k)\n",
    "        rerank_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Rerank: {rerank_time:.3f}s (top {len(reranked_results)})\")\n",
    "        \n",
    "        total_time = bm25_time + colbert_time + fusion_time + fetch_time + rerank_time\n",
    "        print(f\"   ‚úì Total retrieval: {total_time:.3f}s\")\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 1: BM25s lexical search\"\"\"\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            query, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=self.stemmer\n",
    "        )\n",
    "        \n",
    "        results, scores = self.indexer.bm25_retriever.retrieve(query_tokens, k=k)\n",
    "        \n",
    "        return [\n",
    "            {'corpus_index': int(results[0][i]), 'score': float(scores[0][i]), 'source': 'bm25'}\n",
    "            for i in range(len(results[0]))\n",
    "        ]\n",
    "    \n",
    "    def _colbert_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 2: ColBERT semantic search\"\"\"\n",
    "        results = self.indexer.colbert_retriever.search(query=query, k=k)\n",
    "        return [\n",
    "            {'corpus_index': r['document_id'], 'score': r['score'], 'source': 'colbert'}\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def _reciprocal_rank_fusion(\n",
    "        self, \n",
    "        bm25_results: List[Dict], \n",
    "        colbert_results: List[Dict],\n",
    "        k: int = 60\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"RRF fusion\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for rank, result in enumerate(bm25_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        for rank, result in enumerate(colbert_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{'corpus_index': idx, 'rrf_score': score} for idx, score in sorted_results]\n",
    "    \n",
    "    def _fetch_chunks_from_db(self, corpus_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Fetch chunks from database using corpus index -> chunk ID mapping\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for corpus_idx in corpus_indices:\n",
    "            # Convert corpus index to database chunk ID\n",
    "            if corpus_idx < len(self.corpus_to_chunk_id):\n",
    "                chunk_id = self.corpus_to_chunk_id[corpus_idx]\n",
    "                \n",
    "                # Fetch from database using the actual chunk ID\n",
    "                chunk = self.db_session.query(Chunk).filter_by(id=chunk_id).first()\n",
    "                if chunk:\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk.id,\n",
    "                        'text': chunk.text,\n",
    "                        'document_id': chunk.document_id,\n",
    "                        'heading_path': chunk.heading_path,\n",
    "                        'has_images': chunk.has_images,\n",
    "                        'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Chunk ID {chunk_id} not found in database\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Corpus index {corpus_idx} out of range (max: {len(self.corpus_to_chunk_id)-1})\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _colbert_rerank(self, query: str, chunks: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 3: ColBERT reranking with score preservation\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        reranked_results = self.indexer.colbert_retriever.rerank(query=query, documents=documents, k=top_k)\n",
    "        \n",
    "        final_results = []\n",
    "        for result in reranked_results:\n",
    "            original_chunk = chunks[result['result_index']]\n",
    "            intermediate_scores = original_chunk.get('intermediate_scores', {})\n",
    "            \n",
    "            final_results.append({\n",
    "                'chunk_id': original_chunk['chunk_id'],\n",
    "                'text': original_chunk['text'],\n",
    "                'document_id': original_chunk['document_id'],\n",
    "                'heading_path': original_chunk.get('heading_path', ''),\n",
    "                'has_images': original_chunk.get('has_images', False),\n",
    "                'metadata': original_chunk['metadata'],\n",
    "                'score': result['score'],  # Final ColBERT rerank score (cosine similarity)\n",
    "                'rank': result['rank'],\n",
    "                'bm25_score': intermediate_scores.get('bm25_score', 0.0),\n",
    "                'colbert_score': intermediate_scores.get('colbert_score', 0.0),\n",
    "                'rrf_score': intermediate_scores.get('rrf_score', 0.0)\n",
    "            })\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAG CHATBOT WITH ADAPTIVE CHUNKING\n",
    "# ============================================================================\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Complete RAG chatbot with adaptive chunk selection\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, retriever: HybridRetriever, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.ollama = ollama_client\n",
    "        self.conversation_history = []\n",
    "        self.debug_mode = True  # Enable debugging to see what's being sent to LLM\n",
    "    \n",
    "    def _determine_top_k(self, query: str) -> int:\n",
    "        \"\"\"\n",
    "        Determine optimal number of chunks based on query complexity.\n",
    "        \n",
    "        Simple queries (e.g., \"What is X?\") ‚Üí fewer chunks (5)\n",
    "        Complex queries (e.g., \"List all...\", \"Compare...\") ‚Üí more chunks (10)\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Keywords indicating need for comprehensive answers\n",
    "        comprehensive_keywords = [\n",
    "            'all', 'list', 'different', 'various', 'types of', 'kinds of',\n",
    "            'compare', 'contrast', 'difference', 'similarities',\n",
    "            'explain', 'describe in detail', 'comprehensive',\n",
    "            'multiple', 'several', 'many'\n",
    "        ]\n",
    "        \n",
    "        # Check if query needs comprehensive answer\n",
    "        needs_comprehensive = any(keyword in query_lower for keyword in comprehensive_keywords)\n",
    "        \n",
    "        if needs_comprehensive:\n",
    "            return self.config.final_top_k_max  # 10 chunks for comprehensive answers\n",
    "        else:\n",
    "            return self.config.final_top_k_default  # 7 chunks for normal queries\n",
    "    \n",
    "    def chat(self, query: str, stream: bool = True, top_k: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process user query with adaptive chunk selection.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            stream: Enable streaming response\n",
    "            top_k: Override automatic top_k selection (optional)\n",
    "        \"\"\"\n",
    "        # Determine optimal number of chunks\n",
    "        if top_k is None:\n",
    "            top_k = self._determine_top_k(query)\n",
    "        \n",
    "        print(f\"\\nüí° Query complexity analysis: Using {top_k} chunks\")\n",
    "        \n",
    "        # Retrieve relevant chunks (use top_k_final parameter name)\n",
    "        retrieved_chunks = self.retriever.retrieve(query, top_k_final=top_k)\n",
    "        \n",
    "        # Build context with smart truncation\n",
    "        context, actual_chunks_used = self._build_context_adaptive(retrieved_chunks)\n",
    "        \n",
    "        # DEBUG: Show what's being sent to LLM\n",
    "        if self.debug_mode:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üêõ DEBUG: Context being sent to LLM\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Context length: {len(context)} characters\")\n",
    "            print(f\"Chunks retrieved: {len(retrieved_chunks)}\")\n",
    "            print(f\"Chunks actually used: {actual_chunks_used}\")\n",
    "            print(f\"\\nFirst 800 characters of context:\")\n",
    "            print(context[:800])\n",
    "            print(f\"\\n... [truncated, full context is {len(context)} chars]\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Clean display header\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí¨ Question: {query}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nü§ñ Answer:\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        })\n",
    "        \n",
    "        # Generate response with streaming\n",
    "        response = self.ollama.chat(\n",
    "            messages=self.conversation_history,\n",
    "            context=context,\n",
    "            stream=stream\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Clean footer with metadata\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"‚è±Ô∏è  {elapsed:.1f}s | üìö {actual_chunks_used} chunks | üìù {len(context)} chars | üéØ top_k={top_k}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': response\n",
    "        })\n",
    "        \n",
    "        # Display source information\n",
    "        print(f\"\\nüìñ Sources Used:\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        for i in range(actual_chunks_used):\n",
    "            chunk = retrieved_chunks[i]\n",
    "            heading = chunk.get('heading_path', 'No heading')\n",
    "            score = chunk.get('score', 0.0)\n",
    "            char_count = len(chunk.get('text', ''))\n",
    "            print(f\"  {i+1}. [{score:.4f}] {heading[:50]}... ({char_count} chars)\")\n",
    "        \n",
    "        if len(retrieved_chunks) > actual_chunks_used:\n",
    "            print(f\"\\n  ‚ö†Ô∏è  Note: {len(retrieved_chunks) - actual_chunks_used} additional chunks retrieved but not sent to LLM\")\n",
    "            print(f\"     (exceeded max_context_chars limit of {self.config.max_context_chars})\")\n",
    "        \n",
    "        print(f\"{'‚îÄ'*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': self._format_sources(retrieved_chunks[:actual_chunks_used]),\n",
    "            'retrieved_chunks': len(retrieved_chunks),\n",
    "            'used_chunks': actual_chunks_used,\n",
    "            'context_length': len(context),\n",
    "            'top_k': top_k\n",
    "        }\n",
    "    \n",
    "    def _build_context_adaptive(self, chunks: List[Dict]) -> tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Build context with adaptive truncation to stay within max_context_chars.\n",
    "        \n",
    "        Returns:\n",
    "            (context_string, number_of_chunks_used)\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        total_chars = 0\n",
    "        chunks_used = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            chunk_text = chunk['text']\n",
    "            \n",
    "            # Sanity check for old chunks\n",
    "            if len(chunk_text) > 1000:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Source {i} is {len(chunk_text)} chars (expected max 800)\")\n",
    "                print(f\"   This suggests you need to re-index with the new chunker!\")\n",
    "                chunk_text = chunk_text[:800] + \"...\"\n",
    "            \n",
    "            # Calculate what context size would be if we add this chunk\n",
    "            source_header = f\"=== SOURCE {i} ===\"\n",
    "            source_footer = f\"=== END SOURCE {i} ===\"\n",
    "            heading = chunk.get('heading_path', '')\n",
    "            \n",
    "            if heading:\n",
    "                chunk_formatted = f\"{source_header}\\nSection: {heading}\\n\\n{chunk_text}\\n{source_footer}\"\n",
    "            else:\n",
    "                chunk_formatted = f\"{source_header}\\n{chunk_text}\\n{source_footer}\"\n",
    "            \n",
    "            chunk_size = len(chunk_formatted) + 2  # +2 for \\n\\n separator\n",
    "            \n",
    "            # Check if adding this chunk would exceed max context\n",
    "            if total_chars + chunk_size > self.config.max_context_chars:\n",
    "                print(f\"\\n‚ö†Ô∏è  Stopping at {chunks_used} chunks (would exceed {self.config.max_context_chars} char limit)\")\n",
    "                break\n",
    "            \n",
    "            context_parts.append(chunk_formatted)\n",
    "            total_chars += chunk_size\n",
    "            chunks_used += 1\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts), chunks_used\n",
    "    \n",
    "    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format source citations with full text, image paths, and ALL scores\"\"\"\n",
    "        sources = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            source = {\n",
    "                'source_id': i + 1,\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'document_id': chunk['document_id'],\n",
    "                'heading': chunk.get('heading_path', ''),\n",
    "                'score': chunk['score'],  # Final ColBERT rerank score\n",
    "                'bm25_score': chunk.get('bm25_score', 0.0),\n",
    "                'colbert_score': chunk.get('colbert_score', 0.0),\n",
    "                'rrf_score': chunk.get('rrf_score', 0.0),\n",
    "                'has_images': chunk.get('has_images', False),\n",
    "                'text': chunk['text'],  # Include full text\n",
    "                'preview': chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "            }\n",
    "            \n",
    "            # Add image paths if available\n",
    "            if chunk.get('has_images') and chunk.get('metadata'):\n",
    "                image_paths = chunk['metadata'].get('image_paths', [])\n",
    "                source['image_paths'] = image_paths\n",
    "            \n",
    "            sources.append(source)\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"üóëÔ∏è  Conversation history cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    \"\"\"Main application orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Database setup\n",
    "        db_url = f\"sqlite:///{config.db_path}\"\n",
    "        self.engine = create_engine(db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.db_session = Session()\n",
    "        \n",
    "        # Initialize Ollama client\n",
    "        self.ollama = OllamaClient(config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.processor = DocumentProcessor(config, self.ollama)\n",
    "        self.indexer = DualIndexer(config)\n",
    "        self.retriever = None\n",
    "        self.chatbot = None\n",
    "        \n",
    "        # CRITICAL: Store mapping between corpus index and chunk IDs\n",
    "        self.corpus_to_chunk_id = []  # Maps corpus index -> database chunk ID\n",
    "    \n",
    "    def check_ollama(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.ollama_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def index_documents(self, pdf_paths: List[str]) -> None:\n",
    "        \"\"\"Index PDF documents\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"‚ùå Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            chunks, doc_id = self.processor.process_document(pdf_path, self.db_session)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Building Indexes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build corpus and mapping\n",
    "        # CRITICAL FIX: Store the mapping between corpus index and database chunk IDs\n",
    "        all_db_chunks = self.db_session.query(Chunk).order_by(Chunk.id).all()\n",
    "        corpus = []\n",
    "        self.corpus_to_chunk_id = []\n",
    "        \n",
    "        for chunk in all_db_chunks:\n",
    "            corpus.append(chunk.text)\n",
    "            self.corpus_to_chunk_id.append(chunk.id)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Corpus: {len(corpus)} chunks\")\n",
    "        print(f\"  ‚Ä¢ Chunk ID mapping: {len(self.corpus_to_chunk_id)} entries\")\n",
    "        \n",
    "        # Build indexes\n",
    "        self.indexer.build_bm25_index(corpus)\n",
    "        self.indexer.build_colbert_index(corpus)\n",
    "        \n",
    "        # Save the mapping to disk for later use\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        os.makedirs(os.path.dirname(mapping_path), exist_ok=True)\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(self.corpus_to_chunk_id, f)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Document indexed successfully!\")\n",
    "    \n",
    "    def initialize_chatbot(self) -> None:\n",
    "        \"\"\"Initialize chatbot with existing indexes\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"‚ùå Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading indexes...\")\n",
    "        self.indexer.load_indexes()\n",
    "        \n",
    "        # Load the corpus-to-chunk-id mapping\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        try:\n",
    "            with open(mapping_path, 'rb') as f:\n",
    "                self.corpus_to_chunk_id = pickle.load(f)\n",
    "            print(f\"  ‚Ä¢ Loaded {len(self.corpus_to_chunk_id)} chunk ID mappings\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"  ‚ö†Ô∏è  Warning: No corpus mapping found. Please re-index your documents.\")\n",
    "            self.corpus_to_chunk_id = []\n",
    "        \n",
    "        self.retriever = HybridRetriever(self.config, self.indexer, self.db_session, self.corpus_to_chunk_id)\n",
    "        self.chatbot = RAGChatbot(self.config, self.retriever, self.ollama)\n",
    "        \n",
    "        print(\"‚úÖ Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Chat interface\"\"\"\n",
    "        if not self.chatbot:\n",
    "            raise RuntimeError(\"Chatbot not initialized. Call initialize_chatbot() first.\")\n",
    "        \n",
    "        return self.chatbot.chat(query)\n",
    "    \n",
    "    def _filter_relevant_images(self, query: str, image_paths: List[str], chunk_text: str) -> List[str]:\n",
    "        \"\"\"Filter images to only show those DIRECTLY relevant to the user's query - STRICT filtering\"\"\"\n",
    "        if not image_paths:\n",
    "            return []\n",
    "        \n",
    "        relevant_images = []\n",
    "        \n",
    "        # Extract meaningful query keywords (remove stop words)\n",
    "        stop_words = {'what', 'is', 'are', 'the', 'a', 'an', 'how', 'why', 'when', 'where', \n",
    "                      'can', 'could', 'would', 'should', 'do', 'does', 'did', 'of', 'in', 'on',\n",
    "                      'for', 'to', 'with', 'by', 'from', 'at', 'about', 'as', 'into', 'through',\n",
    "                      'diagram', 'chart', 'figure', 'image', 'screenshot', 'show', 'me', 'please'}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        query_words = [w for w in query_lower.split() if w not in stop_words and len(w) > 2]\n",
    "        \n",
    "        if not query_words:\n",
    "            return []  # No meaningful query words, don't show images\n",
    "        \n",
    "        # Get image metadata from database\n",
    "        for img_path in image_paths:\n",
    "            # Extract just the filename for DB lookup\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # Look up image in database to get description\n",
    "            img_record = self.db_session.query(Image).filter(\n",
    "                Image.image_path.like(f\"%{img_filename}\")\n",
    "            ).first()\n",
    "            \n",
    "            if img_record:\n",
    "                # Combine all image metadata\n",
    "                desc_lower = (img_record.description or \"\").lower()\n",
    "                img_type_lower = (img_record.image_type or \"\").lower()\n",
    "                ocr_lower = (img_record.ocr_text or \"\").lower()\n",
    "                \n",
    "                # Create searchable text from image\n",
    "                image_text = f\"{desc_lower} {img_type_lower} {ocr_lower}\"\n",
    "                image_words = [w for w in image_text.split() if w not in stop_words and len(w) > 2]\n",
    "                \n",
    "                # Calculate meaningful overlap\n",
    "                query_set = set(query_words)\n",
    "                image_set = set(image_words)\n",
    "                overlap = query_set.intersection(image_set)\n",
    "                \n",
    "                # STRICT CRITERIA: Need at least 3 meaningful word overlaps\n",
    "                # This ensures the image is actually about what the user asked\n",
    "                if len(overlap) >= 3:\n",
    "                    relevant_images.append(img_path)\n",
    "                    # print(f\"  DEBUG: Image matched with {len(overlap)} overlaps: {overlap}\")\n",
    "        \n",
    "        return relevant_images\n",
    "    \n",
    "    def _display_chunk_with_images(self, chunk_text: str, image_paths: List[str] = None) -> None:\n",
    "        \"\"\"Display chunk text and associated images\"\"\"\n",
    "        from IPython.display import display, Image as IPImage\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"{chunk_text}\\n\")\n",
    "        \n",
    "        # Display images if available\n",
    "        if image_paths:\n",
    "            print(f\"  üì∑ Relevant Images ({len(image_paths)}):\")\n",
    "            for img_path in image_paths:\n",
    "                if os.path.exists(img_path):\n",
    "                    try:\n",
    "                        display(IPImage(filename=img_path, width=400))\n",
    "                        print(f\"  ‚îî‚îÄ {os.path.basename(img_path)}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚îî‚îÄ ‚ö†Ô∏è Could not display {os.path.basename(img_path)}: {e}\\n\")\n",
    "                else:\n",
    "                    print(f\"  ‚îî‚îÄ ‚ö†Ô∏è Image not found: {os.path.basename(img_path)}\\n\")\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAG Chatbot - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Type your questions (or 'exit' to quit, 'clear' to clear history)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"\\nGoodbye! üëã\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'clear':\n",
    "                    self.chatbot.clear_history()\n",
    "                    continue\n",
    "                \n",
    "                result = self.chat(user_input)\n",
    "                print(f\"\\nAssistant: {result['response']}\\n\")\n",
    "                \n",
    "                # Show retrieved chunks with ALL SCORES\n",
    "                if result['sources']:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"üìä Retrieved Chunks with Similarity Scores ({len(result['sources'])})\")\n",
    "                    print(f\"{'='*60}\\n\")\n",
    "                    \n",
    "                    for idx, src in enumerate(result['sources'], 1):\n",
    "                        print(f\"‚îå‚îÄ Chunk {idx} {'‚îÄ'*50}\")\n",
    "                        \n",
    "                        # Show ALL retrieval scores\n",
    "                        print(f\"‚îÇ üéØ Final Score (ColBERT Rerank): {src['score']:.4f}\")\n",
    "                        print(f\"‚îÇ üìà Intermediate Scores:\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ BM25 (lexical):      {src.get('bm25_score', 0.0):.4f}\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ ColBERT (semantic):  {src.get('colbert_score', 0.0):.4f}\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ RRF (fusion):        {src.get('rrf_score', 0.0):.4f}\")\n",
    "                        \n",
    "                        if src['heading']:\n",
    "                            print(f\"‚îÇ üìç Section: {src['heading']}\")\n",
    "                        \n",
    "                        if src['has_images']:\n",
    "                            print(f\"‚îÇ üñºÔ∏è  Contains Images: Yes\")\n",
    "                        \n",
    "                        print(f\"‚îÇ\")\n",
    "                        print(f\"‚îÇ üìÑ Text:\")\n",
    "                        \n",
    "                        # Display chunk text (show first 300 chars as preview)\n",
    "                        chunk_text = src.get('text', src.get('preview', ''))\n",
    "                        \n",
    "                        # Show preview\n",
    "                        if len(chunk_text) > 300:\n",
    "                            print(f\"‚îÇ {chunk_text[:300]}...\")\n",
    "                            print(f\"‚îÇ [Truncated - {len(chunk_text)} total characters]\")\n",
    "                        else:\n",
    "                            print(f\"‚îÇ {chunk_text}\")\n",
    "                        \n",
    "                        # Filter and display only STRICTLY RELEVANT images\n",
    "                        if src['has_images'] and src.get('image_paths'):\n",
    "                            # Filter images based on query relevance with STRICT criteria\n",
    "                            relevant_images = self._filter_relevant_images(\n",
    "                                user_input, \n",
    "                                src['image_paths'], \n",
    "                                chunk_text\n",
    "                            )\n",
    "                            \n",
    "                            if relevant_images:\n",
    "                                print(f\"‚îÇ\")\n",
    "                                print(f\"‚îÇ [Showing {len(relevant_images)}/{len(src['image_paths'])} images matching your query]\")\n",
    "                                self._display_chunk_with_images(\"\", relevant_images)\n",
    "                            else:\n",
    "                                print(f\"‚îÇ\")\n",
    "                                print(f\"‚îÇ [This chunk has images, but none directly match your specific query]\")\n",
    "                        \n",
    "                        print(f\"‚îî{'‚îÄ'*60}\\n\")\n",
    "                    \n",
    "                    print()\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! üëã\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"Print database statistics\"\"\"\n",
    "        doc_count = self.db_session.query(Document).count()\n",
    "        chunk_count = self.db_session.query(Chunk).count()\n",
    "        image_count = self.db_session.query(Image).count()\n",
    "        \n",
    "        print(f\"\\nüìä Database Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Documents: {doc_count}\")\n",
    "        print(f\"   ‚Ä¢ Chunks: {chunk_count}\")\n",
    "        print(f\"   ‚Ä¢ Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c75a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_context(self, chunks: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build context from retrieved chunks.\n",
    "    \n",
    "    No truncation needed since chunks are now properly sized (600-800 chars)\n",
    "    at indexing time by the improved MarkdownSemanticChunker.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_text = chunk['text']\n",
    "        \n",
    "        # Sanity check: warn if chunk is unexpectedly large (shouldn't happen with new chunker)\n",
    "        if len(chunk_text) > 1000:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Source {i} is {len(chunk_text)} chars (expected max 800)\")\n",
    "            print(f\"   This suggests you need to re-index with the new chunker!\")\n",
    "            # Truncate as fallback for old chunks\n",
    "            chunk_text = chunk_text[:800] + \"...\"\n",
    "        \n",
    "        # Clear source boundaries help model understand context\n",
    "        source_header = f\"=== SOURCE {i} ===\"\n",
    "        source_footer = f\"=== END SOURCE {i} ===\"\n",
    "        \n",
    "        # Include heading path for better context\n",
    "        heading = chunk.get('heading_path', '')\n",
    "        if heading:\n",
    "            context_parts.append(f\"{source_header}\\nSection: {heading}\\n\\n{chunk_text}\\n{source_footer}\")\n",
    "        else:\n",
    "            context_parts.append(f\"{source_header}\\n{chunk_text}\\n{source_footer}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3caa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name jinaai/jina-colbert-v2. Creating a new one with mean pooling.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "\n",
      "============================================================\n",
      "Processing: /Users/aireesm4/Python_Projects/hybrid-rag-ColBERTv2/PDFs/MPO Timesheet FAQ.pdf\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Converting PDF to Markdown... ‚úì 0.84s\n",
      "  ‚Ä¢ Extracted 15,528 characters\n",
      "\n",
      "[Step 2/5] Extracting and analyzing images...\n",
      "    Analyzing image 1 on page 4... ‚úì (12.3s)\n",
      "    Analyzing image 2 on page 4... ‚úì (11.5s)\n",
      "    Analyzing image 3 on page 5... ‚úì (10.6s)\n",
      "    Analyzing image 4 on page 5... ‚úì (10.4s)\n",
      "    Analyzing image 5 on page 5... ‚úì (10.6s)\n",
      "    Analyzing image 6 on page 6... ‚úì (9.1s)\n",
      "    Analyzing image 7 on page 6... ‚úì (10.3s)\n",
      "    Analyzing image 8 on page 7... ‚úì (12.3s)\n",
      "    Analyzing image 9 on page 7... ‚úì (9.9s)\n",
      "    Analyzing image 10 on page 7... ‚úì (8.4s)\n",
      "    Analyzing image 11 on page 8... ‚úì (9.5s)\n",
      "    Analyzing image 12 on page 8... ‚úì (45.5s)\n",
      "    Analyzing image 13 on page 8... ‚úì (11.5s)\n",
      "  ‚úì Completed in 172.01s\n",
      "  ‚Ä¢ Extracted 13 images\n",
      "  ‚Ä¢ Vision analysis: ‚úì\n",
      "\n",
      "[Step 3/5] Markdown-aware semantic chunking... ‚úì 0.00s\n",
      "  ‚Ä¢ Created 28 semantic chunks\n",
      "\n",
      "[Step 4/5] Enriching chunks with image context... ‚úì 0.01s\n",
      "  ‚Ä¢ 3 chunks enriched with image context + OCR text\n",
      "\n",
      "[Step 5/5] Saving chunks to database... ‚úì 0.01s\n",
      "\n",
      "============================================================\n",
      "Building Indexes\n",
      "============================================================\n",
      "  ‚Ä¢ Corpus: 28 chunks\n",
      "  ‚Ä¢ Chunk ID mapping: 28 entries\n",
      "\n",
      "[BM25s] Building lexical search index... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ada8d44a5e402487b844bb57843c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a21296da314db9bb1c4888e234f0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10a2549fff54a2991579891962a4e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d66acc816e475f8a57fcfe436669fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 0.05s\n",
      "\n",
      "[ColBERT] Building semantic search index...\n",
      "  Encoding 28 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3284a6064bf44818dcfbf356c4a1d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì 4.06s\n",
      "\n",
      "‚úÖ Document indexed successfully!\n",
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Loading indexes...\n",
      "  ‚Ä¢ Loaded 28 chunk ID mappings\n",
      "‚úÖ Chatbot initialized and ready!\n",
      "\n",
      "============================================================\n",
      "RAG Chatbot - Interactive Mode\n",
      "============================================================\n",
      "Type your questions (or 'exit' to quit, 'clear' to clear history)\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 10 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 28, using k=28 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a953433f8521476289845c3616f3c885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bc05ed3e26400c83a50962f5d81105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da386238c1c4763ae72723fa51482cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.052s (28 results)\n",
      "   ‚Ä¢ ColBERT: 0.461s (28 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (28 candidates)\n",
      "   ‚Ä¢ Fetch: 0.006s (28 chunks)\n",
      "   ‚Ä¢ Rerank: 3.146s (top 10)\n",
      "   ‚úì Total retrieval: 3.665s\n",
      "\n",
      "‚ö†Ô∏è  Stopping at 8 chunks (would exceed 6000 char limit)\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5909 characters\n",
      "Chunks retrieved: 10\n",
      "Chunks actually used: 8\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "[Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "A5. All timesheets are due for submission every Friday by 5:00pm.\n",
      "Please Note: The only exception is if you are on a support team and work weekends or have been involved\n",
      "with a weekend rollout, then you have until Mondays no later than 10:00am to submit your timesheet for\n",
      "approval.\n",
      "\n",
      "A6. Yes, it is recommended that all users going on vacation submit timesheets before they leave.\n",
      "\n",
      "A7. You can submit your timesheet up to **8 weeks** in advance.\n",
      "\n",
      "A8. Yes, if you have worked on a Holiday, Saturday and Sunday, hours can be added to that day. The Holiday\n",
      "day will display as greyed out, the same as Saturday and Sunday, but it is accessible.\n",
      "=== END SOURCE 1 ===\n",
      "\n",
      "\n",
      "\n",
      "... [truncated, full context is 5909 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: how many weeks in advance can i post my vacation or personal leave?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "According to Source 1, A7: \"You can submit your timesheet up to **8 weeks** in advance.\" This applies to both vacation time and personal leave. However, it's worth noting that you should still go on vacation before submitting a timesheet (Source 1, A6).\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  7.1s | üìö 8 chunks | üìù 5909 chars | üéØ top_k=10\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.6061] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (702 chars)\n",
      "  2. [0.5177] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (775 chars)\n",
      "  3. [0.4809] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (520 chars)\n",
      "  4. [0.4463] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (712 chars)\n",
      "  5. [0.4146] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (327 chars)\n",
      "  6. [0.4004] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (701 chars)\n",
      "  7. [0.4001] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (762 chars)\n",
      "  8. [0.4001] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (628 chars)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: 2 additional chunks retrieved but not sent to LLM\n",
      "     (exceeded max_context_chars limit of 6000)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: According to Source 1, A7: \"You can submit your timesheet up to **8 weeks** in advance.\" This applies to both vacation time and personal leave. However, it's worth noting that you should still go on vacation before submitting a timesheet (Source 1, A6).\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (8)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6061\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      4.0058\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6061\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0323\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "A5. All timesheets are due for submission every Friday by 5:00pm.\n",
      "Please Note: The only exception is if you are on a support team and work weekends or have been involved\n",
      "with a weekend rollout, then you have until Mondays no later than 10...\n",
      "‚îÇ [Truncated - 702 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5177\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      9.6347\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5177\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0325\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "5. When are Timesheets due? 6. Can I submit my vacation time in advance? 7. How many weeks in advance can I post my vacation or personal leave? 8. Can I submit hours on Holidays, Saturday, and Sunday? 9. My timesheet was ‚ÄòRejected‚Äô what d...\n",
      "‚îÇ [Truncated - 775 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4809\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.8571\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4809\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0310\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "A3. Validate that the timesheet workweek status is NOT ‚ÄòSubmitted‚Äô, ‚ÄòApproved‚Äô, ‚ÄòRejected‚Äô or ‚ÄòPeriod Closed‚Äô.\n",
      "Only ‚Äò **In Progress** ‚Äô or ‚Äò **Not Yet Created** ‚Äô timesheets can be updated. To validate the status, go to My\n",
      "Timesheets, loo...\n",
      "‚îÇ [Truncated - 520 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4463\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.6778\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4463\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0306\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "A10. Go to My Timesheets. Highlight the week of the timesheet that needs to be recalled then click the ‚ÄòRecall‚Äô\n",
      "button in the upper right corner, click ‚ÄòOk‚Äô then go to that timesheet week and click ‚ÄòClick to create‚Äô, adjust\n",
      "hours and subm...\n",
      "‚îÇ [Truncated - 712 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4146\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.4045\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4146\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0299\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "What happens to the hours I entered on my timesheet if I delete it? 15. What is the recommended setting for my timesheet view? 16. What is the recommended setting for my timesheet summary view? 17. Who can submit my timesheet if I am not ...\n",
      "‚îÇ [Truncated - 327 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4004\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      5.1121\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4004\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0313\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "**RM and PM Questions** (click the item to go to the answer section)\n",
      "1. A resource is not seeing the project task I have created and assigned. What are my checkpoints?\n",
      "2. I am a PM, how do I add my Resource Manager as the Status Manager?\n",
      "...\n",
      "‚îÇ [Truncated - 701 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4001\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.8397\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4001\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0290\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "**Time Tracking Resources Answers**\n",
      "A1. Contact the Project Manager of the project you have been assigned to. Ask the PM if you have been\n",
      "assigned a task. The PM may need to update and successfully published the plan. The resource should ...\n",
      "‚îÇ [Truncated - 762 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 8 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4001\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.5262\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4001\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0282\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "Once you have recalled the timesheet, zero out all hours for the week you submitted and save the timesheet.\n",
      "Then go and enter the time once again and ‚ÄòTurn in Final timesheet‚Äô again.\n",
      "\n",
      "If the timesheet approval is still not visible on your...\n",
      "‚îÇ [Truncated - 628 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize config with SEPARATE models for vision and chat\n",
    "# Vision: gemma3:4b (multimodal, for analyzing images)\n",
    "# Chat: gemma3:4b (FASTER BUT prone to hallucinations)\n",
    "# Chat: llama3.2:3b (FASTER and STREAMING - recommended for 16GB RAM Mac Mini M4) -> CURRENT\n",
    "# Note: gpt-oss:20b is available but VERY slow. Use only if you need maximum quality.\n",
    "config = RAGConfig(chat_model='llama3.2:3b')  # Changed from gpt-oss:20b to llama3.2:3b for better performance\n",
    "app = RAGApplication(config)\n",
    "\n",
    "# Check Ollama\n",
    "if not app.check_ollama():\n",
    "    print(\"‚ùå Ollama is not running!\")\n",
    "    print(\"\\nTo start Ollama:\")\n",
    "    print(\"  1. Open a terminal\")\n",
    "    print(\"  2. Run: ollama serve\")\n",
    "    print(\"  3. Keep that terminal open\")\n",
    "    print(\"\\nThen run this cell again.\")\n",
    "else:\n",
    "    # Simple menu with proper exit handling\n",
    "    exit_program = False\n",
    "    \n",
    "    while not exit_program:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAG Chatbot - Choose an option:\")\n",
    "        print(\"1. Upload and index a PDF\")\n",
    "        print(\"2. Start interactive chat\")\n",
    "        print(\"3. Show database statistics\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            file_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "            if os.path.exists(file_path):\n",
    "                app.index_documents([file_path])\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            app.initialize_chatbot()\n",
    "            app.interactive_chat()\n",
    "            # Back to main menu after chat exits\n",
    "            print(\"\\n[Returned to main menu]\")\n",
    "            \n",
    "        elif choice == '3':\n",
    "            app.print_stats()\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Goodbye! üëã\")\n",
    "            print(\"=\"*50)\n",
    "            exit_program = True\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1-4.\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Program exited successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593762a",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "- check if we can change to a bigger model\n",
    "- check system prompt for LLM response (temp as well)\n",
    "- check token limit for LLM response (max 6000) - should be longer\n",
    "- check token limit for ColBERT (max 512) - should be longer\n",
    "- check out different chunking strategies PDF from weaviate\n",
    "- check the RRF (fusion) scoring and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRAG (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
