{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ef7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Local RAG Chatbot with Image Understanding\\n===================================================\\n\\n‚úÖ No Cloud Dependencies (runs 100% locally)\\n‚úÖ No RAGatouille (direct Jina ColBERT v2 implementation)\\n‚úÖ PyMuPDF4LLM for PDF conversion\\n‚úÖ Image extraction and analysis with LLaVA vision model\\n‚úÖ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\\n‚úÖ Markdown-aware semantic chunking\\n‚úÖ SQLite database for storage\\n\\nRequirements:\\n- Ollama (for LLMs: llama3.2:3b, llava:7b)\\n- Mac Mini M4 or similar (16GB RAM recommended)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Local RAG Chatbot with Image Understanding\n",
    "===================================================\n",
    "\n",
    "‚úÖ No Cloud Dependencies (runs 100% locally)\n",
    "‚úÖ No RAGatouille (direct Jina ColBERT v2 implementation)\n",
    "‚úÖ PyMuPDF4LLM for PDF conversion\n",
    "‚úÖ Image extraction and analysis with LLaVA vision model\n",
    "‚úÖ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\n",
    "‚úÖ Markdown-aware semantic chunking\n",
    "‚úÖ SQLite database for storage\n",
    "\n",
    "Requirements:\n",
    "- Ollama (for LLMs: llama3.2:3b, llava:7b)\n",
    "- Mac Mini M4 or similar (16GB RAM recommended)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f723da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Suppress tokenizers parallelism warning when forking\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress deprecation warnings from transformers/sentence-transformers\n",
    "warnings.filterwarnings('ignore', message='.*torch_dtype.*deprecated.*')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage  # Renamed to avoid conflict with database model\n",
    "\n",
    "# PDF and text processing\n",
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF for image extraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Retrieval\n",
    "import bm25s\n",
    "from bm25s.hf import BM25HF\n",
    "import Stemmer  # PyStemmer for stemming\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "\n",
    "# LLM\n",
    "import requests  # For Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a1fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for local RAG system\"\"\"\n",
    "    # Base directory (set to project root - parent of notebooks folder)\n",
    "    base_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    # Database\n",
    "    db_path: str = None\n",
    "    \n",
    "    # Chunking - FIXED to use CHARACTER counts (more reliable than token counts)\n",
    "    # Optimized for small models (3B params) with limited context windows\n",
    "    min_chunk_size: int = 600   # Minimum 600 characters per chunk\n",
    "    max_chunk_size: int = 800   # Maximum 800 characters per chunk (HARD LIMIT)\n",
    "    chunk_overlap: int = 200    # 200 character overlap between chunks\n",
    "    \n",
    "    # Retrieval - ADAPTIVE strategy for better recall\n",
    "    bm25_top_k: int = 100       # BM25 initial candidates\n",
    "    colbert_top_k: int = 100    # ColBERT initial candidates\n",
    "    \n",
    "    # Adaptive top-k: Adjust based on query complexity\n",
    "    final_top_k_min: int = 5    # Minimum chunks (for simple queries)\n",
    "    final_top_k_max: int = 10   # Maximum chunks (for complex queries)\n",
    "    final_top_k_default: int = 7  # Default for most queries\n",
    "    \n",
    "    # Context window management\n",
    "    max_context_chars: int = 6000  # Max chars to send to LLM (7-8 chunks √ó 800)\n",
    "    \n",
    "    # Models\n",
    "    chat_model: str = \"llama3.2:3b\"\n",
    "    vision_model: str = \"gemma3:4b\"\n",
    "    embedding_model: str = \"jinaai/jina-colbert-v2\"\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_url: str = \"http://localhost:11434\"\n",
    "    ollama_timeout: int = 300  # Increased timeout for slower models\n",
    "    \n",
    "    # Paths (will be set to absolute paths in __post_init__)\n",
    "    bm25_index_path: str = None\n",
    "    colbert_index_path: str = None\n",
    "    images_dir: str = None\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set absolute paths after initialization\"\"\"\n",
    "        if self.db_path is None:\n",
    "            self.db_path = os.path.join(self.base_dir, \"rag_local.db\")\n",
    "        if self.bm25_index_path is None:\n",
    "            self.bm25_index_path = os.path.join(self.base_dir, \"indexes\", \"bm25s\")\n",
    "        if self.colbert_index_path is None:\n",
    "            self.colbert_index_path = os.path.join(self.base_dir, \"indexes\", \"colbert\")\n",
    "        if self.images_dir is None:\n",
    "            self.images_dir = os.path.join(self.base_dir, \"extracted_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    upload_date = Column(DateTime, default=datetime.utcnow)\n",
    "    total_pages = Column(Integer)\n",
    "    status = Column(String(50))\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = 'images'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    page_number = Column(Integer, nullable=False)\n",
    "    image_path = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    image_type = Column(String(50))\n",
    "    ocr_text = Column(Text)\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = 'chunks'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    text = Column(Text, nullable=False)\n",
    "    heading_path = Column(String(500))\n",
    "    token_count = Column(Integer)\n",
    "    has_images = Column(Boolean, default=False)\n",
    "    chunk_metadata = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA CLIENT WITH STREAMING SUPPORT\n",
    "# ============================================================================\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with Ollama API with streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.ollama_url\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        model: str, \n",
    "        prompt: str, \n",
    "        system: str = \"\",\n",
    "        images: List[str] = None,\n",
    "        timeout: int = 300,\n",
    "        stream: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text with Ollama (with optional streaming)\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                # Streaming mode - print tokens as they arrive\n",
    "                response = requests.post(url, json=payload, timeout=timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"response\" in chunk:\n",
    "                            token = chunk[\"response\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "                        \n",
    "                        # Check if done\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "                \n",
    "                print()  # Newline after streaming\n",
    "                return full_response\n",
    "            else:\n",
    "                # Non-streaming mode - wait for complete response\n",
    "                response = requests.post(url, json=payload, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                return result.get(\"response\", \"\")\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            return f\"Error: Request timed out after {timeout} seconds\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def analyze_image(self, image_path: str) -> Dict:\n",
    "        \"\"\"Analyze image using vision model\"\"\"\n",
    "        import base64\n",
    "        \n",
    "        # Read and encode image\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "        # Prompt for image analysis\n",
    "        prompt = \"\"\"Analyze this image and provide:\n",
    "1. A detailed description of what you see\n",
    "2. The type of image (diagram, chart, photo, screenshot, etc.)\n",
    "3. Any text visible in the image (OCR)\n",
    "\n",
    "Format your response as:\n",
    "DESCRIPTION: [your description]\n",
    "TYPE: [image type]\n",
    "TEXT: [any visible text]\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.generate(\n",
    "                model=self.config.vision_model,\n",
    "                prompt=prompt,\n",
    "                images=[image_data],\n",
    "                timeout=self.config.ollama_timeout\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            description = \"\"\n",
    "            image_type = \"unknown\"\n",
    "            ocr_text = \"\"\n",
    "            \n",
    "            lines = response.split('\\n')\n",
    "            current_section = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('DESCRIPTION:'):\n",
    "                    current_section = 'description'\n",
    "                    description = line.replace('DESCRIPTION:', '').strip()\n",
    "                elif line.startswith('TYPE:'):\n",
    "                    current_section = 'type'\n",
    "                    image_type = line.replace('TYPE:', '').strip()\n",
    "                elif line.startswith('TEXT:'):\n",
    "                    current_section = 'text'\n",
    "                    ocr_text = line.replace('TEXT:', '').strip()\n",
    "                elif current_section == 'description' and line:\n",
    "                    description += ' ' + line\n",
    "                elif current_section == 'text' and line:\n",
    "                    ocr_text += ' ' + line\n",
    "            \n",
    "            # FIXED: Return 'type' key instead of 'image_type' to match what DocumentProcessor expects\n",
    "            return {\n",
    "                'description': description.strip() or response[:200],  # Fallback to first 200 chars\n",
    "                'type': image_type.strip() or 'image',  # Changed from 'image_type' to 'type'\n",
    "                'ocr_text': ocr_text.strip()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è  Error analyzing image: {e}\")\n",
    "            return {\n",
    "                'description': 'Image analysis failed',\n",
    "                'type': 'unknown',  # Changed from 'image_type' to 'type'\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "    \n",
    "    def chat(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        context: str = None,\n",
    "        stream: bool = True  # Enable streaming by default!\n",
    "    ) -> str:\n",
    "        \"\"\"Chat with context - ULTRA-STRONG grounding to prevent hallucination\"\"\"\n",
    "        \n",
    "        # Use /api/chat endpoint for proper message handling\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "\n",
    "        # ULTRA-STRONG system message - Maximum grounding for small models\n",
    "        # UPDATED: Removed citation requirement for natural conversation flow\n",
    "        if context:\n",
    "            system_msg = f\"\"\"You are a helpful AI assistant that answers questions based on provided documents.\n",
    "\n",
    "üö´ ABSOLUTE RULES:\n",
    "- ONLY use information from the documents provided below\n",
    "- DO NOT use knowledge from your training data\n",
    "- DO NOT make assumptions beyond what's written\n",
    "- If the answer is not in the documents, say: \"I don't have that information in the provided documents.\"\n",
    "\n",
    "‚úÖ HOW TO ANSWER:\n",
    "- Read the documents carefully\n",
    "- Provide clear, direct answers\n",
    "- Use natural language (no need to cite \"Source 1\" etc.)\n",
    "- Be concise but complete\n",
    "\n",
    "üìÑ DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "Answer the user's question naturally and helpfully using only the information above.\"\"\"\n",
    "        else:\n",
    "            system_msg = \"You are a helpful assistant.\"\n",
    "\n",
    "        # CRITICAL FIX: When context is provided (RAG mode), only pass the current query\n",
    "        # This prevents the LLM from getting confused by previous answers based on different contexts\n",
    "        if context:\n",
    "            # For RAG queries: Only send the LATEST user question (not full conversation history)\n",
    "            # This prevents hallucination from mixing old context with new queries\n",
    "            chat_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": messages[-1][\"content\"]}  # Only current question!\n",
    "            ]\n",
    "        else:\n",
    "            # For non-RAG queries: Include full conversation history\n",
    "            chat_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg}\n",
    "            ]\n",
    "            for msg in messages:\n",
    "                chat_messages.append({\n",
    "                    \"role\": msg[\"role\"],\n",
    "                    \"content\": msg[\"content\"]\n",
    "                })\n",
    "\n",
    "        # Call Ollama chat API with STRONGER grounding parameters\n",
    "        payload = {\n",
    "            \"model\": self.config.chat_model,\n",
    "            \"messages\": chat_messages,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.0,  # ZERO temperature for maximum factuality!\n",
    "                \"top_p\": 0.8,  # Reduced for more focused responses\n",
    "                \"top_k\": 20,  # Limit vocabulary to most likely tokens\n",
    "                \"repeat_penalty\": 1.2,  # Increased to prevent repetition\n",
    "                \"num_ctx\": 4096  # Ensure enough context window\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if stream:\n",
    "                response = requests.post(url, json=payload, timeout=self.config.ollama_timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                            token = chunk[\"message\"][\"content\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "\n",
    "                print()\n",
    "                return full_response\n",
    "            else:\n",
    "                response = requests.post(url, json=payload, timeout=self.config.ollama_timeout)\n",
    "                response.raise_for_status()\n",
    "                return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            return f\"Error: Request timed out after {self.config.ollama_timeout} seconds\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75436919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKDOWN-AWARE SEMANTIC CHUNKER - REWRITTEN FOR PROPER SIZE ENFORCEMENT\n",
    "# ============================================================================\n",
    "\n",
    "class MarkdownSemanticChunker:\n",
    "    \"\"\"\n",
    "    Intelligent markdown chunking that STRICTLY respects size limits while maintaining hierarchy.\n",
    "    \n",
    "    Key improvements:\n",
    "    - Uses CHARACTER counts (not misleading token counts)\n",
    "    - HARD enforces max_chunk_size (no more 26K char chunks!)\n",
    "    - Respects markdown hierarchy when possible\n",
    "    - Splits at sentence boundaries for better semantic coherence\n",
    "    - Maintains overlap for context continuity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def chunk_markdown(self, markdown_text: str, doc_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Create semantically meaningful chunks with STRICT size enforcement\"\"\"\n",
    "        sections = self._parse_markdown_hierarchy(markdown_text)\n",
    "        chunks = self._create_chunks_from_sections(sections, doc_context)\n",
    "        return chunks\n",
    "    \n",
    "    def _parse_markdown_hierarchy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse markdown into hierarchical sections\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        heading_stack = []\n",
    "        \n",
    "        for line in lines:\n",
    "            heading_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            \n",
    "            if heading_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                level = len(heading_match.group(1))\n",
    "                title = heading_match.group(2).strip()\n",
    "                \n",
    "                heading_stack = [(lvl, ttl) for lvl, ttl in heading_stack if lvl < level]\n",
    "                heading_stack.append((level, title))\n",
    "                \n",
    "                parent_path = ' > '.join([ttl for _, ttl in heading_stack[:-1]])\n",
    "                full_path = ' > '.join([ttl for _, ttl in heading_stack])\n",
    "                \n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'content': '',\n",
    "                    'parent_path': parent_path,\n",
    "                    'full_path': full_path\n",
    "                }\n",
    "            else:\n",
    "                if current_section is not None:\n",
    "                    current_section['content'] += line + '\\n'\n",
    "                else:\n",
    "                    if not sections or sections[-1]['level'] != 0:\n",
    "                        sections.append({\n",
    "                            'level': 0,\n",
    "                            'title': 'Introduction',\n",
    "                            'content': line + '\\n',\n",
    "                            'parent_path': '',\n",
    "                            'full_path': 'Introduction'\n",
    "                        })\n",
    "                    else:\n",
    "                        sections[-1]['content'] += line + '\\n'\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_chunks_from_sections(self, sections: List[Dict], doc_context: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create chunks with HARD size limits while respecting markdown hierarchy.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Try to keep sections together if they fit\n",
    "        2. If section is too large, split at paragraph boundaries\n",
    "        3. If paragraph is too large, split at sentence boundaries\n",
    "        4. ALWAYS enforce max_chunk_size as HARD limit\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for section in sections:\n",
    "            section_chunks = self._process_section(section, doc_context)\n",
    "            chunks.extend(section_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_section(self, section: Dict, doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Process a single section, splitting if necessary\"\"\"\n",
    "        # Format section with heading\n",
    "        heading_text = self._format_heading(section)\n",
    "        content = section['content'].strip()\n",
    "        \n",
    "        # Calculate sizes\n",
    "        heading_size = len(heading_text)\n",
    "        content_size = len(content)\n",
    "        total_size = heading_size + content_size\n",
    "        \n",
    "        # Case 1: Entire section fits within max size\n",
    "        if total_size <= self.config.max_chunk_size:\n",
    "            return [{\n",
    "                'text': heading_text + content,\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': total_size,\n",
    "                'token_count': self._estimate_tokens(total_size),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'complete_section'\n",
    "            }]\n",
    "        \n",
    "        # Case 2: Section is too large - need to split\n",
    "        # Try splitting at paragraph boundaries first\n",
    "        paragraphs = re.split(r'\\n\\n+', content)\n",
    "        \n",
    "        if len(paragraphs) > 1:\n",
    "            return self._split_by_paragraphs(section, heading_text, paragraphs, doc_context)\n",
    "        else:\n",
    "            # Single large paragraph - split by sentences\n",
    "            return self._split_by_sentences(section, heading_text, content, doc_context)\n",
    "    \n",
    "    def _split_by_paragraphs(\n",
    "        self, \n",
    "        section: Dict, \n",
    "        heading_text: str, \n",
    "        paragraphs: List[str], \n",
    "        doc_context: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Split section by paragraphs, respecting max_chunk_size\"\"\"\n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = 1\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            para_size = len(para) + 2  # +2 for \\n\\n\n",
    "            \n",
    "            # Check if adding this paragraph would exceed max size\n",
    "            if current_size + para_size > self.config.max_chunk_size:\n",
    "                # Save current chunk if it has content beyond heading\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'split_section',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                # Check if paragraph itself is too large\n",
    "                if para_size > self.config.max_chunk_size - len(heading_text):\n",
    "                    # Paragraph is too large - split by sentences\n",
    "                    sentence_chunks = self._split_paragraph_by_sentences(\n",
    "                        section, heading_text, para, doc_context, part_num\n",
    "                    )\n",
    "                    chunks.extend(sentence_chunks)\n",
    "                    part_num += len(sentence_chunks)\n",
    "                    current_text = heading_text\n",
    "                    current_size = len(heading_text)\n",
    "                else:\n",
    "                    # Start new chunk with this paragraph\n",
    "                    current_text = heading_text + para + '\\n\\n'\n",
    "                    current_size = len(current_text)\n",
    "            else:\n",
    "                # Add paragraph to current chunk\n",
    "                current_text += para + '\\n\\n'\n",
    "                current_size += para_size\n",
    "        \n",
    "        # Add final chunk if it has content\n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'split_section',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_by_sentences(\n",
    "        self, \n",
    "        section: Dict, \n",
    "        heading_text: str, \n",
    "        content: str, \n",
    "        doc_context: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Split content by sentences when paragraphs are too large\"\"\"\n",
    "        # Simple sentence splitting (can be improved with nltk if needed)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "        \n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = 1\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            sentence_size = len(sentence) + 1  # +1 for space\n",
    "            \n",
    "            # Check if adding this sentence would exceed max size\n",
    "            if current_size + sentence_size > self.config.max_chunk_size:\n",
    "                # Save current chunk if it has content\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'sentence_split',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                # If sentence itself is too large, truncate it (last resort)\n",
    "                if sentence_size > self.config.max_chunk_size - len(heading_text):\n",
    "                    truncated = sentence[:self.config.max_chunk_size - len(heading_text) - 3] + \"...\"\n",
    "                    chunks.append({\n",
    "                        'text': heading_text + truncated,\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(heading_text + truncated),\n",
    "                        'token_count': self._estimate_tokens(len(heading_text + truncated)),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'truncated',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                    current_text = heading_text\n",
    "                    current_size = len(heading_text)\n",
    "                else:\n",
    "                    # Start new chunk with this sentence\n",
    "                    current_text = heading_text + sentence + ' '\n",
    "                    current_size = len(current_text)\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_text += sentence + ' '\n",
    "                current_size += sentence_size\n",
    "        \n",
    "        # Add final chunk if it has content\n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'sentence_split',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_paragraph_by_sentences(\n",
    "        self,\n",
    "        section: Dict,\n",
    "        heading_text: str,\n",
    "        paragraph: str,\n",
    "        doc_context: str,\n",
    "        start_part_num: int\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Helper to split a single large paragraph by sentences\"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', paragraph)\n",
    "        chunks = []\n",
    "        current_text = heading_text\n",
    "        current_size = len(heading_text)\n",
    "        part_num = start_part_num\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            sentence_size = len(sentence) + 1\n",
    "            \n",
    "            if current_size + sentence_size > self.config.max_chunk_size:\n",
    "                if current_size > len(heading_text):\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'char_count': len(current_text.strip()),\n",
    "                        'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'sentence_split',\n",
    "                        'part': part_num\n",
    "                    })\n",
    "                    part_num += 1\n",
    "                \n",
    "                current_text = heading_text + sentence + ' '\n",
    "                current_size = len(current_text)\n",
    "            else:\n",
    "                current_text += sentence + ' '\n",
    "                current_size += sentence_size\n",
    "        \n",
    "        if current_size > len(heading_text):\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'char_count': len(current_text.strip()),\n",
    "                'token_count': self._estimate_tokens(len(current_text.strip())),\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'sentence_split',\n",
    "                'part': part_num\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _format_heading(self, section: Dict) -> str:\n",
    "        \"\"\"Format section heading with context\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if section['parent_path']:\n",
    "            parts.append(f\"[Context: {section['parent_path']}]\")\n",
    "        \n",
    "        if section['title'] and section['title'] != 'Introduction':\n",
    "            heading_prefix = '#' * section['level']\n",
    "            parts.append(f\"{heading_prefix} {section['title']}\")\n",
    "        \n",
    "        if parts:\n",
    "            return '\\n\\n'.join(parts) + '\\n\\n'\n",
    "        return ''\n",
    "    \n",
    "    def _estimate_tokens(self, char_count: int) -> int:\n",
    "        \"\"\"Estimate token count from character count (rough approximation)\"\"\"\n",
    "        # Rough estimate: 1 token ‚âà 4 characters for English text\n",
    "        return char_count // 4\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        DEPRECATED: Old method that was broken due to truncation.\n",
    "        Kept for compatibility but now just returns character count.\n",
    "        \"\"\"\n",
    "        return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR WITH IMAGE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF processing with image extraction and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.ollama = ollama_client\n",
    "        self.chunker = MarkdownSemanticChunker(config)\n",
    "        \n",
    "        # Create images directory\n",
    "        os.makedirs(config.images_dir, exist_ok=True)\n",
    "    \n",
    "    def _sanitize_utf8(self, text: str) -> str:\n",
    "        \"\"\"IMPROVED: Robust UTF-8 sanitization to prevent database corruption and LLM errors\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Remove invalid UTF-8 sequences\n",
    "            clean_text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Step 2: Remove null bytes which cause database issues\n",
    "            clean_text = clean_text.replace('\\x00', '')\n",
    "            \n",
    "            # Step 3: Remove problematic control characters (keep newlines, tabs, carriage returns)\n",
    "            clean_text = ''.join(\n",
    "                char for char in clean_text\n",
    "                if char in ['\\n', '\\t', '\\r'] or ord(char) >= 32\n",
    "            )\n",
    "            \n",
    "            # Step 4: Normalize whitespace (optional but helps with consistency)\n",
    "            # Replace multiple spaces with single space\n",
    "            import re\n",
    "            clean_text = re.sub(r' +', ' ', clean_text)\n",
    "            \n",
    "            return clean_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è  UTF-8 sanitization error: {e}\")\n",
    "            # Last resort: keep only printable ASCII\n",
    "            return ''.join(char for char in text if 32 <= ord(char) <= 126 or char in ['\\n', '\\t'])\n",
    "    \n",
    "    def pdf_to_markdown(self, pdf_path: str) -> str:\n",
    "        \"\"\"Convert PDF to Markdown using PyMuPDF4LLM\"\"\"\n",
    "        markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        # Sanitize to remove invalid UTF-8\n",
    "        return self._sanitize_utf8(markdown_text)\n",
    "    \n",
    "    def _group_nearby_rectangles(self, rects: List[fitz.Rect], proximity_threshold: float = 20) -> List[List[int]]:\n",
    "        \"\"\"Group rectangles that are close to each other\"\"\"\n",
    "        if not rects:\n",
    "            return []\n",
    "\n",
    "        # Each rect gets assigned to a group\n",
    "        groups = []\n",
    "        assigned = [False] * len(rects)\n",
    "\n",
    "        for i, rect in enumerate(rects):\n",
    "            if assigned[i]:\n",
    "                continue\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [i]\n",
    "            assigned[i] = True\n",
    "\n",
    "            # Find all rects that should be in this group\n",
    "            changed = True\n",
    "            while changed:\n",
    "                changed = False\n",
    "                for j, other_rect in enumerate(rects):\n",
    "                    if assigned[j]:\n",
    "                        continue\n",
    "\n",
    "                    # Check if this rect is close to any rect in current group\n",
    "                    for group_idx in current_group:\n",
    "                        group_rect = rects[group_idx]\n",
    "\n",
    "                        # Calculate distance between rectangles\n",
    "                        # Expand each rect by proximity_threshold and check for intersection\n",
    "                        expanded_group = fitz.Rect(\n",
    "                            group_rect.x0 - proximity_threshold,\n",
    "                            group_rect.y0 - proximity_threshold,\n",
    "                            group_rect.x1 + proximity_threshold,\n",
    "                            group_rect.y1 + proximity_threshold\n",
    "                        )\n",
    "\n",
    "                        if expanded_group.intersects(other_rect):\n",
    "                            current_group.append(j)\n",
    "                            assigned[j] = True\n",
    "                            changed = True\n",
    "                            break\n",
    "\n",
    "            groups.append(current_group)\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def extract_images_from_pdf(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        document_id: int,\n",
    "        min_image_size: int = 50,  # Minimum width/height in pixels\n",
    "        proximity_threshold: float = 20  # Group images within this distance (points)\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract images from PDF with intelligent grouping.\n",
    "        Groups nearby images together to capture complete diagrams.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            image_list = page.get_images(full=True)\n",
    "\n",
    "            if not image_list:\n",
    "                continue\n",
    "\n",
    "            # Get bounding boxes for all images on this page\n",
    "            image_bboxes = []\n",
    "            for img_info in image_list:\n",
    "                xref = img_info[0]\n",
    "                # Get all instances of this image on the page\n",
    "                rects = page.get_image_rects(xref)\n",
    "                if rects:\n",
    "                    for rect in rects:\n",
    "                        # Check minimum size\n",
    "                        width = rect.width\n",
    "                        height = rect.height\n",
    "                        if width >= min_image_size and height >= min_image_size:\n",
    "                            image_bboxes.append({\n",
    "                                'rect': rect,\n",
    "                                'xref': xref,\n",
    "                                'width': width,\n",
    "                                'height': height\n",
    "                            })\n",
    "\n",
    "            if not image_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Group nearby images\n",
    "            rects_only = [bbox['rect'] for bbox in image_bboxes]\n",
    "            groups = self._group_nearby_rectangles(rects_only, proximity_threshold)\n",
    "\n",
    "            # Process each group\n",
    "            for group_idx, group in enumerate(groups):\n",
    "                if len(group) == 1:\n",
    "                    # Single image - extract normally\n",
    "                    bbox = image_bboxes[group[0]]\n",
    "                    try:\n",
    "                        base_image = doc.extract_image(bbox['xref'])\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        pil_image = PILImage.open(io.BytesIO(image_bytes))\n",
    "\n",
    "                        # Save image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_img{len(images)+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        if pil_image.mode == 'RGBA':\n",
    "                            pil_image = pil_image.convert('RGB')\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': False,\n",
    "                            'bbox': bbox['rect']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Failed to extract single image on page {page_num+1}: {e}\")\n",
    "\n",
    "                else:\n",
    "                    # Multiple images grouped together - capture as screenshot\n",
    "                    # Calculate bounding box that encompasses all images in group\n",
    "                    union_rect = image_bboxes[group[0]]['rect']\n",
    "                    for idx in group[1:]:\n",
    "                        union_rect = union_rect | image_bboxes[idx]['rect']  # Union of rectangles\n",
    "\n",
    "                    # Add some padding\n",
    "                    padding = 5\n",
    "                    union_rect = fitz.Rect(\n",
    "                        max(0, union_rect.x0 - padding),\n",
    "                        max(0, union_rect.y0 - padding),\n",
    "                        min(page.rect.width, union_rect.x1 + padding),\n",
    "                        min(page.rect.height, union_rect.y1 + padding)\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        # Render this region as an image\n",
    "                        mat = fitz.Matrix(2, 2)  # 2x zoom for better quality\n",
    "                        pix = page.get_pixmap(matrix=mat, clip=union_rect)\n",
    "\n",
    "                        # Convert to PIL Image\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        pil_image = PILImage.open(io.BytesIO(img_data))\n",
    "\n",
    "                        # Save composite image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_composite{group_idx+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': True,\n",
    "                            'num_components': len(group),\n",
    "                            'bbox': union_rect\n",
    "                        })\n",
    "\n",
    "                        print(f\"    üìä Grouped {len(group)} images into composite on page {page_num+1}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Failed to create composite image on page {page_num+1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        return images\n",
    "    \n",
    "    def analyze_images(\n",
    "        self, \n",
    "        images: List[Dict],\n",
    "        document_id: int,\n",
    "        db_session\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Analyze images with vision model and save to database\"\"\"\n",
    "        image_ids = []\n",
    "        \n",
    "        for idx, img_info in enumerate(images):\n",
    "            print(f\"    Analyzing image {idx+1} on page {img_info['page_number']}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Analyze with vision model\n",
    "            analysis = self.ollama.analyze_image(img_info['image_path'])\n",
    "            \n",
    "            # Save to database with UTF-8 sanitization\n",
    "            image_record = Image(\n",
    "                document_id=document_id,\n",
    "                page_number=img_info['page_number'],\n",
    "                image_path=img_info['image_path'],\n",
    "                description=self._sanitize_utf8(analysis['description']),\n",
    "                image_type=self._sanitize_utf8(analysis['type']),\n",
    "                ocr_text=self._sanitize_utf8(analysis['ocr_text'])\n",
    "            )\n",
    "            db_session.add(image_record)\n",
    "            db_session.flush()\n",
    "            \n",
    "            image_ids.append(image_record.id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì ({elapsed:.1f}s)\")\n",
    "        \n",
    "        db_session.commit()\n",
    "        return image_ids\n",
    "    \n",
    "    def enrich_chunks_with_images(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        images_data: List[Dict],\n",
    "        db_session\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Add image context (description + OCR text) to relevant chunks for better search accuracy\"\"\"\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_copy = chunk.copy()\n",
    "            \n",
    "            # Find images that might be relevant to this chunk\n",
    "            # Simple heuristic: chunks that mention visual content keywords\n",
    "            relevant_images = []\n",
    "            \n",
    "            for img in images_data:\n",
    "                if any(keyword in chunk['text'].lower() for keyword in \n",
    "                       ['figure', 'image', 'diagram', 'chart', 'screenshot', 'see below', 'shown in']):\n",
    "                    relevant_images.append(img)\n",
    "            \n",
    "            if relevant_images:\n",
    "                # Build comprehensive image context including OCR text\n",
    "                image_context = \"\\n\\n[Images in this section]:\\n\"\n",
    "                image_metadata = []\n",
    "                \n",
    "                for img in relevant_images:\n",
    "                    # Add type and description\n",
    "                    image_context += f\"- {img['type'].capitalize()}: {img['description']}\\n\"\n",
    "                    \n",
    "                    # CRITICAL: Add OCR text if available (makes text in images searchable!)\n",
    "                    if img.get('ocr_text') and img['ocr_text'].strip():\n",
    "                        image_context += f\"  Text visible in image: {img['ocr_text']}\\n\"\n",
    "                    \n",
    "                    image_metadata.append({\n",
    "                        'path': img['image_path'],\n",
    "                        'description': img['description'],\n",
    "                        'type': img['type'],\n",
    "                        'ocr_text': img.get('ocr_text', '')\n",
    "                    })\n",
    "                \n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'] + image_context)\n",
    "                chunk_copy['has_images'] = True\n",
    "                chunk_copy['image_paths'] = [img['image_path'] for img in relevant_images]\n",
    "                chunk_copy['image_metadata'] = image_metadata\n",
    "            else:\n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'])\n",
    "                chunk_copy['has_images'] = False\n",
    "            \n",
    "            enriched_chunks.append(chunk_copy)\n",
    "        \n",
    "        return enriched_chunks\n",
    "    \n",
    "    def process_document(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        db_session\n",
    "    ) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Convert to markdown\n",
    "        print(\"\\n[Step 1/5] Converting PDF to Markdown...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        markdown_text = self.pdf_to_markdown(pdf_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Extracted {len(markdown_text):,} characters\")\n",
    "        \n",
    "        # Create document record\n",
    "        doc = Document(\n",
    "            filename=os.path.basename(pdf_path),\n",
    "            status='processing'\n",
    "        )\n",
    "        db_session.add(doc)\n",
    "        db_session.commit()\n",
    "        \n",
    "        # Step 2: Extract and analyze images\n",
    "        print(\"\\n[Step 2/5] Extracting and analyzing images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        images = self.extract_images_from_pdf(pdf_path, doc.id)\n",
    "        \n",
    "        if images:\n",
    "            image_ids = self.analyze_images(images, doc.id, db_session)\n",
    "            \n",
    "            # Get image data for enrichment\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_record = db_session.query(Image).filter_by(id=img_id).first()\n",
    "                if img_record:\n",
    "                    images_data.append({\n",
    "                        'image_path': img_record.image_path,\n",
    "                        'description': img_record.description,\n",
    "                        'type': img_record.image_type,\n",
    "                        'ocr_text': img_record.ocr_text\n",
    "                    })\n",
    "        else:\n",
    "            images_data = []\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ‚úì Completed in {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Extracted {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"  ‚Ä¢ Vision analysis: ‚úì\")\n",
    "        \n",
    "        # Step 3: Markdown-aware semantic chunking\n",
    "        print(\"\\n[Step 3/5] Markdown-aware semantic chunking...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        doc_context = f\"Document: {os.path.basename(pdf_path)}\\n\\n{markdown_text[:500]}\"\n",
    "        chunks = self.chunker.chunk_markdown(markdown_text, doc_context)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Created {len(chunks)} semantic chunks\")\n",
    "        \n",
    "        # Step 4: Enrich chunks with image context (INCLUDING OCR TEXT!)\n",
    "        print(\"\\n[Step 4/5] Enriching chunks with image context...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        if images_data:\n",
    "            chunks = self.enrich_chunks_with_images(chunks, images_data, db_session)\n",
    "            chunks_with_images = sum(1 for c in chunks if c.get('has_images', False))\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì {elapsed:.2f}s\")\n",
    "            print(f\"  ‚Ä¢ {chunks_with_images} chunks enriched with image context + OCR text\")\n",
    "        else:\n",
    "            # Still sanitize even if no images\n",
    "            for chunk in chunks:\n",
    "                chunk['text'] = self._sanitize_utf8(chunk['text'])\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"‚úì {elapsed:.2f}s\")\n",
    "            print(f\"  ‚Ä¢ No images to enrich\")\n",
    "        \n",
    "        # Step 5: Save to database\n",
    "        print(\"\\n[Step 5/5] Saving chunks to database...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_record = Chunk(\n",
    "                document_id=doc.id,\n",
    "                chunk_index=idx,\n",
    "                text=self._sanitize_utf8(chunk['text']),  # Sanitize before saving\n",
    "                heading_path=self._sanitize_utf8(chunk.get('heading_path', '')),  # Sanitize heading too\n",
    "                token_count=chunk.get('token_count', 0),\n",
    "                has_images=chunk.get('has_images', False),\n",
    "                chunk_metadata=self._sanitize_utf8(json.dumps({\n",
    "                    k: v for k, v in chunk.items() \n",
    "                    if k not in ['text', 'heading_path', 'token_count', 'has_images']\n",
    "                })) if chunk else ''\n",
    "            )\n",
    "            db_session.add(chunk_record)\n",
    "        \n",
    "        doc.status = 'indexed'\n",
    "        db_session.commit()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "        \n",
    "        return chunks, doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08725b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JINA COLBERT V2 RETRIEVER (NO RAGATOUILLE!)\n",
    "# ============================================================================\n",
    "\n",
    "class JinaColBERTRetriever:\n",
    "    \"\"\"Direct implementation of Jina ColBERT v2 (no RAGatouille dependency)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = SentenceTransformer(\n",
    "            config.embedding_model,\n",
    "            trust_remote_code=True,\n",
    "            device=config.device\n",
    "        )\n",
    "        # Set max sequence length to avoid truncation warnings\n",
    "        self.model.max_seq_length = 512\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Index corpus with ColBERT embeddings\"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        print(f\"  Encoding {len(corpus)} documents...\")\n",
    "        \n",
    "        # Encode corpus (this gives us token-level embeddings)\n",
    "        # Truncate long sequences to avoid errors\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        os.makedirs(self.config.colbert_index_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'embeddings': self.corpus_embeddings,\n",
    "            'corpus': corpus\n",
    "        }, os.path.join(self.config.colbert_index_path, 'index.pt'))\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.config.colbert_index_path, 'index.pt')\n",
    "        data = torch.load(index_file, map_location=self.config.device)\n",
    "        self.corpus_embeddings = data['embeddings']\n",
    "        self.corpus = data['corpus']\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using MaxSim scoring\"\"\"\n",
    "        if not self.corpus or len(self.corpus) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Handle single item corpus\n",
    "        if len(self.corpus) == 1:\n",
    "            return [{\n",
    "                'document_id': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'text': self.corpus[0]\n",
    "            }]\n",
    "        \n",
    "        # Get top-k\n",
    "        k = min(k, len(scores))\n",
    "        top_k_indices = torch.topk(scores, k=k).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'document_id': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'text': self.corpus[idx] if self.corpus else None\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank documents with more accurate scoring\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Encode query and documents\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = self.model.encode(\n",
    "            documents, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, doc_embeddings)\n",
    "        \n",
    "        # Handle single document\n",
    "        if len(documents) == 1:\n",
    "            return [{\n",
    "                'result_index': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'rank': 1,\n",
    "                'text': documents[0]\n",
    "            }]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(sorted_indices[:k]):\n",
    "            results.append({\n",
    "                'result_index': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'rank': rank + 1,\n",
    "                'text': documents[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _maxsim_score(\n",
    "        self, \n",
    "        query_embedding: torch.Tensor, \n",
    "        doc_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MaxSim score between query and documents\n",
    "        \n",
    "        MaxSim: For each query token, find max similarity with all doc tokens,\n",
    "        then average across query tokens\n",
    "        \"\"\"\n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 1:\n",
    "            query_embedding = query_embedding.unsqueeze(0)\n",
    "        if doc_embeddings.dim() == 1:\n",
    "            doc_embeddings = doc_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # For 2D embeddings (single vector per doc), compute cosine similarity directly\n",
    "        if query_embedding.dim() == 2 and doc_embeddings.dim() == 2:\n",
    "            # Normalize embeddings\n",
    "            query_norm = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            doc_norm = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            scores = torch.mm(query_norm, doc_norm.t())\n",
    "            \n",
    "            # Return as 1D tensor\n",
    "            return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()\n",
    "        \n",
    "        # For 3D embeddings (token-level), use mean pooling\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_vec = query_embedding.mean(dim=1)\n",
    "        else:\n",
    "            query_vec = query_embedding\n",
    "            \n",
    "        if doc_embeddings.dim() == 3:\n",
    "            doc_vec = doc_embeddings.mean(dim=1)\n",
    "        else:\n",
    "            doc_vec = doc_embeddings\n",
    "        \n",
    "        # Normalize\n",
    "        query_vec = torch.nn.functional.normalize(query_vec, p=2, dim=-1)\n",
    "        doc_vec = torch.nn.functional.normalize(doc_vec, p=2, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        if query_vec.dim() == 1:\n",
    "            query_vec = query_vec.unsqueeze(0)\n",
    "        if doc_vec.dim() == 1:\n",
    "            doc_vec = doc_vec.unsqueeze(0)\n",
    "            \n",
    "        scores = torch.mm(query_vec, doc_vec.t())\n",
    "        \n",
    "        # Return as 1D tensor\n",
    "        return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUAL INDEXER (BM25s + Jina ColBERT)\n",
    "# ============================================================================\n",
    "\n",
    "class DualIndexer:\n",
    "    \"\"\"Manages BM25s and Jina ColBERT v2 indexes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.bm25_retriever = None\n",
    "        self.colbert_retriever = JinaColBERTRetriever(config)\n",
    "    \n",
    "    def build_bm25_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build BM25s index\"\"\"\n",
    "        print(\"\\n[BM25s] Building lexical search index...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create stemmer\n",
    "        stemmer = Stemmer.Stemmer(\"english\")\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        corpus_tokens = bm25s.tokenize(\n",
    "            corpus, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=stemmer\n",
    "        )\n",
    "        \n",
    "        self.bm25_retriever = bm25s.BM25()\n",
    "        self.bm25_retriever.index(corpus_tokens)\n",
    "        \n",
    "        os.makedirs(self.config.bm25_index_path, exist_ok=True)\n",
    "        self.bm25_retriever.save(self.config.bm25_index_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úì {elapsed:.2f}s\")\n",
    "    \n",
    "    def build_colbert_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build Jina ColBERT v2 index\"\"\"\n",
    "        print(\"\\n[ColBERT] Building semantic search index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.colbert_retriever.index(corpus)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ‚úì {elapsed:.2f}s\")\n",
    "    \n",
    "    def load_indexes(self) -> None:\n",
    "        \"\"\"Load indexes from disk\"\"\"\n",
    "        self.bm25_retriever = bm25s.BM25.load(self.config.bm25_index_path)\n",
    "        self.colbert_retriever.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID RETRIEVER WITH RRF AND RERANKING\n",
    "# ============================================================================\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Three-stage retrieval: BM25s + ColBERT + ColBERT Reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, indexer: DualIndexer, db_session, corpus_to_chunk_id: List[int] = None):\n",
    "        self.config = config\n",
    "        self.indexer = indexer\n",
    "        self.db_session = db_session\n",
    "        self.stemmer = Stemmer.Stemmer(\"english\")\n",
    "        # CRITICAL: Mapping from corpus index to database chunk ID\n",
    "        self.corpus_to_chunk_id = corpus_to_chunk_id or []\n",
    "    \n",
    "    def retrieve(self, query: str, top_k_final: int = None) -> List[Dict]:\n",
    "        \"\"\"Three-stage hybrid retrieval with detailed scoring\"\"\"\n",
    "        if top_k_final is None:\n",
    "            top_k_final = self.config.final_top_k\n",
    "        \n",
    "        print(f\"\\nüîç Retrieving relevant chunks...\")\n",
    "        \n",
    "        # Get corpus size to adjust k values\n",
    "        corpus_size = len(self.indexer.colbert_retriever.corpus) if self.indexer.colbert_retriever.corpus else 0\n",
    "        \n",
    "        # Adjust k values based on corpus size\n",
    "        bm25_k = min(self.config.bm25_top_k, corpus_size) if corpus_size > 0 else self.config.bm25_top_k\n",
    "        colbert_k = min(self.config.colbert_top_k, corpus_size) if corpus_size > 0 else self.config.colbert_top_k\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Corpus size: {corpus_size}, using k={bm25_k} for retrieval\")\n",
    "        \n",
    "        # Stage 1: BM25s\n",
    "        start = time.time()\n",
    "        bm25_results = self._bm25_search(query, k=bm25_k)\n",
    "        bm25_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ BM25s: {bm25_time:.3f}s ({len(bm25_results)} results)\")\n",
    "        \n",
    "        # Stage 2: ColBERT\n",
    "        start = time.time()\n",
    "        colbert_results = self._colbert_search(query, k=colbert_k)\n",
    "        colbert_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ ColBERT: {colbert_time:.3f}s ({len(colbert_results)} results)\")\n",
    "        \n",
    "        # Fusion\n",
    "        start = time.time()\n",
    "        fused_results = self._reciprocal_rank_fusion(bm25_results, colbert_results)\n",
    "        candidates = fused_results[:min(50, len(fused_results))]\n",
    "        fusion_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Fusion: {fusion_time:.3f}s ({len(candidates)} candidates)\")\n",
    "        \n",
    "        # Fetch chunks - USING THE MAPPING!\n",
    "        start = time.time()\n",
    "        candidate_corpus_indices = [r['corpus_index'] for r in candidates]\n",
    "        candidate_chunks = self._fetch_chunks_from_db(candidate_corpus_indices)\n",
    "        \n",
    "        # PRESERVE INTERMEDIATE SCORES\n",
    "        # Map corpus_index to intermediate scores\n",
    "        score_map = {}\n",
    "        for bm25_result in bm25_results:\n",
    "            idx = bm25_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['bm25_score'] = bm25_result['score']\n",
    "        \n",
    "        for colbert_result in colbert_results:\n",
    "            idx = colbert_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['colbert_score'] = colbert_result['score']\n",
    "        \n",
    "        for fused_result in candidates:\n",
    "            idx = fused_result['corpus_index']\n",
    "            if idx in score_map:\n",
    "                score_map[idx]['rrf_score'] = fused_result['rrf_score']\n",
    "        \n",
    "        # Add intermediate scores to chunks\n",
    "        for i, chunk in enumerate(candidate_chunks):\n",
    "            corpus_idx = candidate_corpus_indices[i]\n",
    "            if corpus_idx in score_map:\n",
    "                chunk['intermediate_scores'] = score_map[corpus_idx]\n",
    "        \n",
    "        fetch_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Fetch: {fetch_time:.3f}s ({len(candidate_chunks)} chunks)\")\n",
    "        \n",
    "        # Stage 3: Rerank\n",
    "        start = time.time()\n",
    "        final_k = min(top_k_final, len(candidate_chunks))\n",
    "        reranked_results = self._colbert_rerank(query, candidate_chunks, top_k=final_k)\n",
    "        rerank_time = time.time() - start\n",
    "        print(f\"   ‚Ä¢ Rerank: {rerank_time:.3f}s (top {len(reranked_results)})\")\n",
    "        \n",
    "        total_time = bm25_time + colbert_time + fusion_time + fetch_time + rerank_time\n",
    "        print(f\"   ‚úì Total retrieval: {total_time:.3f}s\")\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 1: BM25s lexical search\"\"\"\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            query, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=self.stemmer\n",
    "        )\n",
    "        \n",
    "        results, scores = self.indexer.bm25_retriever.retrieve(query_tokens, k=k)\n",
    "        \n",
    "        return [\n",
    "            {'corpus_index': int(results[0][i]), 'score': float(scores[0][i]), 'source': 'bm25'}\n",
    "            for i in range(len(results[0]))\n",
    "        ]\n",
    "    \n",
    "    def _colbert_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 2: ColBERT semantic search\"\"\"\n",
    "        results = self.indexer.colbert_retriever.search(query=query, k=k)\n",
    "        return [\n",
    "            {'corpus_index': r['document_id'], 'score': r['score'], 'source': 'colbert'}\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def _reciprocal_rank_fusion(\n",
    "        self, \n",
    "        bm25_results: List[Dict], \n",
    "        colbert_results: List[Dict],\n",
    "        k: int = 60\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"RRF fusion\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for rank, result in enumerate(bm25_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        for rank, result in enumerate(colbert_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{'corpus_index': idx, 'rrf_score': score} for idx, score in sorted_results]\n",
    "    \n",
    "    def _fetch_chunks_from_db(self, corpus_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Fetch chunks from database using corpus index -> chunk ID mapping\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for corpus_idx in corpus_indices:\n",
    "            # Convert corpus index to database chunk ID\n",
    "            if corpus_idx < len(self.corpus_to_chunk_id):\n",
    "                chunk_id = self.corpus_to_chunk_id[corpus_idx]\n",
    "                \n",
    "                # Fetch from database using the actual chunk ID\n",
    "                chunk = self.db_session.query(Chunk).filter_by(id=chunk_id).first()\n",
    "                if chunk:\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk.id,\n",
    "                        'text': chunk.text,\n",
    "                        'document_id': chunk.document_id,\n",
    "                        'heading_path': chunk.heading_path,\n",
    "                        'has_images': chunk.has_images,\n",
    "                        'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Chunk ID {chunk_id} not found in database\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Corpus index {corpus_idx} out of range (max: {len(self.corpus_to_chunk_id)-1})\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _colbert_rerank(self, query: str, chunks: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 3: ColBERT reranking with score preservation\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        reranked_results = self.indexer.colbert_retriever.rerank(query=query, documents=documents, k=top_k)\n",
    "        \n",
    "        final_results = []\n",
    "        for result in reranked_results:\n",
    "            original_chunk = chunks[result['result_index']]\n",
    "            intermediate_scores = original_chunk.get('intermediate_scores', {})\n",
    "            \n",
    "            final_results.append({\n",
    "                'chunk_id': original_chunk['chunk_id'],\n",
    "                'text': original_chunk['text'],\n",
    "                'document_id': original_chunk['document_id'],\n",
    "                'heading_path': original_chunk.get('heading_path', ''),\n",
    "                'has_images': original_chunk.get('has_images', False),\n",
    "                'metadata': original_chunk['metadata'],\n",
    "                'score': result['score'],  # Final ColBERT rerank score (cosine similarity)\n",
    "                'rank': result['rank'],\n",
    "                'bm25_score': intermediate_scores.get('bm25_score', 0.0),\n",
    "                'colbert_score': intermediate_scores.get('colbert_score', 0.0),\n",
    "                'rrf_score': intermediate_scores.get('rrf_score', 0.0)\n",
    "            })\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAG CHATBOT WITH ADAPTIVE CHUNKING\n",
    "# ============================================================================\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Complete RAG chatbot with adaptive chunk selection\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, retriever: HybridRetriever, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.ollama = ollama_client\n",
    "        self.conversation_history = []\n",
    "        self.debug_mode = True  # Enable debugging to see what's being sent to LLM\n",
    "    \n",
    "    def _determine_top_k(self, query: str) -> int:\n",
    "        \"\"\"\n",
    "        Determine optimal number of chunks based on query complexity.\n",
    "        \n",
    "        Simple queries (e.g., \"What is X?\") ‚Üí fewer chunks (5)\n",
    "        Complex queries (e.g., \"List all...\", \"Compare...\") ‚Üí more chunks (10)\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Keywords indicating need for comprehensive answers\n",
    "        comprehensive_keywords = [\n",
    "            'all', 'list', 'different', 'various', 'types of', 'kinds of',\n",
    "            'compare', 'contrast', 'difference', 'similarities',\n",
    "            'explain', 'describe in detail', 'comprehensive',\n",
    "            'multiple', 'several', 'many'\n",
    "        ]\n",
    "        \n",
    "        # Check if query needs comprehensive answer\n",
    "        needs_comprehensive = any(keyword in query_lower for keyword in comprehensive_keywords)\n",
    "        \n",
    "        if needs_comprehensive:\n",
    "            return self.config.final_top_k_max  # 10 chunks for comprehensive answers\n",
    "        else:\n",
    "            return self.config.final_top_k_default  # 7 chunks for normal queries\n",
    "    \n",
    "    def chat(self, query: str, stream: bool = True, top_k: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Process user query with adaptive chunk selection.\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            stream: Enable streaming response\n",
    "            top_k: Override automatic top_k selection (optional)\n",
    "        \"\"\"\n",
    "        # Determine optimal number of chunks\n",
    "        if top_k is None:\n",
    "            top_k = self._determine_top_k(query)\n",
    "        \n",
    "        print(f\"\\nüí° Query complexity analysis: Using {top_k} chunks\")\n",
    "        \n",
    "        # Retrieve relevant chunks (use top_k_final parameter name)\n",
    "        retrieved_chunks = self.retriever.retrieve(query, top_k_final=top_k)\n",
    "        \n",
    "        # Build context with smart truncation\n",
    "        context, actual_chunks_used = self._build_context_adaptive(retrieved_chunks)\n",
    "        \n",
    "        # DEBUG: Show what's being sent to LLM\n",
    "        if self.debug_mode:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üêõ DEBUG: Context being sent to LLM\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Context length: {len(context)} characters\")\n",
    "            print(f\"Chunks retrieved: {len(retrieved_chunks)}\")\n",
    "            print(f\"Chunks actually used: {actual_chunks_used}\")\n",
    "            print(f\"\\nFirst 800 characters of context:\")\n",
    "            print(context[:800])\n",
    "            print(f\"\\n... [truncated, full context is {len(context)} chars]\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Clean display header\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí¨ Question: {query}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nü§ñ Answer:\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        })\n",
    "        \n",
    "        # Generate response with streaming\n",
    "        response = self.ollama.chat(\n",
    "            messages=self.conversation_history,\n",
    "            context=context,\n",
    "            stream=stream\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Clean footer with metadata\n",
    "        print(f\"\\n{'‚îÄ'*70}\")\n",
    "        print(f\"‚è±Ô∏è  {elapsed:.1f}s | üìö {actual_chunks_used} chunks | üìù {len(context)} chars | üéØ top_k={top_k}\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': response\n",
    "        })\n",
    "        \n",
    "        # Display source information\n",
    "        print(f\"\\nüìñ Sources Used:\")\n",
    "        print(f\"{'‚îÄ'*70}\")\n",
    "        for i in range(actual_chunks_used):\n",
    "            chunk = retrieved_chunks[i]\n",
    "            heading = chunk.get('heading_path', 'No heading')\n",
    "            score = chunk.get('score', 0.0)\n",
    "            char_count = len(chunk.get('text', ''))\n",
    "            print(f\"  {i+1}. [{score:.4f}] {heading[:50]}... ({char_count} chars)\")\n",
    "        \n",
    "        if len(retrieved_chunks) > actual_chunks_used:\n",
    "            print(f\"\\n  ‚ö†Ô∏è  Note: {len(retrieved_chunks) - actual_chunks_used} additional chunks retrieved but not sent to LLM\")\n",
    "            print(f\"     (exceeded max_context_chars limit of {self.config.max_context_chars})\")\n",
    "        \n",
    "        print(f\"{'‚îÄ'*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': self._format_sources(retrieved_chunks[:actual_chunks_used]),\n",
    "            'retrieved_chunks': len(retrieved_chunks),\n",
    "            'used_chunks': actual_chunks_used,\n",
    "            'context_length': len(context),\n",
    "            'top_k': top_k\n",
    "        }\n",
    "    \n",
    "    def _build_context_adaptive(self, chunks: List[Dict]) -> tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Build context with adaptive truncation to stay within max_context_chars.\n",
    "        \n",
    "        Returns:\n",
    "            (context_string, number_of_chunks_used)\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        total_chars = 0\n",
    "        chunks_used = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            chunk_text = chunk['text']\n",
    "            \n",
    "            # Sanity check for old chunks\n",
    "            if len(chunk_text) > 1000:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Source {i} is {len(chunk_text)} chars (expected max 800)\")\n",
    "                print(f\"   This suggests you need to re-index with the new chunker!\")\n",
    "                chunk_text = chunk_text[:800] + \"...\"\n",
    "            \n",
    "            # Calculate what context size would be if we add this chunk\n",
    "            source_header = f\"=== SOURCE {i} ===\"\n",
    "            source_footer = f\"=== END SOURCE {i} ===\"\n",
    "            heading = chunk.get('heading_path', '')\n",
    "            \n",
    "            if heading:\n",
    "                chunk_formatted = f\"{source_header}\\nSection: {heading}\\n\\n{chunk_text}\\n{source_footer}\"\n",
    "            else:\n",
    "                chunk_formatted = f\"{source_header}\\n{chunk_text}\\n{source_footer}\"\n",
    "            \n",
    "            chunk_size = len(chunk_formatted) + 2  # +2 for \\n\\n separator\n",
    "            \n",
    "            # Check if adding this chunk would exceed max context\n",
    "            if total_chars + chunk_size > self.config.max_context_chars:\n",
    "                print(f\"\\n‚ö†Ô∏è  Stopping at {chunks_used} chunks (would exceed {self.config.max_context_chars} char limit)\")\n",
    "                break\n",
    "            \n",
    "            context_parts.append(chunk_formatted)\n",
    "            total_chars += chunk_size\n",
    "            chunks_used += 1\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts), chunks_used\n",
    "    \n",
    "    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format source citations with full text, image paths, and ALL scores\"\"\"\n",
    "        sources = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            source = {\n",
    "                'source_id': i + 1,\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'document_id': chunk['document_id'],\n",
    "                'heading': chunk.get('heading_path', ''),\n",
    "                'score': chunk['score'],  # Final ColBERT rerank score\n",
    "                'bm25_score': chunk.get('bm25_score', 0.0),\n",
    "                'colbert_score': chunk.get('colbert_score', 0.0),\n",
    "                'rrf_score': chunk.get('rrf_score', 0.0),\n",
    "                'has_images': chunk.get('has_images', False),\n",
    "                'text': chunk['text'],  # Include full text\n",
    "                'preview': chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "            }\n",
    "            \n",
    "            # Add image paths if available\n",
    "            if chunk.get('has_images') and chunk.get('metadata'):\n",
    "                image_paths = chunk['metadata'].get('image_paths', [])\n",
    "                source['image_paths'] = image_paths\n",
    "            \n",
    "            sources.append(source)\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"üóëÔ∏è  Conversation history cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    \"\"\"Main application orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Database setup\n",
    "        db_url = f\"sqlite:///{config.db_path}\"\n",
    "        self.engine = create_engine(db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.db_session = Session()\n",
    "        \n",
    "        # Initialize Ollama client\n",
    "        self.ollama = OllamaClient(config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.processor = DocumentProcessor(config, self.ollama)\n",
    "        self.indexer = DualIndexer(config)\n",
    "        self.retriever = None\n",
    "        self.chatbot = None\n",
    "        \n",
    "        # CRITICAL: Store mapping between corpus index and chunk IDs\n",
    "        self.corpus_to_chunk_id = []  # Maps corpus index -> database chunk ID\n",
    "    \n",
    "    def check_ollama(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.ollama_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def index_documents(self, pdf_paths: List[str]) -> None:\n",
    "        \"\"\"Index PDF documents\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"‚ùå Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            chunks, doc_id = self.processor.process_document(pdf_path, self.db_session)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Building Indexes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build corpus and mapping\n",
    "        # CRITICAL FIX: Store the mapping between corpus index and database chunk IDs\n",
    "        all_db_chunks = self.db_session.query(Chunk).order_by(Chunk.id).all()\n",
    "        corpus = []\n",
    "        self.corpus_to_chunk_id = []\n",
    "        \n",
    "        for chunk in all_db_chunks:\n",
    "            corpus.append(chunk.text)\n",
    "            self.corpus_to_chunk_id.append(chunk.id)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Corpus: {len(corpus)} chunks\")\n",
    "        print(f\"  ‚Ä¢ Chunk ID mapping: {len(self.corpus_to_chunk_id)} entries\")\n",
    "        \n",
    "        # Build indexes\n",
    "        self.indexer.build_bm25_index(corpus)\n",
    "        self.indexer.build_colbert_index(corpus)\n",
    "        \n",
    "        # Save the mapping to disk for later use\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        os.makedirs(os.path.dirname(mapping_path), exist_ok=True)\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(self.corpus_to_chunk_id, f)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Document indexed successfully!\")\n",
    "    \n",
    "    def initialize_chatbot(self) -> None:\n",
    "        \"\"\"Initialize chatbot with existing indexes\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"‚ùå Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading indexes...\")\n",
    "        self.indexer.load_indexes()\n",
    "        \n",
    "        # Load the corpus-to-chunk-id mapping\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        try:\n",
    "            with open(mapping_path, 'rb') as f:\n",
    "                self.corpus_to_chunk_id = pickle.load(f)\n",
    "            print(f\"  ‚Ä¢ Loaded {len(self.corpus_to_chunk_id)} chunk ID mappings\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"  ‚ö†Ô∏è  Warning: No corpus mapping found. Please re-index your documents.\")\n",
    "            self.corpus_to_chunk_id = []\n",
    "        \n",
    "        self.retriever = HybridRetriever(self.config, self.indexer, self.db_session, self.corpus_to_chunk_id)\n",
    "        self.chatbot = RAGChatbot(self.config, self.retriever, self.ollama)\n",
    "        \n",
    "        print(\"‚úÖ Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Chat interface\"\"\"\n",
    "        if not self.chatbot:\n",
    "            raise RuntimeError(\"Chatbot not initialized. Call initialize_chatbot() first.\")\n",
    "        \n",
    "        return self.chatbot.chat(query)\n",
    "    \n",
    "    def _filter_relevant_images(self, query: str, image_paths: List[str], chunk_text: str) -> List[str]:\n",
    "        \"\"\"Filter images to only show those DIRECTLY relevant to the user's query - STRICT filtering\"\"\"\n",
    "        if not image_paths:\n",
    "            return []\n",
    "        \n",
    "        relevant_images = []\n",
    "        \n",
    "        # Extract meaningful query keywords (remove stop words)\n",
    "        stop_words = {'what', 'is', 'are', 'the', 'a', 'an', 'how', 'why', 'when', 'where', \n",
    "                      'can', 'could', 'would', 'should', 'do', 'does', 'did', 'of', 'in', 'on',\n",
    "                      'for', 'to', 'with', 'by', 'from', 'at', 'about', 'as', 'into', 'through',\n",
    "                      'diagram', 'chart', 'figure', 'image', 'screenshot', 'show', 'me', 'please'}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        query_words = [w for w in query_lower.split() if w not in stop_words and len(w) > 2]\n",
    "        \n",
    "        if not query_words:\n",
    "            return []  # No meaningful query words, don't show images\n",
    "        \n",
    "        # Get image metadata from database\n",
    "        for img_path in image_paths:\n",
    "            # Extract just the filename for DB lookup\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # Look up image in database to get description\n",
    "            img_record = self.db_session.query(Image).filter(\n",
    "                Image.image_path.like(f\"%{img_filename}\")\n",
    "            ).first()\n",
    "            \n",
    "            if img_record:\n",
    "                # Combine all image metadata\n",
    "                desc_lower = (img_record.description or \"\").lower()\n",
    "                img_type_lower = (img_record.image_type or \"\").lower()\n",
    "                ocr_lower = (img_record.ocr_text or \"\").lower()\n",
    "                \n",
    "                # Create searchable text from image\n",
    "                image_text = f\"{desc_lower} {img_type_lower} {ocr_lower}\"\n",
    "                image_words = [w for w in image_text.split() if w not in stop_words and len(w) > 2]\n",
    "                \n",
    "                # Calculate meaningful overlap\n",
    "                query_set = set(query_words)\n",
    "                image_set = set(image_words)\n",
    "                overlap = query_set.intersection(image_set)\n",
    "                \n",
    "                # STRICT CRITERIA: Need at least 3 meaningful word overlaps\n",
    "                # This ensures the image is actually about what the user asked\n",
    "                if len(overlap) >= 3:\n",
    "                    relevant_images.append(img_path)\n",
    "                    # print(f\"  DEBUG: Image matched with {len(overlap)} overlaps: {overlap}\")\n",
    "        \n",
    "        return relevant_images\n",
    "    \n",
    "    def _display_chunk_with_images(self, chunk_text: str, image_paths: List[str] = None) -> None:\n",
    "        \"\"\"Display chunk text and associated images\"\"\"\n",
    "        from IPython.display import display, Image as IPImage\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"{chunk_text}\\n\")\n",
    "        \n",
    "        # Display images if available\n",
    "        if image_paths:\n",
    "            print(f\"  üì∑ Relevant Images ({len(image_paths)}):\")\n",
    "            for img_path in image_paths:\n",
    "                if os.path.exists(img_path):\n",
    "                    try:\n",
    "                        display(IPImage(filename=img_path, width=400))\n",
    "                        print(f\"  ‚îî‚îÄ {os.path.basename(img_path)}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚îî‚îÄ ‚ö†Ô∏è Could not display {os.path.basename(img_path)}: {e}\\n\")\n",
    "                else:\n",
    "                    print(f\"  ‚îî‚îÄ ‚ö†Ô∏è Image not found: {os.path.basename(img_path)}\\n\")\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAG Chatbot - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Type your questions (or 'exit' to quit, 'clear' to clear history)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"\\nGoodbye! üëã\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'clear':\n",
    "                    self.chatbot.clear_history()\n",
    "                    continue\n",
    "                \n",
    "                result = self.chat(user_input)\n",
    "                print(f\"\\nAssistant: {result['response']}\\n\")\n",
    "                \n",
    "                # Show retrieved chunks with ALL SCORES\n",
    "                if result['sources']:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"üìä Retrieved Chunks with Similarity Scores ({len(result['sources'])})\")\n",
    "                    print(f\"{'='*60}\\n\")\n",
    "                    \n",
    "                    for idx, src in enumerate(result['sources'], 1):\n",
    "                        print(f\"‚îå‚îÄ Chunk {idx} {'‚îÄ'*50}\")\n",
    "                        \n",
    "                        # Show ALL retrieval scores\n",
    "                        print(f\"‚îÇ üéØ Final Score (ColBERT Rerank): {src['score']:.4f}\")\n",
    "                        print(f\"‚îÇ üìà Intermediate Scores:\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ BM25 (lexical):      {src.get('bm25_score', 0.0):.4f}\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ ColBERT (semantic):  {src.get('colbert_score', 0.0):.4f}\")\n",
    "                        print(f\"‚îÇ    ‚Ä¢ RRF (fusion):        {src.get('rrf_score', 0.0):.4f}\")\n",
    "                        \n",
    "                        if src['heading']:\n",
    "                            print(f\"‚îÇ üìç Section: {src['heading']}\")\n",
    "                        \n",
    "                        if src['has_images']:\n",
    "                            print(f\"‚îÇ üñºÔ∏è  Contains Images: Yes\")\n",
    "                        \n",
    "                        print(f\"‚îÇ\")\n",
    "                        print(f\"‚îÇ üìÑ Text:\")\n",
    "                        \n",
    "                        # Display chunk text (show first 300 chars as preview)\n",
    "                        chunk_text = src.get('text', src.get('preview', ''))\n",
    "                        \n",
    "                        # Show preview\n",
    "                        if len(chunk_text) > 300:\n",
    "                            print(f\"‚îÇ {chunk_text[:300]}...\")\n",
    "                            print(f\"‚îÇ [Truncated - {len(chunk_text)} total characters]\")\n",
    "                        else:\n",
    "                            print(f\"‚îÇ {chunk_text}\")\n",
    "                        \n",
    "                        # Filter and display only STRICTLY RELEVANT images\n",
    "                        if src['has_images'] and src.get('image_paths'):\n",
    "                            # Filter images based on query relevance with STRICT criteria\n",
    "                            relevant_images = self._filter_relevant_images(\n",
    "                                user_input, \n",
    "                                src['image_paths'], \n",
    "                                chunk_text\n",
    "                            )\n",
    "                            \n",
    "                            if relevant_images:\n",
    "                                print(f\"‚îÇ\")\n",
    "                                print(f\"‚îÇ [Showing {len(relevant_images)}/{len(src['image_paths'])} images matching your query]\")\n",
    "                                self._display_chunk_with_images(\"\", relevant_images)\n",
    "                            else:\n",
    "                                print(f\"‚îÇ\")\n",
    "                                print(f\"‚îÇ [This chunk has images, but none directly match your specific query]\")\n",
    "                        \n",
    "                        print(f\"‚îî{'‚îÄ'*60}\\n\")\n",
    "                    \n",
    "                    print()\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! üëã\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"Print database statistics\"\"\"\n",
    "        doc_count = self.db_session.query(Document).count()\n",
    "        chunk_count = self.db_session.query(Chunk).count()\n",
    "        image_count = self.db_session.query(Image).count()\n",
    "        \n",
    "        print(f\"\\nüìä Database Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Documents: {doc_count}\")\n",
    "        print(f\"   ‚Ä¢ Chunks: {chunk_count}\")\n",
    "        print(f\"   ‚Ä¢ Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c75a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_context(self, chunks: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build context from retrieved chunks.\n",
    "    \n",
    "    No truncation needed since chunks are now properly sized (600-800 chars)\n",
    "    at indexing time by the improved MarkdownSemanticChunker.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_text = chunk['text']\n",
    "        \n",
    "        # Sanity check: warn if chunk is unexpectedly large (shouldn't happen with new chunker)\n",
    "        if len(chunk_text) > 1000:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Source {i} is {len(chunk_text)} chars (expected max 800)\")\n",
    "            print(f\"   This suggests you need to re-index with the new chunker!\")\n",
    "            # Truncate as fallback for old chunks\n",
    "            chunk_text = chunk_text[:800] + \"...\"\n",
    "        \n",
    "        # Clear source boundaries help model understand context\n",
    "        source_header = f\"=== SOURCE {i} ===\"\n",
    "        source_footer = f\"=== END SOURCE {i} ===\"\n",
    "        \n",
    "        # Include heading path for better context\n",
    "        heading = chunk.get('heading_path', '')\n",
    "        if heading:\n",
    "            context_parts.append(f\"{source_header}\\nSection: {heading}\\n\\n{chunk_text}\\n{source_footer}\")\n",
    "        else:\n",
    "            context_parts.append(f\"{source_header}\\n{chunk_text}\\n{source_footer}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3caa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name jinaai/jina-colbert-v2. Creating a new one with mean pooling.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "\n",
      "============================================================\n",
      "Processing: /Users/airees/Python/hybrid-rag-ColBERTv2/PDFs/Weaviate-Advanced-RAG-Techniques-ebook-2.pdf\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Converting PDF to Markdown... ‚úì 10.70s\n",
      "  ‚Ä¢ Extracted 25,669 characters\n",
      "\n",
      "[Step 2/5] Extracting and analyzing images...\n",
      "    üìä Grouped 2 images into composite on page 1\n",
      "    Analyzing image 1 on page 1... ‚úì (6.9s)\n",
      "  ‚úì Completed in 7.61s\n",
      "  ‚Ä¢ Extracted 1 images\n",
      "  ‚Ä¢ Vision analysis: ‚úì\n",
      "\n",
      "[Step 3/5] Markdown-aware semantic chunking... ‚úì 0.00s\n",
      "  ‚Ä¢ Created 33 semantic chunks\n",
      "\n",
      "[Step 4/5] Enriching chunks with image context... ‚úì 0.00s\n",
      "  ‚Ä¢ 1 chunks enriched with image context + OCR text\n",
      "\n",
      "[Step 5/5] Saving chunks to database... ‚úì 0.01s\n",
      "\n",
      "============================================================\n",
      "Building Indexes\n",
      "============================================================\n",
      "  ‚Ä¢ Corpus: 61 chunks\n",
      "  ‚Ä¢ Chunk ID mapping: 61 entries\n",
      "\n",
      "[BM25s] Building lexical search index... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907a92be81fd4966b6d5033acc9bf46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43615a386e74a32a4c7b8f3a7ba8dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7303faaf6b049c697661a0b45c2dace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354ea5dd319141fba3a8b2998c7265a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 0.06s\n",
      "\n",
      "[ColBERT] Building semantic search index...\n",
      "  Encoding 61 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22139bbcf5044ae8a63dafa7bfa2d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì 2.89s\n",
      "\n",
      "‚úÖ Document indexed successfully!\n",
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Loading indexes...\n",
      "  ‚Ä¢ Loaded 61 chunk ID mappings\n",
      "‚úÖ Chatbot initialized and ready!\n",
      "\n",
      "============================================================\n",
      "RAG Chatbot - Interactive Mode\n",
      "============================================================\n",
      "Type your questions (or 'exit' to quit, 'clear' to clear history)\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 10 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5e1534c7a2465cb668a746f5b6c916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42476d4fc48346a29b57a9db138d5248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03aa6cdae679404b8b32f7e67c98506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.036s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.398s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.006s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.378s (top 10)\n",
      "   ‚úì Total retrieval: 2.817s\n",
      "\n",
      "‚ö†Ô∏è  Stopping at 7 chunks (would exceed 6000 char limit)\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5801 characters\n",
      "Chunks retrieved: 10\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "Chunking Strategies\n",
      "\n",
      "Chunking divides large documents into smaller, semantically meaningful segments. This process\n",
      "\n",
      "optimizes retrieval by balancing context preservation with manageable chunk sizes. Various\n",
      "\n",
      "common techniques exist for effective chunking in RAG, some of which are discussed below:\n",
      "\n",
      "Fixed-size chunking is a simple technique that splits text into\n",
      "\n",
      "chunks of a predetermined size, regardless of content structure.\n",
      "\n",
      "While it's cost-effective, it lacks contextual awareness. This can be\n",
      "\n",
      "improved by using overlapping chunks, allowing adjacent chunks to\n",
      "\n",
      "share some content.\n",
      "\n",
      "Recursive chunking offers more flexibility by initially splitting text\n",
      "\n",
      "using a primary separator (like paragraphs) and then applying\n",
      "\n",
      "secondary separators (like sentences\n",
      "\n",
      "... [truncated, full context is 5801 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: what are the different chunking strategies?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "According to Source 1, there are three common techniques for effective chunking in RAG:\n",
      "\n",
      "1. **Fixed-size chunking**: This technique splits text into chunks of a predetermined size, regardless of content structure.\n",
      "2. **Recursive chunking**: This approach initially splits the text using primary separators (like paragraphs) and then applies secondary separators (like sentences) if chunks are still too large.\n",
      "\n",
      "Additionally, Source 6 mentions another technique:\n",
      "\n",
      "3. **Document-based chunking**: This method creates chunks based on the natural divisions within a document, such as headings or sections.\n",
      "4. Source 2 also discusses:\n",
      "5. **Semantic chunking**: Divides text into meaningful units and then vectorizes them to create semantically coherent chunks.\n",
      "\n",
      "Note that these techniques can be combined or used in different ways depending on the specific requirements of the RAG system and the nature of the documents being processed.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  4.2s | üìö 7 chunks | üìù 5801 chars | üéØ top_k=10\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.6670] Introduction... (792 chars)\n",
      "  2. [0.6008] Introduction... (798 chars)\n",
      "  3. [0.5006] Introduction... (730 chars)\n",
      "  4. [0.4894] Introduction... (737 chars)\n",
      "  5. [0.4731] Introduction... (719 chars)\n",
      "  6. [0.4632] Introduction... (791 chars)\n",
      "  7. [0.4582] Introduction... (795 chars)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: 3 additional chunks retrieved but not sent to LLM\n",
      "     (exceeded max_context_chars limit of 6000)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: According to Source 1, there are three common techniques for effective chunking in RAG:\n",
      "\n",
      "1. **Fixed-size chunking**: This technique splits text into chunks of a predetermined size, regardless of content structure.\n",
      "2. **Recursive chunking**: This approach initially splits the text using primary separators (like paragraphs) and then applies secondary separators (like sentences) if chunks are still too large.\n",
      "\n",
      "Additionally, Source 6 mentions another technique:\n",
      "\n",
      "3. **Document-based chunking**: This method creates chunks based on the natural divisions within a document, such as headings or sections.\n",
      "4. Source 2 also discusses:\n",
      "5. **Semantic chunking**: Divides text into meaningful units and then vectorizes them to create semantically coherent chunks.\n",
      "\n",
      "Note that these techniques can be combined or used in different ways depending on the specific requirements of the RAG system and the nature of the documents being processed.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6670\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      2.3913\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6670\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0325\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Chunking Strategies\n",
      "\n",
      "Chunking divides large documents into smaller, semantically meaningful segments. This process\n",
      "\n",
      "optimizes retrieval by balancing context preservation with manageable chunk sizes. Various\n",
      "\n",
      "common techniques exist for effective chunking in RAG, some of which are discussed below:\n",
      "\n",
      "F...\n",
      "‚îÇ [Truncated - 792 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6008\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.2436\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6008\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0302\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ This technique respects the document's structure and adapts well\n",
      "\n",
      "to various use cases.\n",
      "\n",
      "Document-based chunking creates chunks based on the natural\n",
      "\n",
      "divisions within a document, such as headings or sections. It's\n",
      "\n",
      "particularly effective for structured data like HTML, Markdown, or\n",
      "\n",
      "code files but le...\n",
      "‚îÇ [Truncated - 798 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5006\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.8778\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5006\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0292\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ isolated sentences or propositions. While highly accurate, it's also\n",
      "\n",
      "the most computationally demanding approach.\n",
      "\n",
      "Each of the discussed techniques has its strengths, and the choice depends on the RAG system's\n",
      "\n",
      "specific requirements and the nature of the documents being processed. New approaches co...\n",
      "‚îÇ [Truncated - 730 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4894\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.7529\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4894\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0283\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "Query Decomposition\n",
      "\n",
      "Query decomposition is a technique that breaks down complex queries into simpler sub\n",
      "\n",
      "queries. This is useful for answering multifaceted questions requiring diverse information\n",
      "\n",
      "sources, leading to more precise and relevant search result...\n",
      "‚îÇ [Truncated - 737 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4731\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.0140\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4731\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0291\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ information to generate high-quality responses. In this case, you can apply a technique called\n",
      "\n",
      "‚ÄúSentence window retrieval‚Äù. This technique chunks the initial document into smaller pieces\n",
      "\n",
      "(usually single sentences) but stores a larger context window in its metadata. At retrieval time,\n",
      "\n",
      "the smaller ...\n",
      "‚îÇ [Truncated - 719 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4632\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.8650\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4632\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0305\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Query routing is a technique that directs queries to specific pipelines based on their content\n",
      "\n",
      "and intent, enabling a RAG system to handle diverse scenarios effectively. It works by analyzing\n",
      "\n",
      "each query and choosing the best retrieval method or processing pipeline to provide an\n",
      "\n",
      "accurate response....\n",
      "‚îÇ [Truncated - 791 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4582\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.6068\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4582\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0270\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ While this naive approach is straightforward, it has many limitations and can often lead to low\n",
      "\n",
      "quality responses.\n",
      "\n",
      "This e-book discusses various advanced techniques you can apply to improve the performance\n",
      "\n",
      "of your RAG system. These techniques can be applied at various stages in the RAG pipeline, ...\n",
      "‚îÇ [Truncated - 795 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 10 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73eaacc00a014f39855a8e91630af786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7681e20525e64039ac30597e2e628605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76920cd415f346cfb9b973d07ea98af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.016s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.359s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.005s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.245s (top 10)\n",
      "   ‚úì Total retrieval: 2.626s\n",
      "\n",
      "‚ö†Ô∏è  Stopping at 7 chunks (would exceed 6000 char limit)\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5781 characters\n",
      "Chunks retrieved: 10\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "transformation refines and expands unclear, complex, or ambiguous user queries to improve the\n",
      "\n",
      "quality of search results.\n",
      "\n",
      "Query Rewriting involves reformulating the original user query to make it more suitable for\n",
      "\n",
      "retrieval. This is particularly useful in scenarios where user queries are not optimally phrased or\n",
      "\n",
      "expressed differently. This can be achieved by using an LLM to rephrase the original user query\n",
      "\n",
      "or employing specialized smaller language models trained specifically for this task.\n",
      "\n",
      "This approach is called 'Rewrite-Retrieve-Read' instead of the traditional 'Retrieve-then-Read'\n",
      "\n",
      "paradigm.\n",
      "\n",
      "Raw Query Query Re-writer (LLM) Rewritten Query Retriever Retrieved Documents\n",
      "\n",
      "Query Expansion focuses on broadening the original query to capture more \n",
      "\n",
      "... [truncated, full context is 5781 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: explain what is query rerouting\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "Query routing involves directing queries to specific pipelines based on their content and intent, enabling a RAG (Relevance-Aware Retrieval) system to handle diverse scenarios effectively.\n",
      "\n",
      "In other words, query routing allows an AI agent to analyze each user's query and choose the best retrieval method or processing pipeline to provide an accurate response. This often requires implementing multi-index strategies, where different types of information are organized into separate, specialized indexes optimized for specific queries.\n",
      "\n",
      "For example, fact-based questions may be routed to one pipeline, while those requiring summarization or interpretation are sent to another. The goal is to ensure that the most relevant and contextually accurate results are retrieved for each query.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  3.0s | üìö 7 chunks | üìù 5781 chars | üéØ top_k=10\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.6385] Introduction... (768 chars)\n",
      "  2. [0.6234] Introduction... (791 chars)\n",
      "  3. [0.5409] Introduction... (737 chars)\n",
      "  4. [0.5236] Introduction... (770 chars)\n",
      "  5. [0.5211] Introduction... (779 chars)\n",
      "  6. [0.5026] Introduction... (751 chars)\n",
      "  7. [0.4804] Introduction... (746 chars)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: 3 additional chunks retrieved but not sent to LLM\n",
      "     (exceeded max_context_chars limit of 6000)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: Query routing involves directing queries to specific pipelines based on their content and intent, enabling a RAG (Relevance-Aware Retrieval) system to handle diverse scenarios effectively.\n",
      "\n",
      "In other words, query routing allows an AI agent to analyze each user's query and choose the best retrieval method or processing pipeline to provide an accurate response. This often requires implementing multi-index strategies, where different types of information are organized into separate, specialized indexes optimized for specific queries.\n",
      "\n",
      "For example, fact-based questions may be routed to one pipeline, while those requiring summarization or interpretation are sent to another. The goal is to ensure that the most relevant and contextually accurate results are retrieved for each query.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6385\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.1691\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6385\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0315\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ transformation refines and expands unclear, complex, or ambiguous user queries to improve the\n",
      "\n",
      "quality of search results.\n",
      "\n",
      "Query Rewriting involves reformulating the original user query to make it more suitable for\n",
      "\n",
      "retrieval. This is particularly useful in scenarios where user queries are not optim...\n",
      "‚îÇ [Truncated - 768 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6234\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.0409\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6234\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0304\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Query routing is a technique that directs queries to specific pipelines based on their content\n",
      "\n",
      "and intent, enabling a RAG system to handle diverse scenarios effectively. It works by analyzing\n",
      "\n",
      "each query and choosing the best retrieval method or processing pipeline to provide an\n",
      "\n",
      "accurate response....\n",
      "‚îÇ [Truncated - 791 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5409\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.1316\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5409\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0308\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "Query Decomposition\n",
      "\n",
      "Query decomposition is a technique that breaks down complex queries into simpler sub\n",
      "\n",
      "queries. This is useful for answering multifaceted questions requiring diverse information\n",
      "\n",
      "sources, leading to more precise and relevant search result...\n",
      "‚îÇ [Truncated - 737 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5236\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.0449\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5236\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0301\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ leverage a retrieve-and-rerank pipeline. A retrieve-and-rerank pipeline combines the speed of\n",
      "\n",
      "vector search with the contextual richness of a re-ranking model.\n",
      "\n",
      "In vector search, the query and documents are processed separately. First, the documents are\n",
      "\n",
      "pre-indexed. Then, at query time, the query ...\n",
      "‚îÇ [Truncated - 770 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5211\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.9213\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5211\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0291\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ information. This involves using an LLM to generate multiple similar queries based on the user's\n",
      "\n",
      "initial input. These expanded queries are then used in the retrieval process, increasing both the\n",
      "\n",
      "number and relevance of retrieved documents.\n",
      "\n",
      "Note: Due to the increased quantity of retrieved document...\n",
      "‚îÇ [Truncated - 779 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5026\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.8085\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5026\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0287\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ This e-book reviewed advanced RAG techniques that can be applied at various stages of the\n",
      "\n",
      "RAG pipeline to improve retrieval quality and accuracy of generated responses.ÔøΩ\n",
      "\n",
      "p Indexing optimization techniques, like data preprocessing and chunking focus on formatting\n",
      "\n",
      "external data to improve its effic...\n",
      "‚îÇ [Truncated - 751 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4804\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.9971\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4804\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0290\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Agentic RAG functions like a network of specialized agents, each with different expertise. It can\n",
      "\n",
      "choose from various data stores, retrieval strategies (keyword-based, semantic, or hybrid),\n",
      "\n",
      "query transformations (for poorly structured queries), and specialized tools or APIs, such as\n",
      "\n",
      "text-to-SQL c...\n",
      "‚îÇ [Truncated - 746 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 10 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d420ebe597f4b33ae071598605f4e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef35c84df06944bc9c9490ab57b597fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8161431848d0483d9df92162308ed895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.020s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.088s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.006s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.169s (top 10)\n",
      "   ‚úì Total retrieval: 2.282s\n",
      "\n",
      "‚ö†Ô∏è  Stopping at 7 chunks (would exceed 6000 char limit)\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5873 characters\n",
      "Chunks retrieved: 10\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "leverage a retrieve-and-rerank pipeline. A retrieve-and-rerank pipeline combines the speed of\n",
      "\n",
      "vector search with the contextual richness of a re-ranking model.\n",
      "\n",
      "In vector search, the query and documents are processed separately. First, the documents are\n",
      "\n",
      "pre-indexed. Then, at query time, the query is processed, and the documents closest in vector\n",
      "\n",
      "space are retrieved. While vector search is a fast method to retrieve candidates, it can miss\n",
      "\n",
      "contextual nuances.\n",
      "\n",
      "This is where re-ranking models come into play. Because re-ranking models process the query\n",
      "\n",
      "and the documents together at query time, they can capture more contextual nuances.\n",
      "\n",
      "However, they are usually complex and resource-intensive and thus not suitable for first-stage\n",
      "\n",
      "retrieval like vect\n",
      "\n",
      "... [truncated, full context is 5873 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: Explain in detail what is Reranking\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "Reranking, also known as re-ranking or post-retrieval optimization, is a technique used to enhance the quality of generated responses by refining the retrieved documents after they have been selected based on their relevance and similarity to the query. This process involves evaluating the top-ranked documents from the retrieval stage and ranking them again in order of their importance and relevance.\n",
      "\n",
      "In traditional information retrieval systems, the goal is often to retrieve a large number of relevant documents quickly, without necessarily considering the quality or accuracy of those retrieved results. However, for applications such as conversational AI, question-answering, and text summarization, it's essential to prioritize not only speed but also relevance and accuracy.\n",
      "\n",
      "Reranking serves several purposes:\n",
      "\n",
      "1. **Filtering out irrelevant results**: By re-ranking documents based on their content, you can filter out less relevant or noisy information that may have been retrieved during the initial search.\n",
      "2. **Prioritizing most relevant results**: Reranking helps to prioritize the top-ranked documents and ensure they are presented in a clear order of importance, making it easier for users to identify the most accurate answers.\n",
      "3. **Capturing contextual nuances**: Since reranked models process both query and document together at query time, they can capture more nuanced context that might be missed during initial retrieval.\n",
      "\n",
      "Reranking techniques typically involve using machine learning or deep learning models trained on large datasets of labeled examples (e.g., pairs of queries and corresponding documents). These models learn to identify patterns in the data and make predictions about which documents are most relevant for a given query. Some common reranking strategies include:\n",
      "\n",
      "1. **Ranking based on similarity**: Using metrics such as cosine similarity, TF-IDF scores, or other distance measures to rank retrieved documents by their relevance.\n",
      "2. **Classification-based ranking**: Training models to predict whether each document is \"relevant\" or not and then using these predictions for final ranking.\n",
      "3. **Soft-ranking with weights**: Assigning different weights to individual features extracted from the query and documents during reranking, allowing users to control how much importance they assign to specific aspects of relevance.\n",
      "\n",
      "Reranked results can be used in various applications:\n",
      "\n",
      "1. **Conversational AI**: Reranking helps improve response accuracy by ensuring that relevant information is presented clearly.\n",
      "2. **Question-answering systems**: By prioritizing the most accurate answers, reranking enhances overall system performance and user satisfaction.\n",
      "3. **Text summarization**: Reranked results can help generate more coherent summaries of long documents.\n",
      "\n",
      "In summary, reranking plays a crucial role in post-retrieval optimization by refining retrieved information to prioritize relevance, accuracy, and contextuality.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  8.3s | üìö 7 chunks | üìù 5873 chars | üéØ top_k=10\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.5349] Introduction... (770 chars)\n",
      "  2. [0.4680] Introduction... (796 chars)\n",
      "  3. [0.4598] Introduction... (779 chars)\n",
      "  4. [0.4368] Introduction... (768 chars)\n",
      "  5. [0.4170] Introduction... (751 chars)\n",
      "  6. [0.3997] Introduction... (791 chars)\n",
      "  7. [0.3955] Introduction... (779 chars)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: 3 additional chunks retrieved but not sent to LLM\n",
      "     (exceeded max_context_chars limit of 6000)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: Reranking, also known as re-ranking or post-retrieval optimization, is a technique used to enhance the quality of generated responses by refining the retrieved documents after they have been selected based on their relevance and similarity to the query. This process involves evaluating the top-ranked documents from the retrieval stage and ranking them again in order of their importance and relevance.\n",
      "\n",
      "In traditional information retrieval systems, the goal is often to retrieve a large number of relevant documents quickly, without necessarily considering the quality or accuracy of those retrieved results. However, for applications such as conversational AI, question-answering, and text summarization, it's essential to prioritize not only speed but also relevance and accuracy.\n",
      "\n",
      "Reranking serves several purposes:\n",
      "\n",
      "1. **Filtering out irrelevant results**: By re-ranking documents based on their content, you can filter out less relevant or noisy information that may have been retrieved during the initial search.\n",
      "2. **Prioritizing most relevant results**: Reranking helps to prioritize the top-ranked documents and ensure they are presented in a clear order of importance, making it easier for users to identify the most accurate answers.\n",
      "3. **Capturing contextual nuances**: Since reranked models process both query and document together at query time, they can capture more nuanced context that might be missed during initial retrieval.\n",
      "\n",
      "Reranking techniques typically involve using machine learning or deep learning models trained on large datasets of labeled examples (e.g., pairs of queries and corresponding documents). These models learn to identify patterns in the data and make predictions about which documents are most relevant for a given query. Some common reranking strategies include:\n",
      "\n",
      "1. **Ranking based on similarity**: Using metrics such as cosine similarity, TF-IDF scores, or other distance measures to rank retrieved documents by their relevance.\n",
      "2. **Classification-based ranking**: Training models to predict whether each document is \"relevant\" or not and then using these predictions for final ranking.\n",
      "3. **Soft-ranking with weights**: Assigning different weights to individual features extracted from the query and documents during reranking, allowing users to control how much importance they assign to specific aspects of relevance.\n",
      "\n",
      "Reranked results can be used in various applications:\n",
      "\n",
      "1. **Conversational AI**: Reranking helps improve response accuracy by ensuring that relevant information is presented clearly.\n",
      "2. **Question-answering systems**: By prioritizing the most accurate answers, reranking enhances overall system performance and user satisfaction.\n",
      "3. **Text summarization**: Reranked results can help generate more coherent summaries of long documents.\n",
      "\n",
      "In summary, reranking plays a crucial role in post-retrieval optimization by refining retrieved information to prioritize relevance, accuracy, and contextuality.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5349\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.9258\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5349\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0328\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ leverage a retrieve-and-rerank pipeline. A retrieve-and-rerank pipeline combines the speed of\n",
      "\n",
      "vector search with the contextual richness of a re-ranking model.\n",
      "\n",
      "In vector search, the query and documents are processed separately. First, the documents are\n",
      "\n",
      "pre-indexed. Then, at query time, the query ...\n",
      "‚îÇ [Truncated - 770 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4680\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4680\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0251\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ By combining vector search with re-ranking models, you can quickly cast a wide net of potential\n",
      "\n",
      "candidates and then re-order them to improve the quality of relevant context in your prompt.\n",
      "\n",
      "Note that when using a re-ranking model, you should over-retrieve chunks to filter out less\n",
      "\n",
      "relevant ones la...\n",
      "‚îÇ [Truncated - 796 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4598\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.3896\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4598\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0315\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ information. This involves using an LLM to generate multiple similar queries based on the user's\n",
      "\n",
      "initial input. These expanded queries are then used in the retrieval process, increasing both the\n",
      "\n",
      "number and relevance of retrieved documents.\n",
      "\n",
      "Note: Due to the increased quantity of retrieved document...\n",
      "‚îÇ [Truncated - 779 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4368\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4368\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0255\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ transformation refines and expands unclear, complex, or ambiguous user queries to improve the\n",
      "\n",
      "quality of search results.\n",
      "\n",
      "Query Rewriting involves reformulating the original user query to make it more suitable for\n",
      "\n",
      "retrieval. This is particularly useful in scenarios where user queries are not optim...\n",
      "‚îÇ [Truncated - 768 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4170\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4170\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0237\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ This e-book reviewed advanced RAG techniques that can be applied at various stages of the\n",
      "\n",
      "RAG pipeline to improve retrieval quality and accuracy of generated responses.ÔøΩ\n",
      "\n",
      "p Indexing optimization techniques, like data preprocessing and chunking focus on formatting\n",
      "\n",
      "external data to improve its effic...\n",
      "‚îÇ [Truncated - 751 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3997\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3997\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0249\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Query routing is a technique that directs queries to specific pipelines based on their content\n",
      "\n",
      "and intent, enabling a RAG system to handle diverse scenarios effectively. It works by analyzing\n",
      "\n",
      "each query and choosing the best retrieval method or processing pipeline to provide an\n",
      "\n",
      "accurate response....\n",
      "‚îÇ [Truncated - 791 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3955\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.2376\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3955\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0294\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ base model) are iteratively updated through a process called backpropagation to learn from the\n",
      "\n",
      "domain-specific dataset. The result is a fine-tuned LLM that better captures the nuances and\n",
      "\n",
      "requirements of the new data, such as specific terminology, style, or tone.\n",
      "\n",
      "Summary\n",
      "\n",
      "RAG enhances generative ...\n",
      "‚îÇ [Truncated - 779 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 7 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c89c08574ce457f9df33f6196d2d041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e9182d66d34ee587d65cd45e90de6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3d6657d7f84bdc9446921f72b8613f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.018s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.436s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.005s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.102s (top 7)\n",
      "   ‚úì Total retrieval: 2.561s\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 4896 characters\n",
      "Chunks retrieved: 7\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "The generated outputs of LLMs are greatly influenced by the quality, tone, length, and structure of their\n",
      "\n",
      "corresponding prompts. Prompt engineering is the practice of optimizing LLM prompts to improve the quality and\n",
      "\n",
      "accuracy of generated output. Often one of the lowest-hanging fruits when it comes to techniques for improving RAG\n",
      "\n",
      "systems, prompt engineering does not require making changes to the underlying LLM itself. This makes it an efficient\n",
      "\n",
      "and accessible way to enhance performance without complex modifications.\n",
      "\n",
      "There are several different prompting techniques that are especially useful in improving RAG pipelines.\n",
      "\n",
      "...\n",
      "\n",
      "Chain of Thought (CoT) prompting involves asking\n",
      "\n",
      "the model to ‚Äúthink step-by-step‚Äù and break down\n",
      "\n",
      "complex reasoning tasks\n",
      "\n",
      "... [truncated, full context is 4896 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: what is prompt engineering?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "Prompt engineering is the practice of optimizing LLM prompts to improve the quality and accuracy of generated output in Retrieval-Augmented Generation (RAG) systems. It involves designing and refining prompts to elicit more accurate, relevant, and coherent responses from large language models (LLMs). Prompt engineering can be done without modifying the underlying LLM itself, making it an efficient way to enhance performance.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  2.2s | üìö 7 chunks | üìù 4896 chars | üéØ top_k=7\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.6349] Introduction... (777 chars)\n",
      "  2. [0.4337] Introduction... (794 chars)\n",
      "  3. [0.4227] Introduction... (541 chars)\n",
      "  4. [0.4095] Introduction... (732 chars)\n",
      "  5. [0.3949] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (339 chars)\n",
      "  6. [0.3812] **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**... (408 chars)\n",
      "  7. [0.3799] Introduction... (796 chars)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: Prompt engineering is the practice of optimizing LLM prompts to improve the quality and accuracy of generated output in Retrieval-Augmented Generation (RAG) systems. It involves designing and refining prompts to elicit more accurate, relevant, and coherent responses from large language models (LLMs). Prompt engineering can be done without modifying the underlying LLM itself, making it an efficient way to enhance performance.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6349\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      3.6062\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6349\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0328\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ The generated outputs of LLMs are greatly influenced by the quality, tone, length, and structure of their\n",
      "\n",
      "corresponding prompts. Prompt engineering is the practice of optimizing LLM prompts to improve the quality and\n",
      "\n",
      "accuracy of generated output. Often one of the lowest-hanging fruits when it come...\n",
      "‚îÇ [Truncated - 777 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4337\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.3382\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4337\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0311\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ intermediate steps. This can be especially useful\n",
      "\n",
      "when retrieved documents contain conflicting or\n",
      "\n",
      "dense information that requires careful analysis.\n",
      "\n",
      "Tree of Thoughts (ToT) prompting builds on CoT by\n",
      "\n",
      "instructing the model to evaluate its responses at each step\n",
      "\n",
      "in the problem-solving process or ev...\n",
      "‚îÇ [Truncated - 794 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4227\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.1019\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4227\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0300\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ compressing the (retrieved) context, and manipulating the prompt or generative model (LLM).\n",
      "\n",
      "We recommend implementing a validation pipeline to identify which parts of your RAG system\n",
      "\n",
      "need optimization and to assess the effectiveness of advanced techniques. Evaluating your RAG\n",
      "\n",
      "pipeline enables con...\n",
      "‚îÇ [Truncated - 541 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4095\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.3825\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4095\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0308\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Advanced RAG\n",
      "\n",
      "Techniques\n",
      "\n",
      "[A guide on different techniques to improve the ]\n",
      "\n",
      "performance of your Retrieval-Augmented\n",
      "\n",
      "Generation applications.\n",
      "\n",
      "Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "Retrieval-augmented generation (RAG) provides generative large language models (LLMs) with information\n",
      "\n",
      "from an ex...\n",
      "‚îÇ [Truncated - 732 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3949\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3949\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0259\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "Please note that PMO COE intervenes when backups are unavailable and time constraints exist for approval.\n",
      "PMO COE can complete the approvals needed and should provide the reason for approval in the comment\n",
      "box.\n",
      "\n",
      "MPO Timesheet FAQ Version ...\n",
      "‚îÇ [Truncated - 339 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3812\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.0000\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3812\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0263\n",
      "‚îÇ üìç Section: **MPO Timesheet FAQ** > **MPO Timesheet FAQ‚Äôs**\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ [Context: **MPO Timesheet FAQ**]\n",
      "\n",
      "## **MPO Timesheet FAQ‚Äôs**\n",
      "\n",
      "If the task to approved is still not showing on your approval page, kindly submit a ticket using [this link.](https://jira.safeway.com/projects/PMOCOE/issues/PMOCOE-2406?filter=allissues)\n",
      "\n",
      "A9. Status manager is the timesheet approver that...\n",
      "‚îÇ [Truncated - 408 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3799\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      2.4464\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3799\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0311\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ By combining vector search with re-ranking models, you can quickly cast a wide net of potential\n",
      "\n",
      "candidates and then re-order them to improve the quality of relevant context in your prompt.\n",
      "\n",
      "Note that when using a re-ranking model, you should over-retrieve chunks to filter out less\n",
      "\n",
      "relevant ones la...\n",
      "‚îÇ [Truncated - 796 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 10 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df0849c27de4f6390d872d588342675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24858757c2d466299951094fc44aaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc66ea7b77f74a2a8a13fc055e14f7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.014s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.101s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.006s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.222s (top 10)\n",
      "   ‚úì Total retrieval: 2.342s\n",
      "\n",
      "‚ö†Ô∏è  Stopping at 7 chunks (would exceed 6000 char limit)\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5770 characters\n",
      "Chunks retrieved: 10\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "The generated outputs of LLMs are greatly influenced by the quality, tone, length, and structure of their\n",
      "\n",
      "corresponding prompts. Prompt engineering is the practice of optimizing LLM prompts to improve the quality and\n",
      "\n",
      "accuracy of generated output. Often one of the lowest-hanging fruits when it comes to techniques for improving RAG\n",
      "\n",
      "systems, prompt engineering does not require making changes to the underlying LLM itself. This makes it an efficient\n",
      "\n",
      "and accessible way to enhance performance without complex modifications.\n",
      "\n",
      "There are several different prompting techniques that are especially useful in improving RAG pipelines.\n",
      "\n",
      "...\n",
      "\n",
      "Chain of Thought (CoT) prompting involves asking\n",
      "\n",
      "the model to ‚Äúthink step-by-step‚Äù and break down\n",
      "\n",
      "complex reasoning tasks\n",
      "\n",
      "... [truncated, full context is 5770 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: what are the different prompting techniques?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "According to Source 1, there are several different prompting techniques that can be used in Retrieval-Augmented Generation (RAG) systems:\n",
      "\n",
      "* Chain of Thought (CoT) prompting: This involves asking the model to \"think step-by-step\" and break down complex reasoning tasks into a series of intermediate steps.\n",
      "* Tree of Thoughts (ToT) prompting: This builds on CoT by instructing the model to evaluate its responses at each step in the problem-solving process or even generate several different solutions to a problem and choose the best result.\n",
      "\n",
      "These two techniques are mentioned as being especially useful for improving RAG pipelines, particularly when dealing with retrieved documents that contain conflicting or dense information.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  2.7s | üìö 7 chunks | üìù 5770 chars | üéØ top_k=10\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.6602] Introduction... (777 chars)\n",
      "  2. [0.5624] Introduction... (794 chars)\n",
      "  3. [0.5025] Introduction... (796 chars)\n",
      "  4. [0.4799] Introduction... (732 chars)\n",
      "  5. [0.4798] Introduction... (730 chars)\n",
      "  6. [0.4756] Introduction... (751 chars)\n",
      "  7. [0.4678] Introduction... (751 chars)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: 3 additional chunks retrieved but not sent to LLM\n",
      "     (exceeded max_context_chars limit of 6000)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: According to Source 1, there are several different prompting techniques that can be used in Retrieval-Augmented Generation (RAG) systems:\n",
      "\n",
      "* Chain of Thought (CoT) prompting: This involves asking the model to \"think step-by-step\" and break down complex reasoning tasks into a series of intermediate steps.\n",
      "* Tree of Thoughts (ToT) prompting: This builds on CoT by instructing the model to evaluate its responses at each step in the problem-solving process or even generate several different solutions to a problem and choose the best result.\n",
      "\n",
      "These two techniques are mentioned as being especially useful for improving RAG pipelines, particularly when dealing with retrieved documents that contain conflicting or dense information.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.6602\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      3.1284\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.6602\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0328\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ The generated outputs of LLMs are greatly influenced by the quality, tone, length, and structure of their\n",
      "\n",
      "corresponding prompts. Prompt engineering is the practice of optimizing LLM prompts to improve the quality and\n",
      "\n",
      "accuracy of generated output. Often one of the lowest-hanging fruits when it come...\n",
      "‚îÇ [Truncated - 777 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5624\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      2.3919\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5624\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0320\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ intermediate steps. This can be especially useful\n",
      "\n",
      "when retrieved documents contain conflicting or\n",
      "\n",
      "dense information that requires careful analysis.\n",
      "\n",
      "Tree of Thoughts (ToT) prompting builds on CoT by\n",
      "\n",
      "instructing the model to evaluate its responses at each step\n",
      "\n",
      "in the problem-solving process or ev...\n",
      "‚îÇ [Truncated - 794 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.5025\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      1.8845\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.5025\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0315\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ By combining vector search with re-ranking models, you can quickly cast a wide net of potential\n",
      "\n",
      "candidates and then re-order them to improve the quality of relevant context in your prompt.\n",
      "\n",
      "Note that when using a re-ranking model, you should over-retrieve chunks to filter out less\n",
      "\n",
      "relevant ones la...\n",
      "‚îÇ [Truncated - 796 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4799\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      2.7395\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4799\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0318\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ Advanced RAG\n",
      "\n",
      "Techniques\n",
      "\n",
      "[A guide on different techniques to improve the ]\n",
      "\n",
      "performance of your Retrieval-Augmented\n",
      "\n",
      "Generation applications.\n",
      "\n",
      "Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "Retrieval-augmented generation (RAG) provides generative large language models (LLMs) with information\n",
      "\n",
      "from an ex...\n",
      "‚îÇ [Truncated - 732 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4798\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.4911\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4798\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0277\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ isolated sentences or propositions. While highly accurate, it's also\n",
      "\n",
      "the most computationally demanding approach.\n",
      "\n",
      "Each of the discussed techniques has its strengths, and the choice depends on the RAG system's\n",
      "\n",
      "specific requirements and the nature of the documents being processed. New approaches co...\n",
      "‚îÇ [Truncated - 730 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4756\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.3447\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4756\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0261\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ RAG pipelines by enabling LLMs to dynamically interact with\n",
      "\n",
      "retrieved documents, updating reasoning and actions based on\n",
      "\n",
      "external knowledge to provide more accurate and contextually\n",
      "\n",
      "relevant responses.\n",
      "\n",
      "12\n",
      "\n",
      "Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "LLM Fine-Tuning\n",
      "\n",
      "Pre-trained LLMs are trained on...\n",
      "‚îÇ [Truncated - 751 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4678\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.6165\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4678\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0277\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ This e-book reviewed advanced RAG techniques that can be applied at various stages of the\n",
      "\n",
      "RAG pipeline to improve retrieval quality and accuracy of generated responses.ÔøΩ\n",
      "\n",
      "p Indexing optimization techniques, like data preprocessing and chunking focus on formatting\n",
      "\n",
      "external data to improve its effic...\n",
      "‚îÇ [Truncated - 751 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "üí° Query complexity analysis: Using 7 chunks\n",
      "\n",
      "üîç Retrieving relevant chunks...\n",
      "   ‚Ä¢ Corpus size: 61, using k=61 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bb8c56f8f94945ac0c98d082e36588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4e2fe731e7448889777515e19787e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb06a47c081473985a45fe757fb0b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ BM25s: 0.015s (61 results)\n",
      "   ‚Ä¢ ColBERT: 0.115s (61 results)\n",
      "   ‚Ä¢ Fusion: 0.000s (50 candidates)\n",
      "   ‚Ä¢ Fetch: 0.005s (50 chunks)\n",
      "   ‚Ä¢ Rerank: 2.085s (top 7)\n",
      "   ‚úì Total retrieval: 2.220s\n",
      "\n",
      "============================================================\n",
      "üêõ DEBUG: Context being sent to LLM\n",
      "============================================================\n",
      "Context length: 5557 characters\n",
      "Chunks retrieved: 7\n",
      "Chunks actually used: 7\n",
      "\n",
      "First 800 characters of context:\n",
      "=== SOURCE 1 ===\n",
      "Section: Introduction\n",
      "\n",
      "adapt the model's general knowledge to the nuances of that particular domain, improving its\n",
      "\n",
      "performance in that area. Using a fine-tuned model in RAG pipelines can help improve the\n",
      "\n",
      "quality of generated responses, especially when the topic at hand is highly specialized.\n",
      "\n",
      "High-quality domain-specific data is crucial for fine-tuning LLMs. Labeled datasets, like\n",
      "\n",
      "positive and negative customer reviews, can help fine-tuned models better perform\n",
      "\n",
      "downstream tasks like text classification or sentiment analysis. Unlabeled datasets, on the other\n",
      "\n",
      "hand, like the latest articles published on PubMed, can help fine-tuned models gain more\n",
      "\n",
      "domain-specific knowledge and expand their vocabularies.\n",
      "\n",
      "During the fine-tuning process, the model weights of the pre-train\n",
      "\n",
      "... [truncated, full context is 5557 chars]\n",
      "============================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí¨ Question: what is llm fine-tuning?\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "\n",
      "LLM (Large Language Model) fine-tuning involves training a pre-trained LLM on a specific dataset or task to improve its performance in that area, typically for use in Retrieval-Augmented Generation (RAG) pipelines.\n",
      "\n",
      "According to Source 1 and Source 4, fine-tuning an LLM helps it better perform downstream tasks like text classification or sentiment analysis. It also enables the model to gain more domain-specific knowledge and expand its vocabulary when trained on custom datasets.\n",
      "\n",
      "Fine-tuning involves adjusting the model weights of a pre-trained LLM using additional data specific to that task or dataset. This process refines the model's embeddings, allowing it to capture the nuances of the new data and improve performance in RAG applications like retrieval and generation.\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚è±Ô∏è  3.2s | üìö 7 chunks | üìù 5557 chars | üéØ top_k=7\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìñ Sources Used:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  1. [0.4979] Introduction... (789 chars)\n",
      "  2. [0.4088] Introduction... (714 chars)\n",
      "  3. [0.3932] Introduction... (777 chars)\n",
      "  4. [0.3923] Introduction... (751 chars)\n",
      "  5. [0.3727] Introduction... (541 chars)\n",
      "  6. [0.3690] Introduction... (778 chars)\n",
      "  7. [0.3512] Introduction... (768 chars)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "Assistant: LLM (Large Language Model) fine-tuning involves training a pre-trained LLM on a specific dataset or task to improve its performance in that area, typically for use in Retrieval-Augmented Generation (RAG) pipelines.\n",
      "\n",
      "According to Source 1 and Source 4, fine-tuning an LLM helps it better perform downstream tasks like text classification or sentiment analysis. It also enables the model to gain more domain-specific knowledge and expand its vocabulary when trained on custom datasets.\n",
      "\n",
      "Fine-tuning involves adjusting the model weights of a pre-trained LLM using additional data specific to that task or dataset. This process refines the model's embeddings, allowing it to capture the nuances of the new data and improve performance in RAG applications like retrieval and generation.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä Retrieved Chunks with Similarity Scores (7)\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ Chunk 1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4979\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      3.6909\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4979\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0328\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ adapt the model's general knowledge to the nuances of that particular domain, improving its\n",
      "\n",
      "performance in that area. Using a fine-tuned model in RAG pipelines can help improve the\n",
      "\n",
      "quality of generated responses, especially when the topic at hand is highly specialized.\n",
      "\n",
      "High-quality domain-specifi...\n",
      "‚îÇ [Truncated - 789 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.4088\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      3.0069\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.4088\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0320\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ of smaller, domain-specific datasets.\n",
      "\n",
      "Fine-tuning embedding models on custom datasets can significantly improve the quality of\n",
      "\n",
      "embeddings, subsequently improving performance on downstream tasks like RAG. Fine-tuning\n",
      "\n",
      "improves embeddings to better capture the dataset's meaning and context, leading ...\n",
      "‚îÇ [Truncated - 714 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3932\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.9277\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3932\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0290\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ The generated outputs of LLMs are greatly influenced by the quality, tone, length, and structure of their\n",
      "\n",
      "corresponding prompts. Prompt engineering is the practice of optimizing LLM prompts to improve the quality and\n",
      "\n",
      "accuracy of generated output. Often one of the lowest-hanging fruits when it come...\n",
      "‚îÇ [Truncated - 777 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3923\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      3.0129\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3923\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0318\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ RAG pipelines by enabling LLMs to dynamically interact with\n",
      "\n",
      "retrieved documents, updating reasoning and actions based on\n",
      "\n",
      "external knowledge to provide more accurate and contextually\n",
      "\n",
      "relevant responses.\n",
      "\n",
      "12\n",
      "\n",
      "Advanced RAG Techniques | weaviate Ebook\n",
      "\n",
      "LLM Fine-Tuning\n",
      "\n",
      "Pre-trained LLMs are trained on...\n",
      "‚îÇ [Truncated - 751 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3727\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.7448\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3727\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0280\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ compressing the (retrieved) context, and manipulating the prompt or generative model (LLM).\n",
      "\n",
      "We recommend implementing a validation pipeline to identify which parts of your RAG system\n",
      "\n",
      "need optimization and to assess the effectiveness of advanced techniques. Evaluating your RAG\n",
      "\n",
      "pipeline enables con...\n",
      "‚îÇ [Truncated - 541 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3690\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      2.6764\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3690\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0308\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ To fine-tune an existing embedding model you first need to select a base model that you would\n",
      "\n",
      "like to improve. Next, you begin the fine-tuning process by providing the model with your\n",
      "\n",
      "domain-specific data. During this process, the loss function adjusts the model‚Äôs embeddings so\n",
      "\n",
      "that semantically ...\n",
      "‚îÇ [Truncated - 778 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚îå‚îÄ Chunk 7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "‚îÇ üéØ Final Score (ColBERT Rerank): 0.3512\n",
      "‚îÇ üìà Intermediate Scores:\n",
      "‚îÇ    ‚Ä¢ BM25 (lexical):      0.9176\n",
      "‚îÇ    ‚Ä¢ ColBERT (semantic):  0.3512\n",
      "‚îÇ    ‚Ä¢ RRF (fusion):        0.0279\n",
      "‚îÇ üìç Section: Introduction\n",
      "‚îÇ\n",
      "‚îÇ üìÑ Text:\n",
      "‚îÇ transformation refines and expands unclear, complex, or ambiguous user queries to improve the\n",
      "\n",
      "quality of search results.\n",
      "\n",
      "Query Rewriting involves reformulating the original user query to make it more suitable for\n",
      "\n",
      "retrieval. This is particularly useful in scenarios where user queries are not optim...\n",
      "‚îÇ [Truncated - 768 total characters]\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Goodbye! üëã\n",
      "\n",
      "[Returned to main menu]\n",
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Invalid choice. Please enter a number between 1-4.\n",
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Invalid choice. Please enter a number between 1-4.\n",
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n"
     ]
    }
   ],
   "source": [
    "# Initialize config with SEPARATE models for vision and chat\n",
    "# Vision: gemma3:4b (multimodal, for analyzing images)\n",
    "# Chat: gemma3:4b (FASTER BUT prone to hallucinations)\n",
    "# Chat: llama3.2:3b (FASTER and STREAMING - recommended for 16GB RAM Mac Mini M4) -> CURRENT\n",
    "# Note: gpt-oss:20b is available but VERY slow. Use only if you need maximum quality.\n",
    "config = RAGConfig(chat_model='llama3.2:3b')  # Changed from gpt-oss:20b to llama3.2:3b for better performance\n",
    "app = RAGApplication(config)\n",
    "\n",
    "# Check Ollama\n",
    "if not app.check_ollama():\n",
    "    print(\"‚ùå Ollama is not running!\")\n",
    "    print(\"\\nTo start Ollama:\")\n",
    "    print(\"  1. Open a terminal\")\n",
    "    print(\"  2. Run: ollama serve\")\n",
    "    print(\"  3. Keep that terminal open\")\n",
    "    print(\"\\nThen run this cell again.\")\n",
    "else:\n",
    "    # Simple menu with proper exit handling\n",
    "    exit_program = False\n",
    "    \n",
    "    while not exit_program:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAG Chatbot - Choose an option:\")\n",
    "        print(\"1. Upload and index a PDF\")\n",
    "        print(\"2. Start interactive chat\")\n",
    "        print(\"3. Show database statistics\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            file_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "            if os.path.exists(file_path):\n",
    "                app.index_documents([file_path])\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            app.initialize_chatbot()\n",
    "            app.interactive_chat()\n",
    "            # Back to main menu after chat exits\n",
    "            print(\"\\n[Returned to main menu]\")\n",
    "            \n",
    "        elif choice == '3':\n",
    "            app.print_stats()\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Goodbye! üëã\")\n",
    "            print(\"=\"*50)\n",
    "            exit_program = True\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1-4.\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Program exited successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593762a",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "- check if we can change to a bigger model\n",
    "- check system prompt for LLM response (temp as well)\n",
    "- check token limit for LLM response (max 6000) - should be longer\n",
    "- check token limit for ColBERT (max 512) - should be longer\n",
    "- check out different chunking strategies PDF from weaviate\n",
    "- check the RRF (fusion) scoring and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
