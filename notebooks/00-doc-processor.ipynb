{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ef7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Local RAG Chatbot with Image Understanding\\n===================================================\\n\\n✅ No Cloud Dependencies (runs 100% locally)\\n✅ No RAGatouille (direct Jina ColBERT v2 implementation)\\n✅ PyMuPDF4LLM for PDF conversion\\n✅ Image extraction and analysis with LLaVA vision model\\n✅ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\\n✅ Markdown-aware semantic chunking\\n✅ SQLite database for storage\\n\\nRequirements:\\n- Ollama (for LLMs: llama3.2:3b, llava:7b)\\n- Mac Mini M4 or similar (16GB RAM recommended)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Local RAG Chatbot with Image Understanding\n",
    "===================================================\n",
    "\n",
    "✅ No Cloud Dependencies (runs 100% locally)\n",
    "✅ No RAGatouille (direct Jina ColBERT v2 implementation)\n",
    "✅ PyMuPDF4LLM for PDF conversion\n",
    "✅ Image extraction and analysis with LLaVA vision model\n",
    "✅ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\n",
    "✅ Markdown-aware semantic chunking\n",
    "✅ SQLite database for storage\n",
    "\n",
    "Requirements:\n",
    "- Ollama (for LLMs: llama3.2:3b, llava:7b)\n",
    "- Mac Mini M4 or similar (16GB RAM recommended)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f723da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Suppress tokenizers parallelism warning when forking\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress deprecation warnings from transformers/sentence-transformers\n",
    "warnings.filterwarnings('ignore', message='.*torch_dtype.*deprecated.*')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage  # Renamed to avoid conflict with database model\n",
    "\n",
    "# PDF and text processing\n",
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF for image extraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Retrieval\n",
    "import bm25s\n",
    "from bm25s.hf import BM25HF\n",
    "import Stemmer  # PyStemmer for stemming\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "\n",
    "# LLM\n",
    "import requests  # For Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a1fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for local RAG system\"\"\"\n",
    "    # Base directory (set to project root - parent of notebooks folder)\n",
    "    base_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    # Database\n",
    "    db_path: str = None\n",
    "    \n",
    "    # Chunking\n",
    "    min_chunk_size: int = 256\n",
    "    max_chunk_size: int = 512  # Reduced to match model's max_seq_length\n",
    "    chunk_overlap: int = 128\n",
    "    \n",
    "    # Retrieval\n",
    "    bm25_top_k: int = 100\n",
    "    colbert_top_k: int = 100\n",
    "    final_top_k: int = 15  # Increased from 10 to 15 for better coverage and reduced hallucination\n",
    "    \n",
    "    # Models\n",
    "    chat_model: str = \"llama3.2:3b\"\n",
    "    vision_model: str = \"llava:7b\"\n",
    "    embedding_model: str = \"jinaai/jina-colbert-v2\"\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_url: str = \"http://localhost:11434\"\n",
    "    ollama_timeout: int = 300  # Increased timeout for slower models\n",
    "    \n",
    "    # Paths (will be set to absolute paths in __post_init__)\n",
    "    bm25_index_path: str = None\n",
    "    colbert_index_path: str = None\n",
    "    images_dir: str = None\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set absolute paths after initialization\"\"\"\n",
    "        if self.db_path is None:\n",
    "            self.db_path = os.path.join(self.base_dir, \"rag_local.db\")\n",
    "        if self.bm25_index_path is None:\n",
    "            self.bm25_index_path = os.path.join(self.base_dir, \"indexes\", \"bm25s\")\n",
    "        if self.colbert_index_path is None:\n",
    "            self.colbert_index_path = os.path.join(self.base_dir, \"indexes\", \"colbert\")\n",
    "        if self.images_dir is None:\n",
    "            self.images_dir = os.path.join(self.base_dir, \"extracted_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    upload_date = Column(DateTime, default=datetime.utcnow)\n",
    "    total_pages = Column(Integer)\n",
    "    status = Column(String(50))\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = 'images'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    page_number = Column(Integer, nullable=False)\n",
    "    image_path = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    image_type = Column(String(50))\n",
    "    ocr_text = Column(Text)\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = 'chunks'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    text = Column(Text, nullable=False)\n",
    "    heading_path = Column(String(500))\n",
    "    token_count = Column(Integer)\n",
    "    has_images = Column(Boolean, default=False)\n",
    "    chunk_metadata = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA CLIENT WITH STREAMING SUPPORT\n",
    "# ============================================================================\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with Ollama API with streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.ollama_url\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        model: str, \n",
    "        prompt: str, \n",
    "        system: str = \"\",\n",
    "        images: List[str] = None,\n",
    "        timeout: int = 300,\n",
    "        stream: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text with Ollama (with optional streaming)\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                # Streaming mode - print tokens as they arrive\n",
    "                response = requests.post(url, json=payload, timeout=timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"response\" in chunk:\n",
    "                            token = chunk[\"response\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "                        \n",
    "                        # Check if done\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "                \n",
    "                print()  # Newline after streaming\n",
    "                return full_response\n",
    "            else:\n",
    "                # Non-streaming mode - wait for complete response\n",
    "                response = requests.post(url, json=payload, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response.json()[\"response\"]\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\\n❌ Ollama timeout after {timeout}s - model may be too slow or stuck\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Ollama error: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_image(self, image_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Analyze image using Gemma3 multimodal model with enhanced OCR extraction\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"❌ Image not found: {image_path}\")\n",
    "            return {\n",
    "                'description': 'Image not found',\n",
    "                'type': 'error',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "            \n",
    "        # Read image and convert to base64\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                import base64\n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                \n",
    "            # Enhanced prompt for better OCR and context extraction\n",
    "            description_prompt = \"\"\"Analyze this image carefully and provide detailed information:\n",
    "\n",
    "1. TYPE: Classify this image (diagram, flowchart, chart, graph, table, screenshot, architecture diagram, code snippet, formula, etc.)\n",
    "\n",
    "2. DESCRIPTION: Describe what the image shows in 2-3 detailed sentences. Include:\n",
    "   - Main subject/purpose\n",
    "   - Key components or elements\n",
    "   - Relationships between elements (if applicable)\n",
    "   - Colors, arrows, or visual indicators (if relevant)\n",
    "\n",
    "3. TEXT: Extract ALL visible text from the image. This is CRITICAL for search accuracy.\n",
    "   - Include labels, titles, legends, annotations\n",
    "   - Include numbers, percentages, values\n",
    "   - Include code, formulas, equations\n",
    "   - Include any text in tables, boxes, or speech bubbles\n",
    "   - Preserve the order and structure where possible\n",
    "   - If no text is visible, write \"No text visible\"\n",
    "\n",
    "Format your response EXACTLY as follows:\n",
    "TYPE: [type]\n",
    "DESCRIPTION: [description]\n",
    "TEXT: [all extracted text]\"\"\"\n",
    "            \n",
    "            response = self.generate(\n",
    "                model=\"gemma3:4b\",  # Using Gemma3 for image analysis\n",
    "                prompt=description_prompt,\n",
    "                images=[image_data],\n",
    "                timeout=120,  # Increased timeout for image analysis\n",
    "                stream=False  # Don't stream for image analysis\n",
    "            )\n",
    "            \n",
    "            # Parse response with more robust parsing\n",
    "            result = {\n",
    "                'description': 'No description generated',\n",
    "                'type': 'unknown',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "            \n",
    "            if response:\n",
    "                # Try to extract sections using regex\n",
    "                import re\n",
    "                \n",
    "                # Try to find TYPE\n",
    "                type_match = re.search(r'TYPE:\\s*(.+)', response, re.IGNORECASE)\n",
    "                if type_match:\n",
    "                    result['type'] = type_match.group(1).strip().lower()\n",
    "                \n",
    "                # Try to find DESCRIPTION\n",
    "                desc_match = re.search(r'DESCRIPTION:([\\s\\S]*?)(?=TEXT:|$)', response, re.IGNORECASE)\n",
    "                if desc_match:\n",
    "                    result['description'] = desc_match.group(1).strip()\n",
    "                \n",
    "                # Try to find TEXT\n",
    "                text_match = re.search(r'TEXT:([\\s\\S]*)', response, re.IGNORECASE)\n",
    "                if text_match:\n",
    "                    ocr = text_match.group(1).strip()\n",
    "                    # Don't store if it's just the \"no text\" message\n",
    "                    if ocr.lower() != \"no text visible\":\n",
    "                        result['ocr_text'] = ocr\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing image {image_path}: {str(e)}\")\n",
    "            return {\n",
    "                'description': f'Error analyzing image: {str(e)}',\n",
    "                'type': 'error',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "    \n",
    "    def chat(\n",
    "        self, \n",
    "        messages: List[Dict[str, str]], \n",
    "        context: str = None,\n",
    "        stream: bool = True  # Enable streaming by default!\n",
    "    ) -> str:\n",
    "        \"\"\"Chat with context - with EXTREMELY strong anti-hallucination instructions and streaming\"\"\"\n",
    "        \n",
    "        # Build system message with EXTREMELY STRONG anti-hallucination instructions\n",
    "        if context:\n",
    "            system_msg = \"\"\"You are a document question-answering assistant. Follow these rules with ABSOLUTE strictness:\n",
    "\n",
    "!! CRITICAL RULES - NO EXCEPTIONS !!\n",
    "\n",
    "1. You MUST ONLY use information explicitly stated in the context below\n",
    "2. DO NOT use any knowledge outside the provided context\n",
    "3. DO NOT make inferences, assumptions, or educated guesses\n",
    "4. DO NOT mention products, services, or technologies not explicitly in the context\n",
    "5. If information is NOT in the context, respond EXACTLY: \"I don't have that information in the provided documents\"\n",
    "6. DO NOT provide links, URLs, or suggest where to find more information\n",
    "7. DO NOT say things like \"for the latest information\" or \"check the official website\"\n",
    "8. When answering, cite the specific source number (e.g., \"According to Source 2...\")\n",
    "\n",
    "CONTEXT FROM DOCUMENTS:\n",
    "\"\"\" + context + \"\"\"\n",
    "\n",
    "Remember: If it's not in the context above, you DON'T KNOW IT. Period.\"\"\"\n",
    "        else:\n",
    "            system_msg = \"You are a helpful AI assistant. Please provide accurate and helpful responses based only on what you know.\"\n",
    "        \n",
    "        # Build prompt from messages\n",
    "        prompt = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in messages\n",
    "        ])\n",
    "        \n",
    "        return self.generate(\n",
    "            model=self.config.chat_model,\n",
    "            prompt=prompt,\n",
    "            system=system_msg,\n",
    "            timeout=self.config.ollama_timeout,\n",
    "            stream=stream  # Pass streaming flag\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75436919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKDOWN-AWARE SEMANTIC CHUNKER\n",
    "# ============================================================================\n",
    "\n",
    "class MarkdownSemanticChunker:\n",
    "    \"\"\"Intelligent markdown chunking that respects document structure\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def chunk_markdown(self, markdown_text: str, doc_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Create semantically meaningful chunks\"\"\"\n",
    "        sections = self._parse_markdown_hierarchy(markdown_text)\n",
    "        chunks = self._create_chunks_from_sections(sections, doc_context)\n",
    "        optimized_chunks = self._optimize_chunks(chunks)\n",
    "        return optimized_chunks\n",
    "    \n",
    "    def _parse_markdown_hierarchy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse markdown into hierarchical sections\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        heading_stack = []\n",
    "        \n",
    "        for line in lines:\n",
    "            heading_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            \n",
    "            if heading_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                level = len(heading_match.group(1))\n",
    "                title = heading_match.group(2).strip()\n",
    "                \n",
    "                heading_stack = [(lvl, ttl) for lvl, ttl in heading_stack if lvl < level]\n",
    "                heading_stack.append((level, title))\n",
    "                \n",
    "                parent_path = ' > '.join([ttl for _, ttl in heading_stack[:-1]])\n",
    "                full_path = ' > '.join([ttl for _, ttl in heading_stack])\n",
    "                \n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'content': '',\n",
    "                    'parent_path': parent_path,\n",
    "                    'full_path': full_path\n",
    "                }\n",
    "            else:\n",
    "                if current_section is not None:\n",
    "                    current_section['content'] += line + '\\n'\n",
    "                else:\n",
    "                    if not sections or sections[-1]['level'] != 0:\n",
    "                        sections.append({\n",
    "                            'level': 0,\n",
    "                            'title': 'Introduction',\n",
    "                            'content': line + '\\n',\n",
    "                            'parent_path': '',\n",
    "                            'full_path': 'Introduction'\n",
    "                        })\n",
    "                    else:\n",
    "                        sections[-1]['content'] += line + '\\n'\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_chunks_from_sections(self, sections: List[Dict], doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Create chunks from sections\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = None\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text = self._format_section_text(section)\n",
    "            section_tokens = self._count_tokens(section_text)\n",
    "            \n",
    "            if section_tokens > self.config.max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                split_chunks = self._split_large_section(section, doc_context)\n",
    "                chunks.extend(split_chunks)\n",
    "            \n",
    "            elif section_tokens >= self.config.min_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                chunks.append({\n",
    "                    'text': section_text,\n",
    "                    'heading_path': section['full_path'],\n",
    "                    'level': section['level'],\n",
    "                    'token_count': section_tokens,\n",
    "                    'doc_context': doc_context,\n",
    "                    'type': 'section'\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                if current_chunk is None:\n",
    "                    current_chunk = {\n",
    "                        'text': section_text,\n",
    "                        'heading_path': section['parent_path'] or section['title'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': section_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'accumulated',\n",
    "                        'sections': [section['title']]\n",
    "                    }\n",
    "                else:\n",
    "                    combined_text = current_chunk['text'] + '\\n\\n' + section_text\n",
    "                    combined_tokens = self._count_tokens(combined_text)\n",
    "                    \n",
    "                    if combined_tokens <= self.config.max_chunk_size:\n",
    "                        current_chunk['text'] = combined_text\n",
    "                        current_chunk['token_count'] = combined_tokens\n",
    "                        current_chunk['sections'].append(section['title'])\n",
    "                    else:\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = {\n",
    "                            'text': section_text,\n",
    "                            'heading_path': section['parent_path'] or section['title'],\n",
    "                            'level': section['level'],\n",
    "                            'token_count': section_tokens,\n",
    "                            'doc_context': doc_context,\n",
    "                            'type': 'accumulated',\n",
    "                            'sections': [section['title']]\n",
    "                        }\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_large_section(self, section: Dict, doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Split large section at paragraph boundaries\"\"\"\n",
    "        heading_text = f\"# {section['title']}\\n\\n\"\n",
    "        parent_context = f\"Context: {section['parent_path']}\\n\\n\" if section['parent_path'] else \"\"\n",
    "        \n",
    "        paragraphs = re.split(r'\\n\\n+', section['content'].strip())\n",
    "        \n",
    "        chunks = []\n",
    "        current_text = heading_text + parent_context\n",
    "        current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = self._count_tokens(para)\n",
    "            \n",
    "            if current_tokens + para_tokens <= self.config.max_chunk_size:\n",
    "                current_text += para + '\\n\\n'\n",
    "                current_tokens += para_tokens\n",
    "            else:\n",
    "                if current_text.strip() != heading_text.strip():\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': current_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'split_section',\n",
    "                        'part': len(chunks) + 1\n",
    "                    })\n",
    "                \n",
    "                current_text = heading_text + parent_context + para + '\\n\\n'\n",
    "                current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        if current_text.strip():\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'token_count': current_tokens,\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'split_section',\n",
    "                'part': len(chunks) + 1\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Merge very small chunks\"\"\"\n",
    "        optimized = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            chunk = chunks[i]\n",
    "            \n",
    "            if (chunk['token_count'] < self.config.min_chunk_size and \n",
    "                i < len(chunks) - 1):\n",
    "                \n",
    "                next_chunk = chunks[i + 1]\n",
    "                combined_text = chunk['text'] + '\\n\\n' + next_chunk['text']\n",
    "                combined_tokens = self._count_tokens(combined_text)\n",
    "                \n",
    "                if combined_tokens <= self.config.max_chunk_size:\n",
    "                    merged_chunk = {\n",
    "                        'text': combined_text,\n",
    "                        'heading_path': chunk['heading_path'],\n",
    "                        'token_count': combined_tokens,\n",
    "                        'doc_context': chunk['doc_context'],\n",
    "                        'type': 'merged'\n",
    "                    }\n",
    "                    optimized.append(merged_chunk)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            \n",
    "            optimized.append(chunk)\n",
    "            i += 1\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _format_section_text(self, section: Dict) -> str:\n",
    "        \"\"\"Format section with heading and context\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if section['parent_path']:\n",
    "            parts.append(f\"[Context: {section['parent_path']}]\")\n",
    "        \n",
    "        if section['title'] and section['title'] != 'Introduction':\n",
    "            heading_prefix = '#' * section['level']\n",
    "            parts.append(f\"{heading_prefix} {section['title']}\")\n",
    "        \n",
    "        parts.append(section['content'].strip())\n",
    "        \n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text with truncation\"\"\"\n",
    "        return len(self.tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR WITH IMAGE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF processing with image extraction and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.ollama = ollama_client\n",
    "        self.chunker = MarkdownSemanticChunker(config)\n",
    "        \n",
    "        # Create images directory\n",
    "        os.makedirs(config.images_dir, exist_ok=True)\n",
    "    \n",
    "    def _sanitize_utf8(self, text: str) -> str:\n",
    "        \"\"\"Sanitize text to remove invalid UTF-8 characters\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Encode to UTF-8 with error handling, then decode back\n",
    "        # This removes any invalid UTF-8 sequences\n",
    "        try:\n",
    "            # First try strict encoding\n",
    "            return text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️  UTF-8 sanitization error: {e}\")\n",
    "            # Fallback: replace problematic characters\n",
    "            return text.encode('ascii', errors='ignore').decode('ascii', errors='ignore')\n",
    "    \n",
    "    def pdf_to_markdown(self, pdf_path: str) -> str:\n",
    "        \"\"\"Convert PDF to Markdown using PyMuPDF4LLM\"\"\"\n",
    "        markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        # Sanitize to remove invalid UTF-8\n",
    "        return self._sanitize_utf8(markdown_text)\n",
    "    \n",
    "    def _group_nearby_rectangles(self, rects: List[fitz.Rect], proximity_threshold: float = 20) -> List[List[int]]:\n",
    "        \"\"\"Group rectangles that are close to each other\"\"\"\n",
    "        if not rects:\n",
    "            return []\n",
    "\n",
    "        # Each rect gets assigned to a group\n",
    "        groups = []\n",
    "        assigned = [False] * len(rects)\n",
    "\n",
    "        for i, rect in enumerate(rects):\n",
    "            if assigned[i]:\n",
    "                continue\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [i]\n",
    "            assigned[i] = True\n",
    "\n",
    "            # Find all rects that should be in this group\n",
    "            changed = True\n",
    "            while changed:\n",
    "                changed = False\n",
    "                for j, other_rect in enumerate(rects):\n",
    "                    if assigned[j]:\n",
    "                        continue\n",
    "\n",
    "                    # Check if this rect is close to any rect in current group\n",
    "                    for group_idx in current_group:\n",
    "                        group_rect = rects[group_idx]\n",
    "\n",
    "                        # Calculate distance between rectangles\n",
    "                        # Expand each rect by proximity_threshold and check for intersection\n",
    "                        expanded_group = fitz.Rect(\n",
    "                            group_rect.x0 - proximity_threshold,\n",
    "                            group_rect.y0 - proximity_threshold,\n",
    "                            group_rect.x1 + proximity_threshold,\n",
    "                            group_rect.y1 + proximity_threshold\n",
    "                        )\n",
    "\n",
    "                        if expanded_group.intersects(other_rect):\n",
    "                            current_group.append(j)\n",
    "                            assigned[j] = True\n",
    "                            changed = True\n",
    "                            break\n",
    "\n",
    "            groups.append(current_group)\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def extract_images_from_pdf(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        document_id: int,\n",
    "        min_image_size: int = 50,  # Minimum width/height in pixels\n",
    "        proximity_threshold: float = 20  # Group images within this distance (points)\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract images from PDF with intelligent grouping.\n",
    "        Groups nearby images together to capture complete diagrams.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            image_list = page.get_images(full=True)\n",
    "\n",
    "            if not image_list:\n",
    "                continue\n",
    "\n",
    "            # Get bounding boxes for all images on this page\n",
    "            image_bboxes = []\n",
    "            for img_info in image_list:\n",
    "                xref = img_info[0]\n",
    "                # Get all instances of this image on the page\n",
    "                rects = page.get_image_rects(xref)\n",
    "                if rects:\n",
    "                    for rect in rects:\n",
    "                        # Check minimum size\n",
    "                        width = rect.width\n",
    "                        height = rect.height\n",
    "                        if width >= min_image_size and height >= min_image_size:\n",
    "                            image_bboxes.append({\n",
    "                                'rect': rect,\n",
    "                                'xref': xref,\n",
    "                                'width': width,\n",
    "                                'height': height\n",
    "                            })\n",
    "\n",
    "            if not image_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Group nearby images\n",
    "            rects_only = [bbox['rect'] for bbox in image_bboxes]\n",
    "            groups = self._group_nearby_rectangles(rects_only, proximity_threshold)\n",
    "\n",
    "            # Process each group\n",
    "            for group_idx, group in enumerate(groups):\n",
    "                if len(group) == 1:\n",
    "                    # Single image - extract normally\n",
    "                    bbox = image_bboxes[group[0]]\n",
    "                    try:\n",
    "                        base_image = doc.extract_image(bbox['xref'])\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        pil_image = PILImage.open(io.BytesIO(image_bytes))\n",
    "\n",
    "                        # Save image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_img{len(images)+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        if pil_image.mode == 'RGBA':\n",
    "                            pil_image = pil_image.convert('RGB')\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': False,\n",
    "                            'bbox': bbox['rect']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ⚠️  Failed to extract single image on page {page_num+1}: {e}\")\n",
    "\n",
    "                else:\n",
    "                    # Multiple images grouped together - capture as screenshot\n",
    "                    # Calculate bounding box that encompasses all images in group\n",
    "                    union_rect = image_bboxes[group[0]]['rect']\n",
    "                    for idx in group[1:]:\n",
    "                        union_rect = union_rect | image_bboxes[idx]['rect']  # Union of rectangles\n",
    "\n",
    "                    # Add some padding\n",
    "                    padding = 5\n",
    "                    union_rect = fitz.Rect(\n",
    "                        max(0, union_rect.x0 - padding),\n",
    "                        max(0, union_rect.y0 - padding),\n",
    "                        min(page.rect.width, union_rect.x1 + padding),\n",
    "                        min(page.rect.height, union_rect.y1 + padding)\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        # Render this region as an image\n",
    "                        mat = fitz.Matrix(2, 2)  # 2x zoom for better quality\n",
    "                        pix = page.get_pixmap(matrix=mat, clip=union_rect)\n",
    "\n",
    "                        # Convert to PIL Image\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        pil_image = PILImage.open(io.BytesIO(img_data))\n",
    "\n",
    "                        # Save composite image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_composite{group_idx+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': True,\n",
    "                            'num_components': len(group),\n",
    "                            'bbox': union_rect\n",
    "                        })\n",
    "\n",
    "                        print(f\"    📊 Grouped {len(group)} images into composite on page {page_num+1}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ⚠️  Failed to create composite image on page {page_num+1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        return images\n",
    "    \n",
    "    def analyze_images(\n",
    "        self, \n",
    "        images: List[Dict],\n",
    "        document_id: int,\n",
    "        db_session\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Analyze images with vision model and save to database\"\"\"\n",
    "        image_ids = []\n",
    "        \n",
    "        for idx, img_info in enumerate(images):\n",
    "            print(f\"    Analyzing image {idx+1} on page {img_info['page_number']}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Analyze with vision model\n",
    "            analysis = self.ollama.analyze_image(img_info['image_path'])\n",
    "            \n",
    "            # Save to database with UTF-8 sanitization\n",
    "            image_record = Image(\n",
    "                document_id=document_id,\n",
    "                page_number=img_info['page_number'],\n",
    "                image_path=img_info['image_path'],\n",
    "                description=self._sanitize_utf8(analysis['description']),\n",
    "                image_type=self._sanitize_utf8(analysis['type']),\n",
    "                ocr_text=self._sanitize_utf8(analysis['ocr_text'])\n",
    "            )\n",
    "            db_session.add(image_record)\n",
    "            db_session.flush()\n",
    "            \n",
    "            image_ids.append(image_record.id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ ({elapsed:.1f}s)\")\n",
    "        \n",
    "        db_session.commit()\n",
    "        return image_ids\n",
    "    \n",
    "    def enrich_chunks_with_images(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        images_data: List[Dict],\n",
    "        db_session\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Add image context (description + OCR text) to relevant chunks for better search accuracy\"\"\"\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_copy = chunk.copy()\n",
    "            \n",
    "            # Find images that might be relevant to this chunk\n",
    "            # Simple heuristic: chunks that mention visual content keywords\n",
    "            relevant_images = []\n",
    "            \n",
    "            for img in images_data:\n",
    "                if any(keyword in chunk['text'].lower() for keyword in \n",
    "                       ['figure', 'image', 'diagram', 'chart', 'screenshot', 'see below', 'shown in']):\n",
    "                    relevant_images.append(img)\n",
    "            \n",
    "            if relevant_images:\n",
    "                # Build comprehensive image context including OCR text\n",
    "                image_context = \"\\n\\n[Images in this section]:\\n\"\n",
    "                image_metadata = []\n",
    "                \n",
    "                for img in relevant_images:\n",
    "                    # Add type and description\n",
    "                    image_context += f\"- {img['type'].capitalize()}: {img['description']}\\n\"\n",
    "                    \n",
    "                    # CRITICAL: Add OCR text if available (makes text in images searchable!)\n",
    "                    if img.get('ocr_text') and img['ocr_text'].strip():\n",
    "                        image_context += f\"  Text visible in image: {img['ocr_text']}\\n\"\n",
    "                    \n",
    "                    image_metadata.append({\n",
    "                        'path': img['image_path'],\n",
    "                        'description': img['description'],\n",
    "                        'type': img['type'],\n",
    "                        'ocr_text': img.get('ocr_text', '')\n",
    "                    })\n",
    "                \n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'] + image_context)\n",
    "                chunk_copy['has_images'] = True\n",
    "                chunk_copy['image_paths'] = [img['image_path'] for img in relevant_images]\n",
    "                chunk_copy['image_metadata'] = image_metadata\n",
    "            else:\n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'])\n",
    "                chunk_copy['has_images'] = False\n",
    "            \n",
    "            enriched_chunks.append(chunk_copy)\n",
    "        \n",
    "        return enriched_chunks\n",
    "    \n",
    "    def process_document(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        db_session\n",
    "    ) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Convert to markdown\n",
    "        print(\"\\n[Step 1/5] Converting PDF to Markdown...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        markdown_text = self.pdf_to_markdown(pdf_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        print(f\"  • Extracted {len(markdown_text):,} characters\")\n",
    "        \n",
    "        # Create document record\n",
    "        doc = Document(\n",
    "            filename=os.path.basename(pdf_path),\n",
    "            status='processing'\n",
    "        )\n",
    "        db_session.add(doc)\n",
    "        db_session.commit()\n",
    "        \n",
    "        # Step 2: Extract and analyze images\n",
    "        print(\"\\n[Step 2/5] Extracting and analyzing images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        images = self.extract_images_from_pdf(pdf_path, doc.id)\n",
    "        \n",
    "        if images:\n",
    "            image_ids = self.analyze_images(images, doc.id, db_session)\n",
    "            \n",
    "            # Get image data for enrichment\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_record = db_session.query(Image).filter_by(id=img_id).first()\n",
    "                if img_record:\n",
    "                    images_data.append({\n",
    "                        'image_path': img_record.image_path,\n",
    "                        'description': img_record.description,\n",
    "                        'type': img_record.image_type,\n",
    "                        'ocr_text': img_record.ocr_text\n",
    "                    })\n",
    "        else:\n",
    "            images_data = []\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "        print(f\"  • Extracted {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"  • Vision analysis: ✓\")\n",
    "        \n",
    "        # Step 3: Markdown-aware semantic chunking\n",
    "        print(\"\\n[Step 3/5] Markdown-aware semantic chunking...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        doc_context = f\"Document: {os.path.basename(pdf_path)}\\n\\n{markdown_text[:500]}\"\n",
    "        chunks = self.chunker.chunk_markdown(markdown_text, doc_context)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        print(f\"  • Created {len(chunks)} semantic chunks\")\n",
    "        \n",
    "        # Step 4: Enrich chunks with image context (INCLUDING OCR TEXT!)\n",
    "        print(\"\\n[Step 4/5] Enriching chunks with image context...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        if images_data:\n",
    "            chunks = self.enrich_chunks_with_images(chunks, images_data, db_session)\n",
    "            chunks_with_images = sum(1 for c in chunks if c.get('has_images', False))\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ {elapsed:.2f}s\")\n",
    "            print(f\"  • {chunks_with_images} chunks enriched with image context + OCR text\")\n",
    "        else:\n",
    "            # Still sanitize even if no images\n",
    "            for chunk in chunks:\n",
    "                chunk['text'] = self._sanitize_utf8(chunk['text'])\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ {elapsed:.2f}s\")\n",
    "            print(f\"  • No images to enrich\")\n",
    "        \n",
    "        # Step 5: Save to database\n",
    "        print(\"\\n[Step 5/5] Saving chunks to database...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_record = Chunk(\n",
    "                document_id=doc.id,\n",
    "                chunk_index=idx,\n",
    "                text=self._sanitize_utf8(chunk['text']),  # Sanitize before saving\n",
    "                heading_path=chunk.get('heading_path', ''),\n",
    "                token_count=chunk.get('token_count', 0),\n",
    "                has_images=chunk.get('has_images', False),\n",
    "                chunk_metadata=json.dumps({\n",
    "                    k: v for k, v in chunk.items() \n",
    "                    if k not in ['text', 'heading_path', 'token_count', 'has_images']\n",
    "                })\n",
    "            )\n",
    "            db_session.add(chunk_record)\n",
    "        \n",
    "        doc.status = 'indexed'\n",
    "        db_session.commit()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        \n",
    "        return chunks, doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08725b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JINA COLBERT V2 RETRIEVER (NO RAGATOUILLE!)\n",
    "# ============================================================================\n",
    "\n",
    "class JinaColBERTRetriever:\n",
    "    \"\"\"Direct implementation of Jina ColBERT v2 (no RAGatouille dependency)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = SentenceTransformer(\n",
    "            config.embedding_model,\n",
    "            trust_remote_code=True,\n",
    "            device=config.device\n",
    "        )\n",
    "        # Set max sequence length to avoid truncation warnings\n",
    "        self.model.max_seq_length = 512\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Index corpus with ColBERT embeddings\"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        print(f\"  Encoding {len(corpus)} documents...\")\n",
    "        \n",
    "        # Encode corpus (this gives us token-level embeddings)\n",
    "        # Truncate long sequences to avoid errors\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        os.makedirs(self.config.colbert_index_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'embeddings': self.corpus_embeddings,\n",
    "            'corpus': corpus\n",
    "        }, os.path.join(self.config.colbert_index_path, 'index.pt'))\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.config.colbert_index_path, 'index.pt')\n",
    "        data = torch.load(index_file, map_location=self.config.device)\n",
    "        self.corpus_embeddings = data['embeddings']\n",
    "        self.corpus = data['corpus']\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using MaxSim scoring\"\"\"\n",
    "        if not self.corpus or len(self.corpus) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Handle single item corpus\n",
    "        if len(self.corpus) == 1:\n",
    "            return [{\n",
    "                'document_id': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'text': self.corpus[0]\n",
    "            }]\n",
    "        \n",
    "        # Get top-k\n",
    "        k = min(k, len(scores))\n",
    "        top_k_indices = torch.topk(scores, k=k).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'document_id': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'text': self.corpus[idx] if self.corpus else None\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank documents with more accurate scoring\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Encode query and documents\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = self.model.encode(\n",
    "            documents, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, doc_embeddings)\n",
    "        \n",
    "        # Handle single document\n",
    "        if len(documents) == 1:\n",
    "            return [{\n",
    "                'result_index': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'rank': 1,\n",
    "                'text': documents[0]\n",
    "            }]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(sorted_indices[:k]):\n",
    "            results.append({\n",
    "                'result_index': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'rank': rank + 1,\n",
    "                'text': documents[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _maxsim_score(\n",
    "        self, \n",
    "        query_embedding: torch.Tensor, \n",
    "        doc_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MaxSim score between query and documents\n",
    "        \n",
    "        MaxSim: For each query token, find max similarity with all doc tokens,\n",
    "        then average across query tokens\n",
    "        \"\"\"\n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 1:\n",
    "            query_embedding = query_embedding.unsqueeze(0)\n",
    "        if doc_embeddings.dim() == 1:\n",
    "            doc_embeddings = doc_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # For 2D embeddings (single vector per doc), compute cosine similarity directly\n",
    "        if query_embedding.dim() == 2 and doc_embeddings.dim() == 2:\n",
    "            # Normalize embeddings\n",
    "            query_norm = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            doc_norm = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            scores = torch.mm(query_norm, doc_norm.t())\n",
    "            \n",
    "            # Return as 1D tensor\n",
    "            return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()\n",
    "        \n",
    "        # For 3D embeddings (token-level), use mean pooling\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_vec = query_embedding.mean(dim=1)\n",
    "        else:\n",
    "            query_vec = query_embedding\n",
    "            \n",
    "        if doc_embeddings.dim() == 3:\n",
    "            doc_vec = doc_embeddings.mean(dim=1)\n",
    "        else:\n",
    "            doc_vec = doc_embeddings\n",
    "        \n",
    "        # Normalize\n",
    "        query_vec = torch.nn.functional.normalize(query_vec, p=2, dim=-1)\n",
    "        doc_vec = torch.nn.functional.normalize(doc_vec, p=2, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        if query_vec.dim() == 1:\n",
    "            query_vec = query_vec.unsqueeze(0)\n",
    "        if doc_vec.dim() == 1:\n",
    "            doc_vec = doc_vec.unsqueeze(0)\n",
    "            \n",
    "        scores = torch.mm(query_vec, doc_vec.t())\n",
    "        \n",
    "        # Return as 1D tensor\n",
    "        return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUAL INDEXER (BM25s + Jina ColBERT)\n",
    "# ============================================================================\n",
    "\n",
    "class DualIndexer:\n",
    "    \"\"\"Manages BM25s and Jina ColBERT v2 indexes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.bm25_retriever = None\n",
    "        self.colbert_retriever = JinaColBERTRetriever(config)\n",
    "    \n",
    "    def build_bm25_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build BM25s index\"\"\"\n",
    "        print(\"\\n[BM25s] Building lexical search index...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create stemmer\n",
    "        stemmer = Stemmer.Stemmer(\"english\")\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        corpus_tokens = bm25s.tokenize(\n",
    "            corpus, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=stemmer\n",
    "        )\n",
    "        \n",
    "        self.bm25_retriever = bm25s.BM25()\n",
    "        self.bm25_retriever.index(corpus_tokens)\n",
    "        \n",
    "        os.makedirs(self.config.bm25_index_path, exist_ok=True)\n",
    "        self.bm25_retriever.save(self.config.bm25_index_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "    \n",
    "    def build_colbert_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build Jina ColBERT v2 index\"\"\"\n",
    "        print(\"\\n[ColBERT] Building semantic search index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.colbert_retriever.index(corpus)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ {elapsed:.2f}s\")\n",
    "    \n",
    "    def load_indexes(self) -> None:\n",
    "        \"\"\"Load indexes from disk\"\"\"\n",
    "        self.bm25_retriever = bm25s.BM25.load(self.config.bm25_index_path)\n",
    "        self.colbert_retriever.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID RETRIEVER WITH RRF AND RERANKING\n",
    "# ============================================================================\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Three-stage retrieval: BM25s + ColBERT + ColBERT Reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, indexer: DualIndexer, db_session, corpus_to_chunk_id: List[int] = None):\n",
    "        self.config = config\n",
    "        self.indexer = indexer\n",
    "        self.db_session = db_session\n",
    "        self.stemmer = Stemmer.Stemmer(\"english\")\n",
    "        # CRITICAL: Mapping from corpus index to database chunk ID\n",
    "        self.corpus_to_chunk_id = corpus_to_chunk_id or []\n",
    "    \n",
    "    def retrieve(self, query: str, top_k_final: int = None) -> List[Dict]:\n",
    "        \"\"\"Three-stage hybrid retrieval with detailed scoring\"\"\"\n",
    "        if top_k_final is None:\n",
    "            top_k_final = self.config.final_top_k\n",
    "        \n",
    "        print(f\"\\n🔍 Retrieving relevant chunks...\")\n",
    "        \n",
    "        # Get corpus size to adjust k values\n",
    "        corpus_size = len(self.indexer.colbert_retriever.corpus) if self.indexer.colbert_retriever.corpus else 0\n",
    "        \n",
    "        # Adjust k values based on corpus size\n",
    "        bm25_k = min(self.config.bm25_top_k, corpus_size) if corpus_size > 0 else self.config.bm25_top_k\n",
    "        colbert_k = min(self.config.colbert_top_k, corpus_size) if corpus_size > 0 else self.config.colbert_top_k\n",
    "        \n",
    "        print(f\"   • Corpus size: {corpus_size}, using k={bm25_k} for retrieval\")\n",
    "        \n",
    "        # Stage 1: BM25s\n",
    "        start = time.time()\n",
    "        bm25_results = self._bm25_search(query, k=bm25_k)\n",
    "        bm25_time = time.time() - start\n",
    "        print(f\"   • BM25s: {bm25_time:.3f}s ({len(bm25_results)} results)\")\n",
    "        \n",
    "        # Stage 2: ColBERT\n",
    "        start = time.time()\n",
    "        colbert_results = self._colbert_search(query, k=colbert_k)\n",
    "        colbert_time = time.time() - start\n",
    "        print(f\"   • ColBERT: {colbert_time:.3f}s ({len(colbert_results)} results)\")\n",
    "        \n",
    "        # Fusion\n",
    "        start = time.time()\n",
    "        fused_results = self._reciprocal_rank_fusion(bm25_results, colbert_results)\n",
    "        candidates = fused_results[:min(50, len(fused_results))]\n",
    "        fusion_time = time.time() - start\n",
    "        print(f\"   • Fusion: {fusion_time:.3f}s ({len(candidates)} candidates)\")\n",
    "        \n",
    "        # Fetch chunks - USING THE MAPPING!\n",
    "        start = time.time()\n",
    "        candidate_corpus_indices = [r['corpus_index'] for r in candidates]\n",
    "        candidate_chunks = self._fetch_chunks_from_db(candidate_corpus_indices)\n",
    "        \n",
    "        # PRESERVE INTERMEDIATE SCORES\n",
    "        # Map corpus_index to intermediate scores\n",
    "        score_map = {}\n",
    "        for bm25_result in bm25_results:\n",
    "            idx = bm25_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['bm25_score'] = bm25_result['score']\n",
    "        \n",
    "        for colbert_result in colbert_results:\n",
    "            idx = colbert_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['colbert_score'] = colbert_result['score']\n",
    "        \n",
    "        for fused_result in candidates:\n",
    "            idx = fused_result['corpus_index']\n",
    "            if idx in score_map:\n",
    "                score_map[idx]['rrf_score'] = fused_result['rrf_score']\n",
    "        \n",
    "        # Add intermediate scores to chunks\n",
    "        for i, chunk in enumerate(candidate_chunks):\n",
    "            corpus_idx = candidate_corpus_indices[i]\n",
    "            if corpus_idx in score_map:\n",
    "                chunk['intermediate_scores'] = score_map[corpus_idx]\n",
    "        \n",
    "        fetch_time = time.time() - start\n",
    "        print(f\"   • Fetch: {fetch_time:.3f}s ({len(candidate_chunks)} chunks)\")\n",
    "        \n",
    "        # Stage 3: Rerank\n",
    "        start = time.time()\n",
    "        final_k = min(top_k_final, len(candidate_chunks))\n",
    "        reranked_results = self._colbert_rerank(query, candidate_chunks, top_k=final_k)\n",
    "        rerank_time = time.time() - start\n",
    "        print(f\"   • Rerank: {rerank_time:.3f}s (top {len(reranked_results)})\")\n",
    "        \n",
    "        total_time = bm25_time + colbert_time + fusion_time + fetch_time + rerank_time\n",
    "        print(f\"   ✓ Total retrieval: {total_time:.3f}s\")\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 1: BM25s lexical search\"\"\"\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            query, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=self.stemmer\n",
    "        )\n",
    "        \n",
    "        results, scores = self.indexer.bm25_retriever.retrieve(query_tokens, k=k)\n",
    "        \n",
    "        return [\n",
    "            {'corpus_index': int(results[0][i]), 'score': float(scores[0][i]), 'source': 'bm25'}\n",
    "            for i in range(len(results[0]))\n",
    "        ]\n",
    "    \n",
    "    def _colbert_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 2: ColBERT semantic search\"\"\"\n",
    "        results = self.indexer.colbert_retriever.search(query=query, k=k)\n",
    "        return [\n",
    "            {'corpus_index': r['document_id'], 'score': r['score'], 'source': 'colbert'}\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def _reciprocal_rank_fusion(\n",
    "        self, \n",
    "        bm25_results: List[Dict], \n",
    "        colbert_results: List[Dict],\n",
    "        k: int = 60\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"RRF fusion\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for rank, result in enumerate(bm25_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        for rank, result in enumerate(colbert_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{'corpus_index': idx, 'rrf_score': score} for idx, score in sorted_results]\n",
    "    \n",
    "    def _fetch_chunks_from_db(self, corpus_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Fetch chunks from database using corpus index -> chunk ID mapping\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for corpus_idx in corpus_indices:\n",
    "            # Convert corpus index to database chunk ID\n",
    "            if corpus_idx < len(self.corpus_to_chunk_id):\n",
    "                chunk_id = self.corpus_to_chunk_id[corpus_idx]\n",
    "                \n",
    "                # Fetch from database using the actual chunk ID\n",
    "                chunk = self.db_session.query(Chunk).filter_by(id=chunk_id).first()\n",
    "                if chunk:\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk.id,\n",
    "                        'text': chunk.text,\n",
    "                        'document_id': chunk.document_id,\n",
    "                        'heading_path': chunk.heading_path,\n",
    "                        'has_images': chunk.has_images,\n",
    "                        'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Chunk ID {chunk_id} not found in database\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Corpus index {corpus_idx} out of range (max: {len(self.corpus_to_chunk_id)-1})\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _colbert_rerank(self, query: str, chunks: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 3: ColBERT reranking with score preservation\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        reranked_results = self.indexer.colbert_retriever.rerank(query=query, documents=documents, k=top_k)\n",
    "        \n",
    "        final_results = []\n",
    "        for result in reranked_results:\n",
    "            original_chunk = chunks[result['result_index']]\n",
    "            intermediate_scores = original_chunk.get('intermediate_scores', {})\n",
    "            \n",
    "            final_results.append({\n",
    "                'chunk_id': original_chunk['chunk_id'],\n",
    "                'text': original_chunk['text'],\n",
    "                'document_id': original_chunk['document_id'],\n",
    "                'heading_path': original_chunk.get('heading_path', ''),\n",
    "                'has_images': original_chunk.get('has_images', False),\n",
    "                'metadata': original_chunk['metadata'],\n",
    "                'score': result['score'],  # Final ColBERT rerank score (cosine similarity)\n",
    "                'rank': result['rank'],\n",
    "                'bm25_score': intermediate_scores.get('bm25_score', 0.0),\n",
    "                'colbert_score': intermediate_scores.get('colbert_score', 0.0),\n",
    "                'rrf_score': intermediate_scores.get('rrf_score', 0.0)\n",
    "            })\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAG CHATBOT WITH STREAMING\n",
    "# ============================================================================\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Complete RAG chatbot with Ollama and streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, retriever: HybridRetriever, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.ollama = ollama_client\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, query: str, stream: bool = True) -> Dict:\n",
    "        \"\"\"Process user query and generate response with streaming\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retriever.retrieve(query)\n",
    "        \n",
    "        # Build context\n",
    "        context = self._build_context(retrieved_chunks)\n",
    "        \n",
    "        # Generate response with streaming\n",
    "        if stream:\n",
    "            print(f\"\\n🤖 Generating response (streaming)...\\n\")\n",
    "        else:\n",
    "            print(f\"\\n🤖 Generating response...\", end=' ')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        })\n",
    "        \n",
    "        response = self.ollama.chat(\n",
    "            messages=self.conversation_history,\n",
    "            context=context,\n",
    "            stream=stream\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if not stream:\n",
    "            print(f\"✓ {elapsed:.1f}s\")\n",
    "        else:\n",
    "            print(f\"\\n⏱️  Response generated in {elapsed:.1f}s\")\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': response\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': self._format_sources(retrieved_chunks),\n",
    "            'retrieved_chunks': len(retrieved_chunks)\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"Build context from retrieved chunks\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            heading = f\" ({chunk['heading_path']})\" if chunk.get('heading_path') else \"\"\n",
    "            \n",
    "            # Add image info if present\n",
    "            image_info = \"\"\n",
    "            if chunk.get('has_images') and chunk.get('metadata', {}).get('image_paths'):\n",
    "                num_images = len(chunk['metadata']['image_paths'])\n",
    "                image_info = f\" [Contains {num_images} image(s)]\"\n",
    "            \n",
    "            context_parts.append(f\"[Source {i}{heading}{image_info}]\\n{chunk['text']}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format source citations with full text, image paths, and ALL scores\"\"\"\n",
    "        sources = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            source = {\n",
    "                'source_id': i + 1,\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'document_id': chunk['document_id'],\n",
    "                'heading': chunk.get('heading_path', ''),\n",
    "                'score': chunk['score'],  # Final ColBERT rerank score\n",
    "                'bm25_score': chunk.get('bm25_score', 0.0),\n",
    "                'colbert_score': chunk.get('colbert_score', 0.0),\n",
    "                'rrf_score': chunk.get('rrf_score', 0.0),\n",
    "                'has_images': chunk.get('has_images', False),\n",
    "                'text': chunk['text'],  # Include full text\n",
    "                'preview': chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "            }\n",
    "            \n",
    "            # Add image paths if available\n",
    "            if chunk.get('has_images') and chunk.get('metadata'):\n",
    "                image_paths = chunk['metadata'].get('image_paths', [])\n",
    "                source['image_paths'] = image_paths\n",
    "            \n",
    "            sources.append(source)\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"🗑️  Conversation history cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    \"\"\"Main application orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Database setup\n",
    "        db_url = f\"sqlite:///{config.db_path}\"\n",
    "        self.engine = create_engine(db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.db_session = Session()\n",
    "        \n",
    "        # Initialize Ollama client\n",
    "        self.ollama = OllamaClient(config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.processor = DocumentProcessor(config, self.ollama)\n",
    "        self.indexer = DualIndexer(config)\n",
    "        self.retriever = None\n",
    "        self.chatbot = None\n",
    "        \n",
    "        # CRITICAL: Store mapping between corpus index and chunk IDs\n",
    "        self.corpus_to_chunk_id = []  # Maps corpus index -> database chunk ID\n",
    "    \n",
    "    def check_ollama(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.ollama_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def index_documents(self, pdf_paths: List[str]) -> None:\n",
    "        \"\"\"Index PDF documents\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"❌ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            chunks, doc_id = self.processor.process_document(pdf_path, self.db_session)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Building Indexes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build corpus and mapping\n",
    "        # CRITICAL FIX: Store the mapping between corpus index and database chunk IDs\n",
    "        all_db_chunks = self.db_session.query(Chunk).order_by(Chunk.id).all()\n",
    "        corpus = []\n",
    "        self.corpus_to_chunk_id = []\n",
    "        \n",
    "        for chunk in all_db_chunks:\n",
    "            corpus.append(chunk.text)\n",
    "            self.corpus_to_chunk_id.append(chunk.id)\n",
    "        \n",
    "        print(f\"  • Corpus: {len(corpus)} chunks\")\n",
    "        print(f\"  • Chunk ID mapping: {len(self.corpus_to_chunk_id)} entries\")\n",
    "        \n",
    "        # Build indexes\n",
    "        self.indexer.build_bm25_index(corpus)\n",
    "        self.indexer.build_colbert_index(corpus)\n",
    "        \n",
    "        # Save the mapping to disk for later use\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        os.makedirs(os.path.dirname(mapping_path), exist_ok=True)\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(self.corpus_to_chunk_id, f)\n",
    "        \n",
    "        print(f\"\\n✅ Document indexed successfully!\")\n",
    "    \n",
    "    def initialize_chatbot(self) -> None:\n",
    "        \"\"\"Initialize chatbot with existing indexes\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"❌ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading indexes...\")\n",
    "        self.indexer.load_indexes()\n",
    "        \n",
    "        # Load the corpus-to-chunk-id mapping\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        try:\n",
    "            with open(mapping_path, 'rb') as f:\n",
    "                self.corpus_to_chunk_id = pickle.load(f)\n",
    "            print(f\"  • Loaded {len(self.corpus_to_chunk_id)} chunk ID mappings\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"  ⚠️  Warning: No corpus mapping found. Please re-index your documents.\")\n",
    "            self.corpus_to_chunk_id = []\n",
    "        \n",
    "        self.retriever = HybridRetriever(self.config, self.indexer, self.db_session, self.corpus_to_chunk_id)\n",
    "        self.chatbot = RAGChatbot(self.config, self.retriever, self.ollama)\n",
    "        \n",
    "        print(\"✅ Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Chat interface\"\"\"\n",
    "        if not self.chatbot:\n",
    "            raise RuntimeError(\"Chatbot not initialized. Call initialize_chatbot() first.\")\n",
    "        \n",
    "        return self.chatbot.chat(query)\n",
    "    \n",
    "    def _filter_relevant_images(self, query: str, image_paths: List[str], chunk_text: str) -> List[str]:\n",
    "        \"\"\"Filter images to only show those DIRECTLY relevant to the user's query - STRICT filtering\"\"\"\n",
    "        if not image_paths:\n",
    "            return []\n",
    "        \n",
    "        relevant_images = []\n",
    "        \n",
    "        # Extract meaningful query keywords (remove stop words)\n",
    "        stop_words = {'what', 'is', 'are', 'the', 'a', 'an', 'how', 'why', 'when', 'where', \n",
    "                      'can', 'could', 'would', 'should', 'do', 'does', 'did', 'of', 'in', 'on',\n",
    "                      'for', 'to', 'with', 'by', 'from', 'at', 'about', 'as', 'into', 'through',\n",
    "                      'diagram', 'chart', 'figure', 'image', 'screenshot', 'show', 'me', 'please'}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        query_words = [w for w in query_lower.split() if w not in stop_words and len(w) > 2]\n",
    "        \n",
    "        if not query_words:\n",
    "            return []  # No meaningful query words, don't show images\n",
    "        \n",
    "        # Get image metadata from database\n",
    "        for img_path in image_paths:\n",
    "            # Extract just the filename for DB lookup\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # Look up image in database to get description\n",
    "            img_record = self.db_session.query(Image).filter(\n",
    "                Image.image_path.like(f\"%{img_filename}\")\n",
    "            ).first()\n",
    "            \n",
    "            if img_record:\n",
    "                # Combine all image metadata\n",
    "                desc_lower = (img_record.description or \"\").lower()\n",
    "                img_type_lower = (img_record.image_type or \"\").lower()\n",
    "                ocr_lower = (img_record.ocr_text or \"\").lower()\n",
    "                \n",
    "                # Create searchable text from image\n",
    "                image_text = f\"{desc_lower} {img_type_lower} {ocr_lower}\"\n",
    "                image_words = [w for w in image_text.split() if w not in stop_words and len(w) > 2]\n",
    "                \n",
    "                # Calculate meaningful overlap\n",
    "                query_set = set(query_words)\n",
    "                image_set = set(image_words)\n",
    "                overlap = query_set.intersection(image_set)\n",
    "                \n",
    "                # STRICT CRITERIA: Need at least 3 meaningful word overlaps\n",
    "                # This ensures the image is actually about what the user asked\n",
    "                if len(overlap) >= 3:\n",
    "                    relevant_images.append(img_path)\n",
    "                    # print(f\"  DEBUG: Image matched with {len(overlap)} overlaps: {overlap}\")\n",
    "        \n",
    "        return relevant_images\n",
    "    \n",
    "    def _display_chunk_with_images(self, chunk_text: str, image_paths: List[str] = None) -> None:\n",
    "        \"\"\"Display chunk text and associated images\"\"\"\n",
    "        from IPython.display import display, Image as IPImage\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"{chunk_text}\\n\")\n",
    "        \n",
    "        # Display images if available\n",
    "        if image_paths:\n",
    "            print(f\"  📷 Relevant Images ({len(image_paths)}):\")\n",
    "            for img_path in image_paths:\n",
    "                if os.path.exists(img_path):\n",
    "                    try:\n",
    "                        display(IPImage(filename=img_path, width=400))\n",
    "                        print(f\"  └─ {os.path.basename(img_path)}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  └─ ⚠️ Could not display {os.path.basename(img_path)}: {e}\\n\")\n",
    "                else:\n",
    "                    print(f\"  └─ ⚠️ Image not found: {os.path.basename(img_path)}\\n\")\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAG Chatbot - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Type your questions (or 'exit' to quit, 'clear' to clear history)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"\\nGoodbye! 👋\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'clear':\n",
    "                    self.chatbot.clear_history()\n",
    "                    continue\n",
    "                \n",
    "                result = self.chat(user_input)\n",
    "                print(f\"\\nAssistant: {result['response']}\\n\")\n",
    "                \n",
    "                # Show retrieved chunks with ALL SCORES\n",
    "                if result['sources']:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"📊 Retrieved Chunks with Similarity Scores ({len(result['sources'])})\")\n",
    "                    print(f\"{'='*60}\\n\")\n",
    "                    \n",
    "                    for idx, src in enumerate(result['sources'], 1):\n",
    "                        print(f\"┌─ Chunk {idx} {'─'*50}\")\n",
    "                        \n",
    "                        # Show ALL retrieval scores\n",
    "                        print(f\"│ 🎯 Final Score (ColBERT Rerank): {src['score']:.4f}\")\n",
    "                        print(f\"│ 📈 Intermediate Scores:\")\n",
    "                        print(f\"│    • BM25 (lexical):      {src.get('bm25_score', 0.0):.4f}\")\n",
    "                        print(f\"│    • ColBERT (semantic):  {src.get('colbert_score', 0.0):.4f}\")\n",
    "                        print(f\"│    • RRF (fusion):        {src.get('rrf_score', 0.0):.4f}\")\n",
    "                        \n",
    "                        if src['heading']:\n",
    "                            print(f\"│ 📍 Section: {src['heading']}\")\n",
    "                        \n",
    "                        if src['has_images']:\n",
    "                            print(f\"│ 🖼️  Contains Images: Yes\")\n",
    "                        \n",
    "                        print(f\"│\")\n",
    "                        print(f\"│ 📄 Text:\")\n",
    "                        \n",
    "                        # Display chunk text (show first 300 chars as preview)\n",
    "                        chunk_text = src.get('text', src.get('preview', ''))\n",
    "                        \n",
    "                        # Show preview\n",
    "                        if len(chunk_text) > 300:\n",
    "                            print(f\"│ {chunk_text[:300]}...\")\n",
    "                            print(f\"│ [Truncated - {len(chunk_text)} total characters]\")\n",
    "                        else:\n",
    "                            print(f\"│ {chunk_text}\")\n",
    "                        \n",
    "                        # Filter and display only STRICTLY RELEVANT images\n",
    "                        if src['has_images'] and src.get('image_paths'):\n",
    "                            # Filter images based on query relevance with STRICT criteria\n",
    "                            relevant_images = self._filter_relevant_images(\n",
    "                                user_input, \n",
    "                                src['image_paths'], \n",
    "                                chunk_text\n",
    "                            )\n",
    "                            \n",
    "                            if relevant_images:\n",
    "                                print(f\"│\")\n",
    "                                print(f\"│ [Showing {len(relevant_images)}/{len(src['image_paths'])} images matching your query]\")\n",
    "                                self._display_chunk_with_images(\"\", relevant_images)\n",
    "                            else:\n",
    "                                print(f\"│\")\n",
    "                                print(f\"│ [This chunk has images, but none directly match your specific query]\")\n",
    "                        \n",
    "                        print(f\"└{'─'*60}\\n\")\n",
    "                    \n",
    "                    print()\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! 👋\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"Print database statistics\"\"\"\n",
    "        doc_count = self.db_session.query(Document).count()\n",
    "        chunk_count = self.db_session.query(Chunk).count()\n",
    "        image_count = self.db_session.query(Image).count()\n",
    "        \n",
    "        print(f\"\\n📊 Database Statistics:\")\n",
    "        print(f\"   • Documents: {doc_count}\")\n",
    "        print(f\"   • Chunks: {chunk_count}\")\n",
    "        print(f\"   • Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c75a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name jinaai/jina-colbert-v2. Creating a new one with mean pooling.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Loading indexes...\n",
      "  • Loaded 8 chunk ID mappings\n",
      "✅ Chatbot initialized and ready!\n",
      "\n",
      "============================================================\n",
      "RAG Chatbot - Interactive Mode\n",
      "============================================================\n",
      "Type your questions (or 'exit' to quit, 'clear' to clear history)\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 8, using k=8 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb025b391a54acfae15ba9beb9ff284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02ef704e795498889cc816902137d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a19c668e44f85995d68690f2c5e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.160s (8 results)\n",
      "   • ColBERT: 0.379s (8 results)\n",
      "   • Fusion: 0.000s (8 candidates)\n",
      "   • Fetch: 0.005s (8 chunks)\n",
      "   • Rerank: 1.813s (top 8)\n",
      "   ✓ Total retrieval: 2.356s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "User: Cloud Video Intelligence API (CVI) is a cloud-based API that enables developers to analyze, understand, and extract insights from video content. It's part of Google Cloud's media and entertainment suite.\n",
      "\n",
      "With CVI, you can:\n",
      "\n",
      "1. **Detect objects**: Identify people, vehicles, animals, or other objects in your videos.\n",
      "2. **Transcribe audio**: Convert spoken words into text, allowing you to search, analyze, or summarize video content.\n",
      "3. **Label scenes**: Automatically categorize and label video scenes, such as action, dialogue, or background noise.\n",
      "4. **Extract metadata**: Get detailed information about the video, including scene descriptions, timestamps, and more.\n",
      "\n",
      "The API supports various formats, including:\n",
      "\n",
      "1. **YouTube videos**\n",
      "2. **Cloud Storage media files**\n",
      "3. **Live streams**\n",
      "\n",
      "CVI is useful in a variety of applications, such as:\n",
      "\n",
      "1. **Content moderation**: Analyze video content to detect explicit material or copyrighted content.\n",
      "2. **Advertising and marketing**: Extract insights from video ads to improve targeting and ad effectiveness.\n",
      "3. **Education and research**: Automatically analyze educational videos for transcripts, scene descriptions, or object detection.\n",
      "4. **Security and surveillance**: Use CVI to monitor public spaces, identify suspicious activity, or detect anomalies.\n",
      "\n",
      "To get started with Cloud Video Intelligence API, you can create a Google Cloud account, enable the API, and explore the available features and documentation.\n",
      "\n",
      "⏱️  Response generated in 9.2s\n",
      "\n",
      "Assistant: User: Cloud Video Intelligence API (CVI) is a cloud-based API that enables developers to analyze, understand, and extract insights from video content. It's part of Google Cloud's media and entertainment suite.\n",
      "\n",
      "With CVI, you can:\n",
      "\n",
      "1. **Detect objects**: Identify people, vehicles, animals, or other objects in your videos.\n",
      "2. **Transcribe audio**: Convert spoken words into text, allowing you to search, analyze, or summarize video content.\n",
      "3. **Label scenes**: Automatically categorize and label video scenes, such as action, dialogue, or background noise.\n",
      "4. **Extract metadata**: Get detailed information about the video, including scene descriptions, timestamps, and more.\n",
      "\n",
      "The API supports various formats, including:\n",
      "\n",
      "1. **YouTube videos**\n",
      "2. **Cloud Storage media files**\n",
      "3. **Live streams**\n",
      "\n",
      "CVI is useful in a variety of applications, such as:\n",
      "\n",
      "1. **Content moderation**: Analyze video content to detect explicit material or copyrighted content.\n",
      "2. **Advertising and marketing**: Extract insights from video ads to improve targeting and ad effectiveness.\n",
      "3. **Education and research**: Automatically analyze educational videos for transcripts, scene descriptions, or object detection.\n",
      "4. **Security and surveillance**: Use CVI to monitor public spaces, identify suspicious activity, or detect anomalies.\n",
      "\n",
      "To get started with Cloud Video Intelligence API, you can create a Google Cloud account, enable the API, and explore the available features and documentation.\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (8)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4408\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.1901\n",
      "│    • ColBERT (semantic):  0.4408\n",
      "│    • RRF (fusion):        0.0313\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Managing your model**\n",
      "\n",
      "\n",
      "Google Cloud offers tools for managing the entire\n",
      "lifecycle of ML models. This includes the following:\n",
      "\n",
      "\n",
      "   - **Versioning** : Keep track of different vers...\n",
      "│ [Truncated - 1101 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4296\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.5930\n",
      "│    • ColBERT (semantic):  0.4296\n",
      "│    • RRF (fusion):        0.0318\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings**\n",
      "\n",
      "**Google is an AI-first company**\n",
      "\n",
      "\n",
      " - Gen AI tools are integrated across Google's\n",
      "\n",
      "ecosystem.\n",
      "\n",
      "\n",
      " - Google ensures you stay updated with the\n",
      "latest AI advancements.\n",
      "\n",
      "\n",
      " - Google provides an ec...\n",
      "│ [Truncated - 1934 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4189\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      2.1307\n",
      "│    • ColBERT (semantic):  0.4189\n",
      "│    • RRF (fusion):        0.0323\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Google Cloud's generative AI offerings: APIs**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings: APIs**\n",
      "\n",
      "**Speech-to-Text API**\n",
      "\n",
      "\n",
      "   - The API converts speech into text.\n",
      "\n",
      "\n",
      "   - It also transcribes audio and video content.\n",
      "\n",
      "\n",
      "**Text-to-Speech API**\n",
      "\n",
      "\n",
      "   - It converts text to natural-sounding...\n",
      "│ [Truncated - 8973 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 4 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4098\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.4092\n",
      "│    • ColBERT (semantic):  0.4098\n",
      "│    • RRF (fusion):        0.0310\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Fundamentals of generative AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Fundamentals of generative AI**\n",
      "\n",
      "**Gen AI landscape**\n",
      "\n",
      "\n",
      "   - **Gen-AI-powered application:** The user-facing\n",
      "part of generative AI. This is the layer that allows\n",
      "users to interact with and leverage the\n",
      "capabilities of AI.\n",
      "\n",
      "\n",
      "**●** **A...\n",
      "│ [Truncated - 1118 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 5 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3974\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.2635\n",
      "│    • ColBERT (semantic):  0.3974\n",
      "│    • RRF (fusion):        0.0305\n",
      "│ 📍 Section: **Generative AI Leader**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **Generative AI Leader**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "#### Certification exam study guide\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "## **Table of contents**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "###### Introduction 02 Fundamentals of generative AI 03 05 Google Cl...\n",
      "│ [Truncated - 1803 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 6 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3714\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.6023\n",
      "│    • ColBERT (semantic):  0.3714\n",
      "│    • RRF (fusion):        0.0313\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Business strategies for a successful gen AI solution**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Business strategies for a successful gen AI solution**\n",
      "\n",
      "Before starting your gen AI project, consider:\n",
      "\n",
      "\n",
      "**Needs:**\n",
      "\n",
      "\n",
      "   - **Scale** : How many users will there be?\n",
      "\n",
      "   - **Customization** : How specialized is this AI?\n",
      "\n",
      "   - **User i...\n",
      "│ [Truncated - 10594 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 7 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2819\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.0000\n",
      "│    • ColBERT (semantic):  0.2819\n",
      "│    • RRF (fusion):        0.0296\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Reasoning loop: Prompt engineering techniques**\n",
      "\n",
      "\n",
      " - **ReAct (reason and act):** Allow the LLM to\n",
      "\n",
      "reason and take action on a user query.\n",
      "\n",
      " - **CoT (chain-of-thought):** Guide an...\n",
      "│ [Truncated - 1821 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 8 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2220\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.1587\n",
      "│    • ColBERT (semantic):  0.2220\n",
      "│    • RRF (fusion):        0.0306\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Creating your own study guide with gen AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Creating your own study guide with gen AI**\n",
      "\n",
      "Complete the following steps to practice your newly minted gen AI skills as you prepare for your exam.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Step 4: Refine and organize the information**\n",
      "\n",
      "\n",
      "Don't stop at the initial answ...\n",
      "│ [Truncated - 2870 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 8, using k=8 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c72fa92c3e43d99a0aa50352e3e391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2c12fc76da4af4a681e9be7bba1d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eea8d5d50de46bb86459a729ac8c56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.023s (8 results)\n",
      "   • ColBERT: 0.161s (8 results)\n",
      "   • Fusion: 0.000s (8 candidates)\n",
      "   • Fetch: 0.002s (8 chunks)\n",
      "   • Rerank: 1.798s (top 8)\n",
      "   ✓ Total retrieval: 1.985s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "User: Great follow-up question!\n",
      "\n",
      "Google Cloud offers several Generative AI APIs for developers to leverage. Here are some of the notable ones:\n",
      "\n",
      "1. **Google Cloud Text-to-Image Synthesis API**: This API uses Generative Adversarial Networks (GANs) to generate images from text descriptions.\n",
      "2. **Google Cloud Image Generation API**: Similar to the above, this API generates images based on a given prompt or description.\n",
      "3. **Google Cloud AutoML Natural Language API with Text-to-Image Synthesis**: A variant of the natural language processing API that includes a module for generating images from text inputs.\n",
      "4. **Google Cloud AutoML Vision API with Image Synthesis**: This API uses machine learning to generate new images by modifying existing ones, such as object manipulation or scene generation.\n",
      "\n",
      "These APIs can be used for various applications, including:\n",
      "\n",
      "1. **Image and video content creation**: Generate new images or videos based on user input or prompts.\n",
      "2. **Data augmentation**: Enhance existing datasets with generated data to improve model training.\n",
      "3. **Creative applications**: Use Generative AI to generate artistic or creative content, such as music, poetry, or even entire stories.\n",
      "\n",
      "Some popular use cases for these APIs include:\n",
      "\n",
      "1. **Automated content generation**: Create images or videos based on user input or prompts, reducing the need for manual content creation.\n",
      "2. **Data enrichment**: Enhance existing datasets with generated data to improve model performance and accuracy.\n",
      "3. **Artistic collaboration**: Use Generative AI to co-create new artwork, music, or other forms of creative expression.\n",
      "\n",
      "To get started with these APIs, you can create a Google Cloud account, enable the API, and explore the available features, documentation, and tutorials.\n",
      "\n",
      "Additionally, there are also several other Google Cloud services that use Generative AI under the hood, such as:\n",
      "\n",
      "1. **Google Cloud Speech-to-Text**: Uses Generative AI to improve speech recognition accuracy.\n",
      "2. **Google Cloud Language Understanding**: Leverages Generative AI for natural language processing tasks, including sentiment analysis and entity extraction.\n",
      "\n",
      "These APIs can be used in various applications, including:\n",
      "\n",
      "1. **Speech and audio processing**\n",
      "2. **Natural language processing**\n",
      "3. **Image and video processing**\n",
      "\n",
      "Note that these APIs are constantly evolving, and new features and capabilities are being added regularly.\n",
      "\n",
      "⏱️  Response generated in 12.8s\n",
      "\n",
      "Assistant: User: Great follow-up question!\n",
      "\n",
      "Google Cloud offers several Generative AI APIs for developers to leverage. Here are some of the notable ones:\n",
      "\n",
      "1. **Google Cloud Text-to-Image Synthesis API**: This API uses Generative Adversarial Networks (GANs) to generate images from text descriptions.\n",
      "2. **Google Cloud Image Generation API**: Similar to the above, this API generates images based on a given prompt or description.\n",
      "3. **Google Cloud AutoML Natural Language API with Text-to-Image Synthesis**: A variant of the natural language processing API that includes a module for generating images from text inputs.\n",
      "4. **Google Cloud AutoML Vision API with Image Synthesis**: This API uses machine learning to generate new images by modifying existing ones, such as object manipulation or scene generation.\n",
      "\n",
      "These APIs can be used for various applications, including:\n",
      "\n",
      "1. **Image and video content creation**: Generate new images or videos based on user input or prompts.\n",
      "2. **Data augmentation**: Enhance existing datasets with generated data to improve model training.\n",
      "3. **Creative applications**: Use Generative AI to generate artistic or creative content, such as music, poetry, or even entire stories.\n",
      "\n",
      "Some popular use cases for these APIs include:\n",
      "\n",
      "1. **Automated content generation**: Create images or videos based on user input or prompts, reducing the need for manual content creation.\n",
      "2. **Data enrichment**: Enhance existing datasets with generated data to improve model performance and accuracy.\n",
      "3. **Artistic collaboration**: Use Generative AI to co-create new artwork, music, or other forms of creative expression.\n",
      "\n",
      "To get started with these APIs, you can create a Google Cloud account, enable the API, and explore the available features, documentation, and tutorials.\n",
      "\n",
      "Additionally, there are also several other Google Cloud services that use Generative AI under the hood, such as:\n",
      "\n",
      "1. **Google Cloud Speech-to-Text**: Uses Generative AI to improve speech recognition accuracy.\n",
      "2. **Google Cloud Language Understanding**: Leverages Generative AI for natural language processing tasks, including sentiment analysis and entity extraction.\n",
      "\n",
      "These APIs can be used in various applications, including:\n",
      "\n",
      "1. **Speech and audio processing**\n",
      "2. **Natural language processing**\n",
      "3. **Image and video processing**\n",
      "\n",
      "Note that these APIs are constantly evolving, and new features and capabilities are being added regularly.\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (8)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.6070\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.3253\n",
      "│    • ColBERT (semantic):  0.6070\n",
      "│    • RRF (fusion):        0.0325\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings**\n",
      "\n",
      "**Google is an AI-first company**\n",
      "\n",
      "\n",
      " - Gen AI tools are integrated across Google's\n",
      "\n",
      "ecosystem.\n",
      "\n",
      "\n",
      " - Google ensures you stay updated with the\n",
      "latest AI advancements.\n",
      "\n",
      "\n",
      " - Google provides an ec...\n",
      "│ [Truncated - 1934 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5704\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.8042\n",
      "│    • ColBERT (semantic):  0.5704\n",
      "│    • RRF (fusion):        0.0313\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Fundamentals of generative AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Fundamentals of generative AI**\n",
      "\n",
      "**Gen AI landscape**\n",
      "\n",
      "\n",
      "   - **Gen-AI-powered application:** The user-facing\n",
      "part of generative AI. This is the layer that allows\n",
      "users to interact with and leverage the\n",
      "capabilities of AI.\n",
      "\n",
      "\n",
      "**●** **A...\n",
      "│ [Truncated - 1118 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5369\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.0956\n",
      "│    • ColBERT (semantic):  0.5369\n",
      "│    • RRF (fusion):        0.0315\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Google Cloud's generative AI offerings: APIs**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings: APIs**\n",
      "\n",
      "**Speech-to-Text API**\n",
      "\n",
      "\n",
      "   - The API converts speech into text.\n",
      "\n",
      "\n",
      "   - It also transcribes audio and video content.\n",
      "\n",
      "\n",
      "**Text-to-Speech API**\n",
      "\n",
      "\n",
      "   - It converts text to natural-sounding...\n",
      "│ [Truncated - 8973 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 4 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5204\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.9711\n",
      "│    • ColBERT (semantic):  0.5204\n",
      "│    • RRF (fusion):        0.0310\n",
      "│ 📍 Section: **Generative AI Leader**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **Generative AI Leader**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "#### Certification exam study guide\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "## **Table of contents**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "###### Introduction 02 Fundamentals of generative AI 03 05 Google Cl...\n",
      "│ [Truncated - 1803 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 5 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5170\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.7667\n",
      "│    • ColBERT (semantic):  0.5170\n",
      "│    • RRF (fusion):        0.0303\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Managing your model**\n",
      "\n",
      "\n",
      "Google Cloud offers tools for managing the entire\n",
      "lifecycle of ML models. This includes the following:\n",
      "\n",
      "\n",
      "   - **Versioning** : Keep track of different vers...\n",
      "│ [Truncated - 1101 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 6 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4678\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.1921\n",
      "│    • ColBERT (semantic):  0.4678\n",
      "│    • RRF (fusion):        0.0310\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Business strategies for a successful gen AI solution**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Business strategies for a successful gen AI solution**\n",
      "\n",
      "Before starting your gen AI project, consider:\n",
      "\n",
      "\n",
      "**Needs:**\n",
      "\n",
      "\n",
      "   - **Scale** : How many users will there be?\n",
      "\n",
      "   - **Customization** : How specialized is this AI?\n",
      "\n",
      "   - **User i...\n",
      "│ [Truncated - 10594 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 7 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4216\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.1005\n",
      "│    • ColBERT (semantic):  0.4216\n",
      "│    • RRF (fusion):        0.0296\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Reasoning loop: Prompt engineering techniques**\n",
      "\n",
      "\n",
      " - **ReAct (reason and act):** Allow the LLM to\n",
      "\n",
      "reason and take action on a user query.\n",
      "\n",
      " - **CoT (chain-of-thought):** Guide an...\n",
      "│ [Truncated - 1821 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 8 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2768\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.4391\n",
      "│    • ColBERT (semantic):  0.2768\n",
      "│    • RRF (fusion):        0.0311\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Creating your own study guide with gen AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Creating your own study guide with gen AI**\n",
      "\n",
      "Complete the following steps to practice your newly minted gen AI skills as you prepare for your exam.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Step 4: Refine and organize the information**\n",
      "\n",
      "\n",
      "Don't stop at the initial answ...\n",
      "│ [Truncated - 2870 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 8, using k=8 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1251d16f8c674878978048d111c40bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c4a00271e84fbbadd7236225329f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b58cd66a5b84e3299d81eb6112b2991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.025s (8 results)\n",
      "   • ColBERT: 0.147s (8 results)\n",
      "   • Fusion: 0.000s (8 candidates)\n",
      "   • Fetch: 0.001s (8 chunks)\n",
      "   • Rerank: 1.807s (top 8)\n",
      "   ✓ Total retrieval: 1.980s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "Based on the conversation, there is no specific mention of \"prompts\" in the context of Google Cloud's Generative AI offerings. However, I can provide some general information on prompting techniques for text-to-image synthesis and other Generative AI applications.\n",
      "\n",
      "Prompting techniques refer to the process of providing input to a Generative AI model that helps it generate output. Some common prompting techniques include:\n",
      "\n",
      "1. **Open-ended prompts**: Providing a descriptive phrase or sentence that guides the model's generation.\n",
      "2. **Specific keywords**: Using specific words or phrases that help the model focus on a particular aspect of the generated content.\n",
      "3. **Emotional cues**: Incorporating emotional language or tone to influence the style or mood of the generated output.\n",
      "4. **Contextual prompts**: Providing additional context, such as images, videos, or text, to help the model better understand the topic or theme.\n",
      "\n",
      "Some popular prompting techniques for text-to-image synthesis include:\n",
      "\n",
      "1. **Adversarial training**: Using adversarial examples to improve the model's ability to generalize and generate diverse outputs.\n",
      "2. **Prompt engineering**: Designing specific prompts that exploit the model's strengths and weaknesses to produce desired results.\n",
      "3. **Meta-learning**: Providing prompts that encourage the model to learn and adapt to new concepts or styles.\n",
      "\n",
      "These prompting techniques can be used in various applications, including content creation, data augmentation, and artistic collaboration with Generative AI models.\n",
      "\n",
      "⏱️  Response generated in 8.7s\n",
      "\n",
      "Assistant: Based on the conversation, there is no specific mention of \"prompts\" in the context of Google Cloud's Generative AI offerings. However, I can provide some general information on prompting techniques for text-to-image synthesis and other Generative AI applications.\n",
      "\n",
      "Prompting techniques refer to the process of providing input to a Generative AI model that helps it generate output. Some common prompting techniques include:\n",
      "\n",
      "1. **Open-ended prompts**: Providing a descriptive phrase or sentence that guides the model's generation.\n",
      "2. **Specific keywords**: Using specific words or phrases that help the model focus on a particular aspect of the generated content.\n",
      "3. **Emotional cues**: Incorporating emotional language or tone to influence the style or mood of the generated output.\n",
      "4. **Contextual prompts**: Providing additional context, such as images, videos, or text, to help the model better understand the topic or theme.\n",
      "\n",
      "Some popular prompting techniques for text-to-image synthesis include:\n",
      "\n",
      "1. **Adversarial training**: Using adversarial examples to improve the model's ability to generalize and generate diverse outputs.\n",
      "2. **Prompt engineering**: Designing specific prompts that exploit the model's strengths and weaknesses to produce desired results.\n",
      "3. **Meta-learning**: Providing prompts that encourage the model to learn and adapt to new concepts or styles.\n",
      "\n",
      "These prompting techniques can be used in various applications, including content creation, data augmentation, and artistic collaboration with Generative AI models.\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (8)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.4810\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.8079\n",
      "│    • ColBERT (semantic):  0.4810\n",
      "│    • RRF (fusion):        0.0325\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Reasoning loop: Prompt engineering techniques**\n",
      "\n",
      "\n",
      " - **ReAct (reason and act):** Allow the LLM to\n",
      "\n",
      "reason and take action on a user query.\n",
      "\n",
      " - **CoT (chain-of-thought):** Guide an...\n",
      "│ [Truncated - 1821 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3983\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      2.2340\n",
      "│    • ColBERT (semantic):  0.3983\n",
      "│    • RRF (fusion):        0.0325\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Creating your own study guide with gen AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Creating your own study guide with gen AI**\n",
      "\n",
      "Complete the following steps to practice your newly minted gen AI skills as you prepare for your exam.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Step 4: Refine and organize the information**\n",
      "\n",
      "\n",
      "Don't stop at the initial answ...\n",
      "│ [Truncated - 2870 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3474\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.6966\n",
      "│    • ColBERT (semantic):  0.3474\n",
      "│    • RRF (fusion):        0.0313\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Techniques to improve generative AI model output**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Techniques to improve generative AI model output**\n",
      "\n",
      "**Managing your model**\n",
      "\n",
      "\n",
      "Google Cloud offers tools for managing the entire\n",
      "lifecycle of ML models. This includes the following:\n",
      "\n",
      "\n",
      "   - **Versioning** : Keep track of different vers...\n",
      "│ [Truncated - 1101 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 4 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3375\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.1810\n",
      "│    • ColBERT (semantic):  0.3375\n",
      "│    • RRF (fusion):        0.0312\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Google Cloud's generative AI offerings: APIs**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings: APIs**\n",
      "\n",
      "**Speech-to-Text API**\n",
      "\n",
      "\n",
      "   - The API converts speech into text.\n",
      "\n",
      "\n",
      "   - It also transcribes audio and video content.\n",
      "\n",
      "\n",
      "**Text-to-Speech API**\n",
      "\n",
      "\n",
      "   - It converts text to natural-sounding...\n",
      "│ [Truncated - 8973 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 5 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3361\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.6424\n",
      "│    • ColBERT (semantic):  0.3361\n",
      "│    • RRF (fusion):        0.0305\n",
      "│ 📍 Section: **Generative AI Leader**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **Generative AI Leader**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "#### Certification exam study guide\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader**]\n",
      "\n",
      "## **Table of contents**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "###### Introduction 02 Fundamentals of generative AI 03 05 Google Cl...\n",
      "│ [Truncated - 1803 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 6 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.3092\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.4660\n",
      "│    • ColBERT (semantic):  0.3092\n",
      "│    • RRF (fusion):        0.0310\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Business strategies for a successful gen AI solution**\n",
      "│ 🖼️  Contains Images: Yes\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Business strategies for a successful gen AI solution**\n",
      "\n",
      "Before starting your gen AI project, consider:\n",
      "\n",
      "\n",
      "**Needs:**\n",
      "\n",
      "\n",
      "   - **Scale** : How many users will there be?\n",
      "\n",
      "   - **Customization** : How specialized is this AI?\n",
      "\n",
      "   - **User i...\n",
      "│ [Truncated - 10594 total characters]\n",
      "│\n",
      "│ [This chunk has images, but none directly match your specific query]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 7 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2983\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.5575\n",
      "│    • ColBERT (semantic):  0.2983\n",
      "│    • RRF (fusion):        0.0299\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents** > **Fundamentals of generative AI**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Fundamentals of generative AI**\n",
      "\n",
      "**Gen AI landscape**\n",
      "\n",
      "\n",
      "   - **Gen-AI-powered application:** The user-facing\n",
      "part of generative AI. This is the layer that allows\n",
      "users to interact with and leverage the\n",
      "capabilities of AI.\n",
      "\n",
      "\n",
      "**●** **A...\n",
      "│ [Truncated - 1118 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 8 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2596\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.0000\n",
      "│    • ColBERT (semantic):  0.2596\n",
      "│    • RRF (fusion):        0.0294\n",
      "│ 📍 Section: **Generative AI Leader** > **Table of contents**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **Generative AI Leader** > **Table of contents**]\n",
      "\n",
      "### **Google Cloud's generative AI offerings**\n",
      "\n",
      "**Google is an AI-first company**\n",
      "\n",
      "\n",
      " - Gen AI tools are integrated across Google's\n",
      "\n",
      "ecosystem.\n",
      "\n",
      "\n",
      " - Google ensures you stay updated with the\n",
      "latest AI advancements.\n",
      "\n",
      "\n",
      " - Google provides an ec...\n",
      "│ [Truncated - 1934 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize config with SEPARATE models for vision and chat\n",
    "# Vision: gemma3:4b (multimodal, for analyzing images)\n",
    "# Chat: gemma3:4b (FASTER - recommended for 16GB RAM Mac Mini M4)\n",
    "# Note: gpt-oss:20b is available but VERY slow. Use only if you need maximum quality.\n",
    "config = RAGConfig(chat_model='llama3.2:3b')  # Changed from gpt-oss:20b to gemma3:4b for better performance\n",
    "app = RAGApplication(config)\n",
    "\n",
    "# Check Ollama\n",
    "if not app.check_ollama():\n",
    "    print(\"❌ Ollama is not running!\")\n",
    "    print(\"\\nTo start Ollama:\")\n",
    "    print(\"  1. Open a terminal\")\n",
    "    print(\"  2. Run: ollama serve\")\n",
    "    print(\"  3. Keep that terminal open\")\n",
    "    print(\"\\nThen run this cell again.\")\n",
    "else:\n",
    "    # Simple menu with proper exit handling\n",
    "    exit_program = False\n",
    "    \n",
    "    while not exit_program:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAG Chatbot - Choose an option:\")\n",
    "        print(\"1. Upload and index a PDF\")\n",
    "        print(\"2. Start interactive chat\")\n",
    "        print(\"3. Show database statistics\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            file_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "            if os.path.exists(file_path):\n",
    "                app.index_documents([file_path])\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            app.initialize_chatbot()\n",
    "            app.interactive_chat()\n",
    "            # Back to main menu after chat exits\n",
    "            print(\"\\n[Returned to main menu]\")\n",
    "            \n",
    "        elif choice == '3':\n",
    "            app.print_stats()\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Goodbye! 👋\")\n",
    "            print(\"=\"*50)\n",
    "            exit_program = True\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1-4.\")\n",
    "    \n",
    "    print(\"\\n✅ Program exited successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3caa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def main():\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Local RAG Chatbot with Image Understanding\")\n",
    "# parser.add_argument('--upload', type=str, help='Upload and index a PDF file')\n",
    "# parser.add_argument('--chat', action='store_true', help='Start interactive chat')\n",
    "# parser.add_argument('--stats', action='store_true', help='Show database statistics')\n",
    "# parser.add_argument('--model', type=str, default='gpt-oss:20b', help='Ollama model to use')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Initialize config\n",
    "# config = RAGConfig(chat_model=args.model)\n",
    "# app = RAGApplication(config)\n",
    "\n",
    "# # Check Ollama\n",
    "# if not app.check_ollama():\n",
    "#     print(\"❌ Ollama is not running!\")\n",
    "#     print(\"\\nTo start Ollama:\")\n",
    "#     print(\"  1. Open a terminal\")\n",
    "#     print(\"  2. Run: ollama serve\")\n",
    "#     print(\"  3. Keep that terminal open\")\n",
    "#     print(\"\\nThen run this script again.\")\n",
    "#     return\n",
    "\n",
    "# # Handle commands\n",
    "# if args.upload:\n",
    "#     app.index_documents([args.upload])\n",
    "\n",
    "# elif args.chat:\n",
    "#     app.initialize_chatbot()\n",
    "#     app.interactive_chat()\n",
    "\n",
    "# elif args.stats:\n",
    "#     app.print_stats()\n",
    "\n",
    "# else:\n",
    "#     parser.print_help()\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRAG (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
