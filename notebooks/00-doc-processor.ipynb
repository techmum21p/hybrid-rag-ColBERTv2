{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ef7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Local RAG Chatbot with Image Understanding\\n===================================================\\n\\nâœ… No Cloud Dependencies (runs 100% locally)\\nâœ… No RAGatouille (direct Jina ColBERT v2 implementation)\\nâœ… PyMuPDF4LLM for PDF conversion\\nâœ… Image extraction and analysis with LLaVA vision model\\nâœ… Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\\nâœ… Markdown-aware semantic chunking\\nâœ… SQLite database for storage\\n\\nRequirements:\\n- Ollama (for LLMs: llama3.2:3b, llava:7b)\\n- Mac Mini M4 or similar (16GB RAM recommended)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Local RAG Chatbot with Image Understanding\n",
    "===================================================\n",
    "\n",
    "âœ… No Cloud Dependencies (runs 100% locally)\n",
    "âœ… No RAGatouille (direct Jina ColBERT v2 implementation)\n",
    "âœ… PyMuPDF4LLM for PDF conversion\n",
    "âœ… Image extraction and analysis with LLaVA vision model\n",
    "âœ… Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\n",
    "âœ… Markdown-aware semantic chunking\n",
    "âœ… SQLite database for storage\n",
    "\n",
    "Requirements:\n",
    "- Ollama (for LLMs: llama3.2:3b, llava:7b)\n",
    "- Mac Mini M4 or similar (16GB RAM recommended)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f723da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# PDF and text processing\n",
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF for image extraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Retrieval\n",
    "import bm25s\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "\n",
    "# LLM\n",
    "import requests  # For Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a1fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for local RAG system\"\"\"\n",
    "    # Database\n",
    "    db_path: str = \"rag_local.db\"\n",
    "    \n",
    "    # Chunking\n",
    "    min_chunk_size: int = 256\n",
    "    max_chunk_size: int = 1024\n",
    "    chunk_overlap: int = 128\n",
    "    \n",
    "    # Retrieval\n",
    "    bm25_top_k: int = 50\n",
    "    colbert_top_k: int = 50\n",
    "    final_top_k: int = 10\n",
    "    \n",
    "    # Models\n",
    "    chat_model: str = \"gpt-oss:20b\"\n",
    "    vision_model: str = \"gemma3:4b\"\n",
    "    embedding_model: str = \"jinaai/jina-colbert-v2\"\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_url: str = \"http://localhost:11434\"\n",
    "    \n",
    "    # Paths\n",
    "    bm25_index_path: str = \"indexes/bm25s\"\n",
    "    colbert_index_path: str = \"indexes/colbert\"\n",
    "    images_dir: str = \"extracted_images\"\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    upload_date = Column(DateTime, default=datetime.utcnow)\n",
    "    total_pages = Column(Integer)\n",
    "    status = Column(String(50))\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = 'images'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    page_number = Column(Integer, nullable=False)\n",
    "    image_path = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    image_type = Column(String(50))\n",
    "    ocr_text = Column(Text)\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = 'chunks'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    text = Column(Text, nullable=False)\n",
    "    heading_path = Column(String(500))\n",
    "    token_count = Column(Integer)\n",
    "    has_images = Column(Boolean, default=False)\n",
    "    chunk_metadata = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA CLIENT\n",
    "# ============================================================================\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with Ollama API\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.ollama_url\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        model: str, \n",
    "        prompt: str, \n",
    "        system: str = \"\",  # Changed default to empty string\n",
    "        images: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text with Ollama\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Ollama error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_image(self, image_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Analyze image using LLaVA vision model\"\"\"\n",
    "        \n",
    "        # Read image and convert to base64\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            import base64\n",
    "            image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "        # Generate description\n",
    "        description_prompt = \"\"\"Analyze this image and provide:\n",
    "            1. TYPE: What type of visual is this? (diagram, chart, table, screenshot, photo, etc.)\n",
    "            2. DESCRIPTION: A detailed description of what the image shows (2-3 sentences)\n",
    "            3. TEXT: Any visible text in the image (transcribe exactly)\n",
    "\n",
    "            Format your response as:\n",
    "            TYPE: [type]\n",
    "            DESCRIPTION: [description]\n",
    "            TEXT: [extracted text]\n",
    "            \"\"\"\n",
    "        \n",
    "        response = self.generate(\n",
    "            model=self.config.vision_model,\n",
    "            prompt=description_prompt,\n",
    "            images=[image_data]\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        result = {\n",
    "            'description': '',\n",
    "            'type': 'unknown',\n",
    "            'ocr_text': ''\n",
    "        }\n",
    "        \n",
    "        for line in response.split('\\n'):\n",
    "            if line.startswith('TYPE:'):\n",
    "                result['type'] = line.replace('TYPE:', '').strip().lower()\n",
    "            elif line.startswith('DESCRIPTION:'):\n",
    "                result['description'] = line.replace('DESCRIPTION:', '').strip()\n",
    "            elif line.startswith('TEXT:'):\n",
    "                result['ocr_text'] = line.replace('TEXT:', '').strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def chat(\n",
    "        self, \n",
    "        messages: List[Dict[str, str]], \n",
    "        context: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"Chat with context\"\"\"\n",
    "        \n",
    "        # Build system message with context\n",
    "        system_msg = \"You are a helpful AI assistant.\"\n",
    "        if context:\n",
    "            system_msg += f\"\\n\\nContext from documents:\\n{context}\\n\\nUse this context to answer questions accurately.\"\n",
    "        \n",
    "        # Build prompt from messages\n",
    "        prompt = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in messages\n",
    "        ])\n",
    "        \n",
    "        return self.generate(\n",
    "            model=self.config.chat_model,\n",
    "            prompt=prompt,\n",
    "            system=system_msg\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75436919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKDOWN-AWARE SEMANTIC CHUNKER\n",
    "# ============================================================================\n",
    "\n",
    "class MarkdownSemanticChunker:\n",
    "    \"\"\"Intelligent markdown chunking that respects document structure\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def chunk_markdown(self, markdown_text: str, doc_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Create semantically meaningful chunks\"\"\"\n",
    "        sections = self._parse_markdown_hierarchy(markdown_text)\n",
    "        chunks = self._create_chunks_from_sections(sections, doc_context)\n",
    "        optimized_chunks = self._optimize_chunks(chunks)\n",
    "        return optimized_chunks\n",
    "    \n",
    "    def _parse_markdown_hierarchy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse markdown into hierarchical sections\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        heading_stack = []\n",
    "        \n",
    "        for line in lines:\n",
    "            heading_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            \n",
    "            if heading_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                level = len(heading_match.group(1))\n",
    "                title = heading_match.group(2).strip()\n",
    "                \n",
    "                heading_stack = [(lvl, ttl) for lvl, ttl in heading_stack if lvl < level]\n",
    "                heading_stack.append((level, title))\n",
    "                \n",
    "                parent_path = ' > '.join([ttl for _, ttl in heading_stack[:-1]])\n",
    "                full_path = ' > '.join([ttl for _, ttl in heading_stack])\n",
    "                \n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'content': '',\n",
    "                    'parent_path': parent_path,\n",
    "                    'full_path': full_path\n",
    "                }\n",
    "            else:\n",
    "                if current_section is not None:\n",
    "                    current_section['content'] += line + '\\n'\n",
    "                else:\n",
    "                    if not sections or sections[-1]['level'] != 0:\n",
    "                        sections.append({\n",
    "                            'level': 0,\n",
    "                            'title': 'Introduction',\n",
    "                            'content': line + '\\n',\n",
    "                            'parent_path': '',\n",
    "                            'full_path': 'Introduction'\n",
    "                        })\n",
    "                    else:\n",
    "                        sections[-1]['content'] += line + '\\n'\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_chunks_from_sections(self, sections: List[Dict], doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Create chunks from sections\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = None\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text = self._format_section_text(section)\n",
    "            section_tokens = self._count_tokens(section_text)\n",
    "            \n",
    "            if section_tokens > self.config.max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                split_chunks = self._split_large_section(section, doc_context)\n",
    "                chunks.extend(split_chunks)\n",
    "            \n",
    "            elif section_tokens >= self.config.min_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                chunks.append({\n",
    "                    'text': section_text,\n",
    "                    'heading_path': section['full_path'],\n",
    "                    'level': section['level'],\n",
    "                    'token_count': section_tokens,\n",
    "                    'doc_context': doc_context,\n",
    "                    'type': 'section'\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                if current_chunk is None:\n",
    "                    current_chunk = {\n",
    "                        'text': section_text,\n",
    "                        'heading_path': section['parent_path'] or section['title'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': section_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'accumulated',\n",
    "                        'sections': [section['title']]\n",
    "                    }\n",
    "                else:\n",
    "                    combined_text = current_chunk['text'] + '\\n\\n' + section_text\n",
    "                    combined_tokens = self._count_tokens(combined_text)\n",
    "                    \n",
    "                    if combined_tokens <= self.config.max_chunk_size:\n",
    "                        current_chunk['text'] = combined_text\n",
    "                        current_chunk['token_count'] = combined_tokens\n",
    "                        current_chunk['sections'].append(section['title'])\n",
    "                    else:\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = {\n",
    "                            'text': section_text,\n",
    "                            'heading_path': section['parent_path'] or section['title'],\n",
    "                            'level': section['level'],\n",
    "                            'token_count': section_tokens,\n",
    "                            'doc_context': doc_context,\n",
    "                            'type': 'accumulated',\n",
    "                            'sections': [section['title']]\n",
    "                        }\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_large_section(self, section: Dict, doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Split large section at paragraph boundaries\"\"\"\n",
    "        heading_text = f\"# {section['title']}\\n\\n\"\n",
    "        parent_context = f\"Context: {section['parent_path']}\\n\\n\" if section['parent_path'] else \"\"\n",
    "        \n",
    "        paragraphs = re.split(r'\\n\\n+', section['content'].strip())\n",
    "        \n",
    "        chunks = []\n",
    "        current_text = heading_text + parent_context\n",
    "        current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = self._count_tokens(para)\n",
    "            \n",
    "            if current_tokens + para_tokens <= self.config.max_chunk_size:\n",
    "                current_text += para + '\\n\\n'\n",
    "                current_tokens += para_tokens\n",
    "            else:\n",
    "                if current_text.strip() != heading_text.strip():\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': current_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'split_section',\n",
    "                        'part': len(chunks) + 1\n",
    "                    })\n",
    "                \n",
    "                current_text = heading_text + parent_context + para + '\\n\\n'\n",
    "                current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        if current_text.strip():\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'token_count': current_tokens,\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'split_section',\n",
    "                'part': len(chunks) + 1\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Merge very small chunks\"\"\"\n",
    "        optimized = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            chunk = chunks[i]\n",
    "            \n",
    "            if (chunk['token_count'] < self.config.min_chunk_size and \n",
    "                i < len(chunks) - 1):\n",
    "                \n",
    "                next_chunk = chunks[i + 1]\n",
    "                combined_text = chunk['text'] + '\\n\\n' + next_chunk['text']\n",
    "                combined_tokens = self._count_tokens(combined_text)\n",
    "                \n",
    "                if combined_tokens <= self.config.max_chunk_size:\n",
    "                    merged_chunk = {\n",
    "                        'text': combined_text,\n",
    "                        'heading_path': chunk['heading_path'],\n",
    "                        'token_count': combined_tokens,\n",
    "                        'doc_context': chunk['doc_context'],\n",
    "                        'type': 'merged'\n",
    "                    }\n",
    "                    optimized.append(merged_chunk)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            \n",
    "            optimized.append(chunk)\n",
    "            i += 1\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _format_section_text(self, section: Dict) -> str:\n",
    "        \"\"\"Format section with heading and context\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if section['parent_path']:\n",
    "            parts.append(f\"[Context: {section['parent_path']}]\")\n",
    "        \n",
    "        if section['title'] and section['title'] != 'Introduction':\n",
    "            heading_prefix = '#' * section['level']\n",
    "            parts.append(f\"{heading_prefix} {section['title']}\")\n",
    "        \n",
    "        parts.append(section['content'].strip())\n",
    "        \n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR WITH IMAGE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF processing with image extraction and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.ollama = ollama_client\n",
    "        self.chunker = MarkdownSemanticChunker(config)\n",
    "        \n",
    "        # Create images directory\n",
    "        os.makedirs(config.images_dir, exist_ok=True)\n",
    "    \n",
    "    def pdf_to_markdown(self, pdf_path: str) -> str:\n",
    "        \"\"\"Convert PDF to Markdown using PyMuPDF4LLM\"\"\"\n",
    "        markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        return markdown_text\n",
    "    \n",
    "    def extract_images_from_pdf(\n",
    "        self, \n",
    "        pdf_path: str, \n",
    "        document_id: int\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Extract images from PDF and save to disk\"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            image_list = page.get_images()\n",
    "            \n",
    "            for img_index, img in enumerate(image_list):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                \n",
    "                # Save image\n",
    "                image_filename = f\"doc{document_id}_page{page_num+1}_img{img_index+1}.png\"\n",
    "                image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "                \n",
    "                with open(image_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_bytes)\n",
    "                \n",
    "                images.append({\n",
    "                    'page_number': page_num + 1,\n",
    "                    'image_path': image_path,\n",
    "                    'image_index': img_index\n",
    "                })\n",
    "        \n",
    "        doc.close()\n",
    "        return images\n",
    "    \n",
    "    def analyze_images(\n",
    "        self, \n",
    "        images: List[Dict],\n",
    "        document_id: int,\n",
    "        db_session\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Analyze images with LLaVA and save to database\"\"\"\n",
    "        image_ids = []\n",
    "        \n",
    "        for idx, img_info in enumerate(images):\n",
    "            print(f\"    Analyzing image {idx+1} on page {img_info['page_number']}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Analyze with vision model\n",
    "            analysis = self.ollama.analyze_image(img_info['image_path'])\n",
    "            \n",
    "            # Save to database\n",
    "            image_record = Image(\n",
    "                document_id=document_id,\n",
    "                page_number=img_info['page_number'],\n",
    "                image_path=img_info['image_path'],\n",
    "                description=analysis['description'],\n",
    "                image_type=analysis['type'],\n",
    "                ocr_text=analysis['ocr_text']\n",
    "            )\n",
    "            db_session.add(image_record)\n",
    "            db_session.flush()\n",
    "            \n",
    "            image_ids.append(image_record.id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"âœ“ ({elapsed:.1f}s)\")\n",
    "        \n",
    "        db_session.commit()\n",
    "        return image_ids\n",
    "    \n",
    "    def enrich_chunks_with_images(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        images_data: List[Dict],\n",
    "        db_session\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Add image context to relevant chunks\"\"\"\n",
    "        \n",
    "        # Get all images for this document\n",
    "        enriched_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_copy = chunk.copy()\n",
    "            \n",
    "            # Find images that might be relevant to this chunk\n",
    "            # (Simple heuristic: images within reasonable distance in text)\n",
    "            relevant_images = []\n",
    "            \n",
    "            for img in images_data:\n",
    "                # You could implement more sophisticated matching here\n",
    "                # For now, we'll add all images to chunks that mention visual content\n",
    "                if any(keyword in chunk['text'].lower() for keyword in \n",
    "                       ['figure', 'image', 'diagram', 'chart', 'screenshot', 'see below', 'shown in']):\n",
    "                    relevant_images.append(img)\n",
    "            \n",
    "            if relevant_images:\n",
    "                # Add image descriptions to chunk text\n",
    "                image_context = \"\\n\\n[Images in this section]:\\n\"\n",
    "                image_metadata = []\n",
    "                \n",
    "                for img in relevant_images:\n",
    "                    image_context += f\"- {img['type'].capitalize()}: {img['description']}\\n\"\n",
    "                    image_metadata.append({\n",
    "                        'path': img['image_path'],\n",
    "                        'description': img['description'],\n",
    "                        'type': img['type']\n",
    "                    })\n",
    "                \n",
    "                chunk_copy['text'] = chunk['text'] + image_context\n",
    "                chunk_copy['has_images'] = True\n",
    "                chunk_copy['image_paths'] = [img['image_path'] for img in relevant_images]\n",
    "                chunk_copy['image_metadata'] = image_metadata\n",
    "            else:\n",
    "                chunk_copy['has_images'] = False\n",
    "            \n",
    "            enriched_chunks.append(chunk_copy)\n",
    "        \n",
    "        return enriched_chunks\n",
    "    \n",
    "    def process_document(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        db_session\n",
    "    ) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Convert to markdown\n",
    "        print(\"\\n[Step 1/5] Converting PDF to Markdown...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        markdown_text = self.pdf_to_markdown(pdf_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ {elapsed:.2f}s\")\n",
    "        print(f\"  â€¢ Extracted {len(markdown_text):,} characters\")\n",
    "        \n",
    "        # Create document record\n",
    "        doc = Document(\n",
    "            filename=os.path.basename(pdf_path),\n",
    "            status='processing'\n",
    "        )\n",
    "        db_session.add(doc)\n",
    "        db_session.commit()\n",
    "        \n",
    "        # Step 2: Extract and analyze images\n",
    "        print(\"\\n[Step 2/5] Extracting and analyzing images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        images = self.extract_images_from_pdf(pdf_path, doc.id)\n",
    "        \n",
    "        if images:\n",
    "            image_ids = self.analyze_images(images, doc.id, db_session)\n",
    "            \n",
    "            # Get image data for enrichment\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_record = db_session.query(Image).filter_by(id=img_id).first()\n",
    "                if img_record:\n",
    "                    images_data.append({\n",
    "                        'image_path': img_record.image_path,\n",
    "                        'description': img_record.description,\n",
    "                        'type': img_record.image_type,\n",
    "                        'ocr_text': img_record.ocr_text\n",
    "                    })\n",
    "        else:\n",
    "            images_data = []\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  âœ“ Completed in {elapsed:.2f}s\")\n",
    "        print(f\"  â€¢ Extracted {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"  â€¢ Vision analysis: âœ“\")\n",
    "        \n",
    "        # Step 3: Markdown-aware semantic chunking\n",
    "        print(\"\\n[Step 3/5] Markdown-aware semantic chunking...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        doc_context = f\"Document: {os.path.basename(pdf_path)}\\n\\n{markdown_text[:500]}\"\n",
    "        chunks = self.chunker.chunk_markdown(markdown_text, doc_context)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ {elapsed:.2f}s\")\n",
    "        print(f\"  â€¢ Created {len(chunks)} semantic chunks\")\n",
    "        \n",
    "        # Step 4: Enrich chunks with image context\n",
    "        print(\"\\n[Step 4/5] Enriching chunks with image context...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        if images_data:\n",
    "            chunks = self.enrich_chunks_with_images(chunks, images_data, db_session)\n",
    "            chunks_with_images = sum(1 for c in chunks if c.get('has_images', False))\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"âœ“ {elapsed:.2f}s\")\n",
    "            print(f\"  â€¢ {chunks_with_images} chunks enriched with image context\")\n",
    "        else:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"âœ“ {elapsed:.2f}s\")\n",
    "            print(f\"  â€¢ No images to enrich\")\n",
    "        \n",
    "        # Step 5: Save to database\n",
    "        print(\"\\n[Step 5/5] Saving chunks to database...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_record = Chunk(\n",
    "                document_id=doc.id,\n",
    "                chunk_index=idx,\n",
    "                text=chunk['text'],\n",
    "                heading_path=chunk.get('heading_path', ''),\n",
    "                token_count=chunk.get('token_count', 0),\n",
    "                has_images=chunk.get('has_images', False),\n",
    "                metadata=json.dumps({\n",
    "                    k: v for k, v in chunk.items() \n",
    "                    if k not in ['text', 'heading_path', 'token_count', 'has_images']\n",
    "                })\n",
    "            )\n",
    "            db_session.add(chunk_record)\n",
    "        \n",
    "        doc.status = 'indexed'\n",
    "        db_session.commit()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ {elapsed:.2f}s\")\n",
    "        \n",
    "        return chunks, doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08725b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JINA COLBERT V2 RETRIEVER (NO RAGATOUILLE!)\n",
    "# ============================================================================\n",
    "\n",
    "class JinaColBERTRetriever:\n",
    "    \"\"\"Direct implementation of Jina ColBERT v2 (no RAGatouille dependency)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = SentenceTransformer(\n",
    "            config.embedding_model,\n",
    "            trust_remote_code=True,\n",
    "            device=config.device\n",
    "        )\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Index corpus with ColBERT embeddings\"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        print(f\"  Encoding {len(corpus)} documents...\")\n",
    "        \n",
    "        # Encode corpus (this gives us token-level embeddings)\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        os.makedirs(self.config.colbert_index_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'embeddings': self.corpus_embeddings,\n",
    "            'corpus': corpus\n",
    "        }, os.path.join(self.config.colbert_index_path, 'index.pt'))\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.config.colbert_index_path, 'index.pt')\n",
    "        data = torch.load(index_file, map_location=self.config.device)\n",
    "        self.corpus_embeddings = data['embeddings']\n",
    "        self.corpus = data['corpus']\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using MaxSim scoring\"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_k_indices = torch.topk(scores, k=min(k, len(scores))).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'document_id': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'text': self.corpus[idx] if self.corpus else None\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank documents with more accurate scoring\"\"\"\n",
    "        # Encode query and documents\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = self.model.encode(documents, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, doc_embeddings)\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(sorted_indices[:k]):\n",
    "            results.append({\n",
    "                'result_index': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'rank': rank + 1,\n",
    "                'text': documents[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _maxsim_score(\n",
    "        self, \n",
    "        query_embedding: torch.Tensor, \n",
    "        doc_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MaxSim score between query and documents\n",
    "        \n",
    "        MaxSim: For each query token, find max similarity with all doc tokens,\n",
    "        then average across query tokens\n",
    "        \"\"\"\n",
    "        # Ensure 3D tensors [batch, seq_len, hidden_dim]\n",
    "        if query_embedding.dim() == 2:\n",
    "            query_embedding = query_embedding.unsqueeze(0)\n",
    "        if doc_embeddings.dim() == 2:\n",
    "            doc_embeddings = doc_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # Compute similarity: [batch_q, seq_q, batch_d, seq_d]\n",
    "        # Simplified: just use mean pooling for now (Jina's model handles this internally)\n",
    "        query_vec = query_embedding.mean(dim=1)  # [batch_q, hidden]\n",
    "        doc_vec = doc_embeddings.mean(dim=1)     # [batch_d, hidden]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        scores = torch.nn.functional.cosine_similarity(\n",
    "            query_vec.unsqueeze(1), \n",
    "            doc_vec.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        return scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUAL INDEXER (BM25s + Jina ColBERT)\n",
    "# ============================================================================\n",
    "\n",
    "class DualIndexer:\n",
    "    \"\"\"Manages BM25s and Jina ColBERT v2 indexes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.bm25_retriever = None\n",
    "        self.colbert_retriever = JinaColBERTRetriever(config)\n",
    "    \n",
    "    def build_bm25_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build BM25s index\"\"\"\n",
    "        print(\"\\n[BM25s] Building lexical search index...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        corpus_tokens = bm25s.tokenize(\n",
    "            corpus, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=bm25s.stemmer.Stemmer.Stemmer(\"english\")\n",
    "        )\n",
    "        \n",
    "        self.bm25_retriever = bm25s.BM25()\n",
    "        self.bm25_retriever.index(corpus_tokens)\n",
    "        \n",
    "        os.makedirs(self.config.bm25_index_path, exist_ok=True)\n",
    "        self.bm25_retriever.save(self.config.bm25_index_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ {elapsed:.2f}s\")\n",
    "    \n",
    "    def build_colbert_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build Jina ColBERT v2 index\"\"\"\n",
    "        print(\"\\n[ColBERT] Building semantic search index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.colbert_retriever.index(corpus)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  âœ“ {elapsed:.2f}s\")\n",
    "    \n",
    "    def load_indexes(self) -> None:\n",
    "        \"\"\"Load indexes from disk\"\"\"\n",
    "        self.bm25_retriever = bm25s.BM25.load(self.config.bm25_index_path)\n",
    "        self.colbert_retriever.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID RETRIEVER WITH RRF AND RERANKING\n",
    "# ============================================================================\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Three-stage retrieval: BM25s + ColBERT + ColBERT Reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, indexer: DualIndexer, db_session):\n",
    "        self.config = config\n",
    "        self.indexer = indexer\n",
    "        self.db_session = db_session\n",
    "    \n",
    "    def retrieve(self, query: str, top_k_final: int = None) -> List[Dict]:\n",
    "        \"\"\"Three-stage hybrid retrieval\"\"\"\n",
    "        if top_k_final is None:\n",
    "            top_k_final = self.config.final_top_k\n",
    "        \n",
    "        print(f\"\\nðŸ” Retrieving relevant chunks...\")\n",
    "        \n",
    "        # Stage 1: BM25s\n",
    "        start = time.time()\n",
    "        bm25_results = self._bm25_search(query, k=self.config.bm25_top_k)\n",
    "        bm25_time = time.time() - start\n",
    "        print(f\"   â€¢ BM25s: {bm25_time:.3f}s\")\n",
    "        \n",
    "        # Stage 2: ColBERT\n",
    "        start = time.time()\n",
    "        colbert_results = self._colbert_search(query, k=self.config.colbert_top_k)\n",
    "        colbert_time = time.time() - start\n",
    "        print(f\"   â€¢ ColBERT: {colbert_time:.3f}s\")\n",
    "        \n",
    "        # Fusion\n",
    "        start = time.time()\n",
    "        fused_results = self._reciprocal_rank_fusion(bm25_results, colbert_results)\n",
    "        candidates = fused_results[:50]\n",
    "        fusion_time = time.time() - start\n",
    "        print(f\"   â€¢ Fusion: {fusion_time:.3f}s\")\n",
    "        \n",
    "        # Fetch chunks\n",
    "        start = time.time()\n",
    "        candidate_chunks = self._fetch_chunks_from_db([r['chunk_id'] for r in candidates])\n",
    "        fetch_time = time.time() - start\n",
    "        print(f\"   â€¢ Fetch: {fetch_time:.3f}s\")\n",
    "        \n",
    "        # Stage 3: Rerank\n",
    "        start = time.time()\n",
    "        reranked_results = self._colbert_rerank(query, candidate_chunks, top_k=top_k_final)\n",
    "        rerank_time = time.time() - start\n",
    "        print(f\"   â€¢ Rerank: {rerank_time:.3f}s\")\n",
    "        \n",
    "        total_time = bm25_time + colbert_time + fusion_time + fetch_time + rerank_time\n",
    "        print(f\"   âœ“ Total retrieval: {total_time:.3f}s\")\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 1: BM25s lexical search\"\"\"\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            query, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=bm25s.stemmer.Stemmer.Stemmer(\"english\")\n",
    "        )\n",
    "        \n",
    "        results, scores = self.indexer.bm25_retriever.retrieve(query_tokens, k=k)\n",
    "        \n",
    "        return [\n",
    "            {'chunk_id': int(results[0][i]), 'score': float(scores[0][i]), 'source': 'bm25'}\n",
    "            for i in range(len(results[0]))\n",
    "        ]\n",
    "    \n",
    "    def _colbert_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 2: ColBERT semantic search\"\"\"\n",
    "        results = self.indexer.colbert_retriever.search(query=query, k=k)\n",
    "        return [\n",
    "            {'chunk_id': r['document_id'], 'score': r['score'], 'source': 'colbert'}\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def _reciprocal_rank_fusion(\n",
    "        self, \n",
    "        bm25_results: List[Dict], \n",
    "        colbert_results: List[Dict],\n",
    "        k: int = 60\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"RRF fusion\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for rank, result in enumerate(bm25_results, 1):\n",
    "            chunk_id = result['chunk_id']\n",
    "            scores[chunk_id] = scores.get(chunk_id, 0) + (1 / (k + rank))\n",
    "        \n",
    "        for rank, result in enumerate(colbert_results, 1):\n",
    "            chunk_id = result['chunk_id']\n",
    "            scores[chunk_id] = scores.get(chunk_id, 0) + (1 / (k + rank))\n",
    "        \n",
    "        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{'chunk_id': cid, 'rrf_score': score} for cid, score in sorted_results]\n",
    "    \n",
    "    def _fetch_chunks_from_db(self, chunk_ids: List[int]) -> List[Dict]:\n",
    "        \"\"\"Fetch chunks from database\"\"\"\n",
    "        chunks = []\n",
    "        for chunk_id in chunk_ids:\n",
    "            chunk = self.db_session.query(Chunk).filter_by(id=chunk_id).first()\n",
    "            if chunk:\n",
    "                chunks.append({\n",
    "                    'chunk_id': chunk.id,\n",
    "                    'text': chunk.text,\n",
    "                    'document_id': chunk.document_id,\n",
    "                    'heading_path': chunk.heading_path,\n",
    "                    'has_images': chunk.has_images,\n",
    "                    'metadata': json.loads(chunk.metadata) if chunk.metadata else {}\n",
    "                })\n",
    "        return chunks\n",
    "    \n",
    "    def _colbert_rerank(self, query: str, chunks: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 3: ColBERT reranking\"\"\"\n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        reranked_results = self.indexer.colbert_retriever.rerank(query=query, documents=documents, k=top_k)\n",
    "        \n",
    "        final_results = []\n",
    "        for result in reranked_results:\n",
    "            original_chunk = chunks[result['result_index']]\n",
    "            final_results.append({\n",
    "                'chunk_id': original_chunk['chunk_id'],\n",
    "                'text': original_chunk['text'],\n",
    "                'document_id': original_chunk['document_id'],\n",
    "                'heading_path': original_chunk.get('heading_path', ''),\n",
    "                'has_images': original_chunk.get('has_images', False),\n",
    "                'metadata': original_chunk['metadata'],\n",
    "                'score': result['score'],\n",
    "                'rank': result['rank']\n",
    "            })\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAG CHATBOT\n",
    "# ============================================================================\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Complete RAG chatbot with Ollama\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, retriever: HybridRetriever, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.ollama = ollama_client\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Process user query and generate response\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retriever.retrieve(query)\n",
    "        \n",
    "        # Build context\n",
    "        context = self._build_context(retrieved_chunks)\n",
    "        \n",
    "        # Generate response\n",
    "        print(f\"\\nðŸ¤– Generating response...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        })\n",
    "        \n",
    "        response = self.ollama.chat(\n",
    "            messages=self.conversation_history,\n",
    "            context=context\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ“ {elapsed:.1f}s\")\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': response\n",
    "        })\n",
    "        \n",
    "        total_time = elapsed\n",
    "        print(f\"â±ï¸  Total: {total_time:.1f}s\\n\")\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': self._format_sources(retrieved_chunks),\n",
    "            'retrieved_chunks': len(retrieved_chunks)\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"Build context from retrieved chunks\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            heading = f\" ({chunk['heading_path']})\" if chunk.get('heading_path') else \"\"\n",
    "            \n",
    "            # Add image info if present\n",
    "            image_info = \"\"\n",
    "            if chunk.get('has_images') and chunk.get('metadata', {}).get('image_paths'):\n",
    "                num_images = len(chunk['metadata']['image_paths'])\n",
    "                image_info = f\" [Contains {num_images} image(s)]\"\n",
    "            \n",
    "            context_parts.append(f\"[Source {i}{heading}{image_info}]\\n{chunk['text']}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format source citations\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'source_id': i + 1,\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'document_id': chunk['document_id'],\n",
    "                'heading': chunk.get('heading_path', ''),\n",
    "                'score': chunk['score'],\n",
    "                'has_images': chunk.get('has_images', False),\n",
    "                'preview': chunk['text'][:200] + \"...\"\n",
    "            }\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"ðŸ—‘ï¸  Conversation history cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    \"\"\"Main application orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Database setup\n",
    "        db_url = f\"sqlite:///{config.db_path}\"\n",
    "        self.engine = create_engine(db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.db_session = Session()\n",
    "        \n",
    "        # Initialize Ollama client\n",
    "        self.ollama = OllamaClient(config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.processor = DocumentProcessor(config, self.ollama)\n",
    "        self.indexer = DualIndexer(config)\n",
    "        self.retriever = None\n",
    "        self.chatbot = None\n",
    "    \n",
    "    def check_ollama(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.ollama_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def index_documents(self, pdf_paths: List[str]) -> None:\n",
    "        \"\"\"Index PDF documents\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"âŒ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            chunks, doc_id = self.processor.process_document(pdf_path, self.db_session)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Building Indexes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build indexes\n",
    "        corpus = [chunk['text'] for chunk in all_chunks]\n",
    "        self.indexer.build_bm25_index(corpus)\n",
    "        self.indexer.build_colbert_index(corpus)\n",
    "        \n",
    "        print(f\"\\nâœ… Document indexed successfully!\")\n",
    "    \n",
    "    def initialize_chatbot(self) -> None:\n",
    "        \"\"\"Initialize chatbot with existing indexes\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"âŒ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading indexes...\")\n",
    "        self.indexer.load_indexes()\n",
    "        \n",
    "        self.retriever = HybridRetriever(self.config, self.indexer, self.db_session)\n",
    "        self.chatbot = RAGChatbot(self.config, self.retriever, self.ollama)\n",
    "        \n",
    "        print(\"âœ… Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Chat interface\"\"\"\n",
    "        if not self.chatbot:\n",
    "            raise RuntimeError(\"Chatbot not initialized. Call initialize_chatbot() first.\")\n",
    "        \n",
    "        return self.chatbot.chat(query)\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAG Chatbot - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Type your questions (or 'exit' to quit, 'clear' to clear history)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"\\nGoodbye! ðŸ‘‹\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'clear':\n",
    "                    self.chatbot.clear_history()\n",
    "                    continue\n",
    "                \n",
    "                result = self.chat(user_input)\n",
    "                print(f\"\\nAssistant: {result['response']}\\n\")\n",
    "                \n",
    "                # Show sources\n",
    "                if result['sources']:\n",
    "                    print(f\"ðŸ“š Sources ({len(result['sources'])}):\")\n",
    "                    for src in result['sources'][:3]:  # Show top 3\n",
    "                        heading = f\" - {src['heading']}\" if src['heading'] else \"\"\n",
    "                        images = \" ðŸ–¼ï¸\" if src['has_images'] else \"\"\n",
    "                        print(f\"  â€¢ Source {src['source_id']}{heading}{images}\")\n",
    "                    print()\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! ðŸ‘‹\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Error: {e}\\n\")\n",
    "    \n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"Print database statistics\"\"\"\n",
    "        doc_count = self.db_session.query(Document).count()\n",
    "        chunk_count = self.db_session.query(Chunk).count()\n",
    "        image_count = self.db_session.query(Image).count()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Database Statistics:\")\n",
    "        print(f\"   â€¢ Documents: {doc_count}\")\n",
    "        print(f\"   â€¢ Chunks: {chunk_count}\")\n",
    "        print(f\"   â€¢ Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3756e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--upload UPLOAD] [--chat] [--stats]\n",
      "                             [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/aireesm4/Library/Jupyter/runtime/kernel-v317f9d1f33af147b38e477d28ed4a1a8aed23e758.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Local RAG Chatbot with Image Understanding\")\n",
    "parser.add_argument('--upload', type=str, help='Upload and index a PDF file')\n",
    "parser.add_argument('--chat', action='store_true', help='Start interactive chat')\n",
    "parser.add_argument('--stats', action='store_true', help='Show database statistics')\n",
    "parser.add_argument('--model', type=str, default='llama3.2:3b', help='Ollama model to use')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Initialize config\n",
    "config = RAGConfig(chat_model=args.model)\n",
    "app = RAGApplication(config)\n",
    "\n",
    "# Check Ollama\n",
    "if not app.check_ollama():\n",
    "    print(\"âŒ Ollama is not running!\")\n",
    "    print(\"\\nTo start Ollama:\")\n",
    "    print(\"  1. Open a terminal\")\n",
    "    print(\"  2. Run: ollama serve\")\n",
    "    print(\"  3. Keep that terminal open\")\n",
    "    print(\"\\nThen run this script again.\")\n",
    "    return\n",
    "\n",
    "# Handle commands\n",
    "if args.upload:\n",
    "    app.index_documents([args.upload])\n",
    "\n",
    "elif args.chat:\n",
    "    app.initialize_chatbot()\n",
    "    app.interactive_chat()\n",
    "\n",
    "elif args.stats:\n",
    "    app.print_stats()\n",
    "\n",
    "else:\n",
    "    parser.print_help()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
