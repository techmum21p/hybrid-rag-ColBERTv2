{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ef7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Local RAG Chatbot with Image Understanding\\n===================================================\\n\\n✅ No Cloud Dependencies (runs 100% locally)\\n✅ No RAGatouille (direct Jina ColBERT v2 implementation)\\n✅ PyMuPDF4LLM for PDF conversion\\n✅ Image extraction and analysis with LLaVA vision model\\n✅ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\\n✅ Markdown-aware semantic chunking\\n✅ SQLite database for storage\\n\\nRequirements:\\n- Ollama (for LLMs: llama3.2:3b, llava:7b)\\n- Mac Mini M4 or similar (16GB RAM recommended)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Local RAG Chatbot with Image Understanding\n",
    "===================================================\n",
    "\n",
    "✅ No Cloud Dependencies (runs 100% locally)\n",
    "✅ No RAGatouille (direct Jina ColBERT v2 implementation)\n",
    "✅ PyMuPDF4LLM for PDF conversion\n",
    "✅ Image extraction and analysis with LLaVA vision model\n",
    "✅ Hybrid retrieval (BM25s + Jina ColBERT v2 + RRF + Reranking)\n",
    "✅ Markdown-aware semantic chunking\n",
    "✅ SQLite database for storage\n",
    "\n",
    "Requirements:\n",
    "- Ollama (for LLMs: llama3.2:3b, llava:7b)\n",
    "- Mac Mini M4 or similar (16GB RAM recommended)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f723da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Suppress tokenizers parallelism warning when forking\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress deprecation warnings from transformers/sentence-transformers\n",
    "warnings.filterwarnings('ignore', message='.*torch_dtype.*deprecated.*')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage  # Renamed to avoid conflict with database model\n",
    "\n",
    "# PDF and text processing\n",
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF for image extraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Retrieval\n",
    "import bm25s\n",
    "from bm25s.hf import BM25HF\n",
    "import Stemmer  # PyStemmer for stemming\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Database\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeBase\n",
    "\n",
    "# LLM\n",
    "import requests  # For Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a1fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for local RAG system\"\"\"\n",
    "    # Base directory (set to project root - parent of notebooks folder)\n",
    "    base_dir: str = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    \n",
    "    # Database\n",
    "    db_path: str = None\n",
    "    \n",
    "    # Chunking\n",
    "    min_chunk_size: int = 256\n",
    "    max_chunk_size: int = 512  # Reduced to match model's max_seq_length\n",
    "    chunk_overlap: int = 128\n",
    "    \n",
    "    # Retrieval\n",
    "    bm25_top_k: int = 100\n",
    "    colbert_top_k: int = 100\n",
    "    final_top_k: int = 15  # Increased from 10 to 15 for better coverage and reduced hallucination\n",
    "    \n",
    "    # Models\n",
    "    chat_model: str = \"llama3.2:3b\"\n",
    "    vision_model: str = \"gemma3:4b\"\n",
    "    embedding_model: str = \"jinaai/jina-colbert-v2\"\n",
    "    \n",
    "    # Ollama\n",
    "    ollama_url: str = \"http://localhost:11434\"\n",
    "    ollama_timeout: int = 300  # Increased timeout for slower models\n",
    "    \n",
    "    # Paths (will be set to absolute paths in __post_init__)\n",
    "    bm25_index_path: str = None\n",
    "    colbert_index_path: str = None\n",
    "    images_dir: str = None\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set absolute paths after initialization\"\"\"\n",
    "        if self.db_path is None:\n",
    "            self.db_path = os.path.join(self.base_dir, \"rag_local.db\")\n",
    "        if self.bm25_index_path is None:\n",
    "            self.bm25_index_path = os.path.join(self.base_dir, \"indexes\", \"bm25s\")\n",
    "        if self.colbert_index_path is None:\n",
    "            self.colbert_index_path = os.path.join(self.base_dir, \"indexes\", \"colbert\")\n",
    "        if self.images_dir is None:\n",
    "            self.images_dir = os.path.join(self.base_dir, \"extracted_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    filename = Column(String(255), nullable=False)\n",
    "    upload_date = Column(DateTime, default=datetime.utcnow)\n",
    "    total_pages = Column(Integer)\n",
    "    status = Column(String(50))\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = 'images'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    page_number = Column(Integer, nullable=False)\n",
    "    image_path = Column(String(500), nullable=False)\n",
    "    description = Column(Text)\n",
    "    image_type = Column(String(50))\n",
    "    ocr_text = Column(Text)\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = 'chunks'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    document_id = Column(Integer, nullable=False)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    text = Column(Text, nullable=False)\n",
    "    heading_path = Column(String(500))\n",
    "    token_count = Column(Integer)\n",
    "    has_images = Column(Boolean, default=False)\n",
    "    chunk_metadata = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3deff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA CLIENT WITH STREAMING SUPPORT\n",
    "# ============================================================================\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with Ollama API with streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.base_url = config.ollama_url\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        model: str, \n",
    "        prompt: str, \n",
    "        system: str = \"\",\n",
    "        images: List[str] = None,\n",
    "        timeout: int = 300,\n",
    "        stream: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text with Ollama (with optional streaming)\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        if system:\n",
    "            payload[\"system\"] = system\n",
    "        \n",
    "        if images:\n",
    "            payload[\"images\"] = images\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                # Streaming mode - print tokens as they arrive\n",
    "                response = requests.post(url, json=payload, timeout=timeout, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"response\" in chunk:\n",
    "                            token = chunk[\"response\"]\n",
    "                            print(token, end='', flush=True)\n",
    "                            full_response += token\n",
    "                        \n",
    "                        # Check if done\n",
    "                        if chunk.get(\"done\", False):\n",
    "                            break\n",
    "                \n",
    "                print()  # Newline after streaming\n",
    "                return full_response\n",
    "            else:\n",
    "                # Non-streaming mode - wait for complete response\n",
    "                response = requests.post(url, json=payload, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response.json()[\"response\"]\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\\n❌ Ollama timeout after {timeout}s - model may be too slow or stuck\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Ollama error: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def analyze_image(self, image_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Analyze image using Gemma3 multimodal model with enhanced OCR extraction\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"❌ Image not found: {image_path}\")\n",
    "            return {\n",
    "                'description': 'Image not found',\n",
    "                'type': 'error',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "            \n",
    "        # Read image and convert to base64\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                import base64\n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                \n",
    "            # Enhanced prompt for better OCR and context extraction\n",
    "            description_prompt = \"\"\"Analyze this image carefully and provide detailed information:\n",
    "\n",
    "1. TYPE: Classify this image (diagram, flowchart, chart, graph, table, screenshot, architecture diagram, code snippet, formula, etc.)\n",
    "\n",
    "2. DESCRIPTION: Describe what the image shows in 2-3 detailed sentences. Include:\n",
    "   - Main subject/purpose\n",
    "   - Key components or elements\n",
    "   - Relationships between elements (if applicable)\n",
    "   - Colors, arrows, or visual indicators (if relevant)\n",
    "\n",
    "3. TEXT: Extract ALL visible text from the image. This is CRITICAL for search accuracy.\n",
    "   - Include labels, titles, legends, annotations\n",
    "   - Include numbers, percentages, values\n",
    "   - Include code, formulas, equations\n",
    "   - Include any text in tables, boxes, or speech bubbles\n",
    "   - Preserve the order and structure where possible\n",
    "   - If no text is visible, write \"No text visible\"\n",
    "\n",
    "Format your response EXACTLY as follows:\n",
    "TYPE: [type]\n",
    "DESCRIPTION: [description]\n",
    "TEXT: [all extracted text]\"\"\"\n",
    "            \n",
    "            response = self.generate(\n",
    "                model=\"gemma3:4b\",  # Using Gemma3 for image analysis\n",
    "                prompt=description_prompt,\n",
    "                images=[image_data],\n",
    "                timeout=120,  # Increased timeout for image analysis\n",
    "                stream=False  # Don't stream for image analysis\n",
    "            )\n",
    "            \n",
    "            # Parse response with more robust parsing\n",
    "            result = {\n",
    "                'description': 'No description generated',\n",
    "                'type': 'unknown',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "            \n",
    "            if response:\n",
    "                # Try to extract sections using regex\n",
    "                import re\n",
    "                \n",
    "                # Try to find TYPE\n",
    "                type_match = re.search(r'TYPE:\\s*(.+)', response, re.IGNORECASE)\n",
    "                if type_match:\n",
    "                    result['type'] = type_match.group(1).strip().lower()\n",
    "                \n",
    "                # Try to find DESCRIPTION\n",
    "                desc_match = re.search(r'DESCRIPTION:([\\s\\S]*?)(?=TEXT:|$)', response, re.IGNORECASE)\n",
    "                if desc_match:\n",
    "                    result['description'] = desc_match.group(1).strip()\n",
    "                \n",
    "                # Try to find TEXT\n",
    "                text_match = re.search(r'TEXT:([\\s\\S]*)', response, re.IGNORECASE)\n",
    "                if text_match:\n",
    "                    ocr = text_match.group(1).strip()\n",
    "                    # Don't store if it's just the \"no text\" message\n",
    "                    if ocr.lower() != \"no text visible\":\n",
    "                        result['ocr_text'] = ocr\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing image {image_path}: {str(e)}\")\n",
    "            return {\n",
    "                'description': f'Error analyzing image: {str(e)}',\n",
    "                'type': 'error',\n",
    "                'ocr_text': ''\n",
    "            }\n",
    "    \n",
    "    def chat(\n",
    "        self, \n",
    "        messages: List[Dict[str, str]], \n",
    "        context: str = None,\n",
    "        stream: bool = True  # Enable streaming by default!\n",
    "    ) -> str:\n",
    "        \"\"\"Chat with context - with EXTREMELY strong anti-hallucination instructions and streaming\"\"\"\n",
    "        \n",
    "        # Build system message with EXTREMELY STRONG anti-hallucination instructions\n",
    "        if context:\n",
    "            system_msg = \"\"\"You are a document question-answering assistant. Follow these rules with ABSOLUTE strictness:\n",
    "\n",
    "!! CRITICAL RULES - NO EXCEPTIONS !!\n",
    "\n",
    "1. You MUST ONLY use information explicitly stated in the context below\n",
    "2. DO NOT use any knowledge outside the provided context\n",
    "3. DO NOT make inferences, assumptions, or educated guesses\n",
    "4. DO NOT mention products, services, or technologies not explicitly in the context\n",
    "5. If information is NOT in the context, respond EXACTLY: \"I don't have that information in the provided documents\"\n",
    "6. DO NOT provide links, URLs, or suggest where to find more information\n",
    "7. DO NOT say things like \"for the latest information\" or \"check the official website\"\n",
    "8. When answering, cite the specific source number (e.g., \"According to Source 2...\")\n",
    "\n",
    "CONTEXT FROM DOCUMENTS:\n",
    "\"\"\" + context + \"\"\"\n",
    "\n",
    "Remember: If it's not in the context above, you DON'T KNOW IT. Period.\"\"\"\n",
    "        else:\n",
    "            system_msg = \"You are a helpful AI assistant. Please provide accurate and helpful responses based only on what you know.\"\n",
    "        \n",
    "        # Build prompt from messages\n",
    "        prompt = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\" \n",
    "            for msg in messages\n",
    "        ])\n",
    "        \n",
    "        return self.generate(\n",
    "            model=self.config.chat_model,\n",
    "            prompt=prompt,\n",
    "            system=system_msg,\n",
    "            timeout=self.config.ollama_timeout,\n",
    "            stream=stream  # Pass streaming flag\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75436919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKDOWN-AWARE SEMANTIC CHUNKER\n",
    "# ============================================================================\n",
    "\n",
    "class MarkdownSemanticChunker:\n",
    "    \"\"\"Intelligent markdown chunking that respects document structure\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def chunk_markdown(self, markdown_text: str, doc_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Create semantically meaningful chunks\"\"\"\n",
    "        sections = self._parse_markdown_hierarchy(markdown_text)\n",
    "        chunks = self._create_chunks_from_sections(sections, doc_context)\n",
    "        optimized_chunks = self._optimize_chunks(chunks)\n",
    "        return optimized_chunks\n",
    "    \n",
    "    def _parse_markdown_hierarchy(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse markdown into hierarchical sections\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        heading_stack = []\n",
    "        \n",
    "        for line in lines:\n",
    "            heading_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n",
    "            \n",
    "            if heading_match:\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                level = len(heading_match.group(1))\n",
    "                title = heading_match.group(2).strip()\n",
    "                \n",
    "                heading_stack = [(lvl, ttl) for lvl, ttl in heading_stack if lvl < level]\n",
    "                heading_stack.append((level, title))\n",
    "                \n",
    "                parent_path = ' > '.join([ttl for _, ttl in heading_stack[:-1]])\n",
    "                full_path = ' > '.join([ttl for _, ttl in heading_stack])\n",
    "                \n",
    "                current_section = {\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'content': '',\n",
    "                    'parent_path': parent_path,\n",
    "                    'full_path': full_path\n",
    "                }\n",
    "            else:\n",
    "                if current_section is not None:\n",
    "                    current_section['content'] += line + '\\n'\n",
    "                else:\n",
    "                    if not sections or sections[-1]['level'] != 0:\n",
    "                        sections.append({\n",
    "                            'level': 0,\n",
    "                            'title': 'Introduction',\n",
    "                            'content': line + '\\n',\n",
    "                            'parent_path': '',\n",
    "                            'full_path': 'Introduction'\n",
    "                        })\n",
    "                    else:\n",
    "                        sections[-1]['content'] += line + '\\n'\n",
    "        \n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_chunks_from_sections(self, sections: List[Dict], doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Create chunks from sections\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = None\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text = self._format_section_text(section)\n",
    "            section_tokens = self._count_tokens(section_text)\n",
    "            \n",
    "            if section_tokens > self.config.max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                split_chunks = self._split_large_section(section, doc_context)\n",
    "                chunks.extend(split_chunks)\n",
    "            \n",
    "            elif section_tokens >= self.config.min_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = None\n",
    "                \n",
    "                chunks.append({\n",
    "                    'text': section_text,\n",
    "                    'heading_path': section['full_path'],\n",
    "                    'level': section['level'],\n",
    "                    'token_count': section_tokens,\n",
    "                    'doc_context': doc_context,\n",
    "                    'type': 'section'\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                if current_chunk is None:\n",
    "                    current_chunk = {\n",
    "                        'text': section_text,\n",
    "                        'heading_path': section['parent_path'] or section['title'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': section_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'accumulated',\n",
    "                        'sections': [section['title']]\n",
    "                    }\n",
    "                else:\n",
    "                    combined_text = current_chunk['text'] + '\\n\\n' + section_text\n",
    "                    combined_tokens = self._count_tokens(combined_text)\n",
    "                    \n",
    "                    if combined_tokens <= self.config.max_chunk_size:\n",
    "                        current_chunk['text'] = combined_text\n",
    "                        current_chunk['token_count'] = combined_tokens\n",
    "                        current_chunk['sections'].append(section['title'])\n",
    "                    else:\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = {\n",
    "                            'text': section_text,\n",
    "                            'heading_path': section['parent_path'] or section['title'],\n",
    "                            'level': section['level'],\n",
    "                            'token_count': section_tokens,\n",
    "                            'doc_context': doc_context,\n",
    "                            'type': 'accumulated',\n",
    "                            'sections': [section['title']]\n",
    "                        }\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_large_section(self, section: Dict, doc_context: str) -> List[Dict]:\n",
    "        \"\"\"Split large section at paragraph boundaries\"\"\"\n",
    "        heading_text = f\"# {section['title']}\\n\\n\"\n",
    "        parent_context = f\"Context: {section['parent_path']}\\n\\n\" if section['parent_path'] else \"\"\n",
    "        \n",
    "        paragraphs = re.split(r'\\n\\n+', section['content'].strip())\n",
    "        \n",
    "        chunks = []\n",
    "        current_text = heading_text + parent_context\n",
    "        current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = self._count_tokens(para)\n",
    "            \n",
    "            if current_tokens + para_tokens <= self.config.max_chunk_size:\n",
    "                current_text += para + '\\n\\n'\n",
    "                current_tokens += para_tokens\n",
    "            else:\n",
    "                if current_text.strip() != heading_text.strip():\n",
    "                    chunks.append({\n",
    "                        'text': current_text.strip(),\n",
    "                        'heading_path': section['full_path'],\n",
    "                        'level': section['level'],\n",
    "                        'token_count': current_tokens,\n",
    "                        'doc_context': doc_context,\n",
    "                        'type': 'split_section',\n",
    "                        'part': len(chunks) + 1\n",
    "                    })\n",
    "                \n",
    "                current_text = heading_text + parent_context + para + '\\n\\n'\n",
    "                current_tokens = self._count_tokens(current_text)\n",
    "        \n",
    "        if current_text.strip():\n",
    "            chunks.append({\n",
    "                'text': current_text.strip(),\n",
    "                'heading_path': section['full_path'],\n",
    "                'level': section['level'],\n",
    "                'token_count': current_tokens,\n",
    "                'doc_context': doc_context,\n",
    "                'type': 'split_section',\n",
    "                'part': len(chunks) + 1\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Merge very small chunks\"\"\"\n",
    "        optimized = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(chunks):\n",
    "            chunk = chunks[i]\n",
    "            \n",
    "            if (chunk['token_count'] < self.config.min_chunk_size and \n",
    "                i < len(chunks) - 1):\n",
    "                \n",
    "                next_chunk = chunks[i + 1]\n",
    "                combined_text = chunk['text'] + '\\n\\n' + next_chunk['text']\n",
    "                combined_tokens = self._count_tokens(combined_text)\n",
    "                \n",
    "                if combined_tokens <= self.config.max_chunk_size:\n",
    "                    merged_chunk = {\n",
    "                        'text': combined_text,\n",
    "                        'heading_path': chunk['heading_path'],\n",
    "                        'token_count': combined_tokens,\n",
    "                        'doc_context': chunk['doc_context'],\n",
    "                        'type': 'merged'\n",
    "                    }\n",
    "                    optimized.append(merged_chunk)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            \n",
    "            optimized.append(chunk)\n",
    "            i += 1\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _format_section_text(self, section: Dict) -> str:\n",
    "        \"\"\"Format section with heading and context\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if section['parent_path']:\n",
    "            parts.append(f\"[Context: {section['parent_path']}]\")\n",
    "        \n",
    "        if section['title'] and section['title'] != 'Introduction':\n",
    "            heading_prefix = '#' * section['level']\n",
    "            parts.append(f\"{heading_prefix} {section['title']}\")\n",
    "        \n",
    "        parts.append(section['content'].strip())\n",
    "        \n",
    "        return '\\n\\n'.join(parts)\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text with truncation\"\"\"\n",
    "        return len(self.tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR WITH IMAGE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF processing with image extraction and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.ollama = ollama_client\n",
    "        self.chunker = MarkdownSemanticChunker(config)\n",
    "        \n",
    "        # Create images directory\n",
    "        os.makedirs(config.images_dir, exist_ok=True)\n",
    "    \n",
    "    def _sanitize_utf8(self, text: str) -> str:\n",
    "        \"\"\"Sanitize text to remove invalid UTF-8 characters\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Encode to UTF-8 with error handling, then decode back\n",
    "        # This removes any invalid UTF-8 sequences\n",
    "        try:\n",
    "            # First try strict encoding\n",
    "            return text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️  UTF-8 sanitization error: {e}\")\n",
    "            # Fallback: replace problematic characters\n",
    "            return text.encode('ascii', errors='ignore').decode('ascii', errors='ignore')\n",
    "    \n",
    "    def pdf_to_markdown(self, pdf_path: str) -> str:\n",
    "        \"\"\"Convert PDF to Markdown using PyMuPDF4LLM\"\"\"\n",
    "        markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "        # Sanitize to remove invalid UTF-8\n",
    "        return self._sanitize_utf8(markdown_text)\n",
    "    \n",
    "    def _group_nearby_rectangles(self, rects: List[fitz.Rect], proximity_threshold: float = 20) -> List[List[int]]:\n",
    "        \"\"\"Group rectangles that are close to each other\"\"\"\n",
    "        if not rects:\n",
    "            return []\n",
    "\n",
    "        # Each rect gets assigned to a group\n",
    "        groups = []\n",
    "        assigned = [False] * len(rects)\n",
    "\n",
    "        for i, rect in enumerate(rects):\n",
    "            if assigned[i]:\n",
    "                continue\n",
    "\n",
    "            # Start a new group\n",
    "            current_group = [i]\n",
    "            assigned[i] = True\n",
    "\n",
    "            # Find all rects that should be in this group\n",
    "            changed = True\n",
    "            while changed:\n",
    "                changed = False\n",
    "                for j, other_rect in enumerate(rects):\n",
    "                    if assigned[j]:\n",
    "                        continue\n",
    "\n",
    "                    # Check if this rect is close to any rect in current group\n",
    "                    for group_idx in current_group:\n",
    "                        group_rect = rects[group_idx]\n",
    "\n",
    "                        # Calculate distance between rectangles\n",
    "                        # Expand each rect by proximity_threshold and check for intersection\n",
    "                        expanded_group = fitz.Rect(\n",
    "                            group_rect.x0 - proximity_threshold,\n",
    "                            group_rect.y0 - proximity_threshold,\n",
    "                            group_rect.x1 + proximity_threshold,\n",
    "                            group_rect.y1 + proximity_threshold\n",
    "                        )\n",
    "\n",
    "                        if expanded_group.intersects(other_rect):\n",
    "                            current_group.append(j)\n",
    "                            assigned[j] = True\n",
    "                            changed = True\n",
    "                            break\n",
    "\n",
    "            groups.append(current_group)\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def extract_images_from_pdf(\n",
    "        self,\n",
    "        pdf_path: str,\n",
    "        document_id: int,\n",
    "        min_image_size: int = 50,  # Minimum width/height in pixels\n",
    "        proximity_threshold: float = 20  # Group images within this distance (points)\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract images from PDF with intelligent grouping.\n",
    "        Groups nearby images together to capture complete diagrams.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            image_list = page.get_images(full=True)\n",
    "\n",
    "            if not image_list:\n",
    "                continue\n",
    "\n",
    "            # Get bounding boxes for all images on this page\n",
    "            image_bboxes = []\n",
    "            for img_info in image_list:\n",
    "                xref = img_info[0]\n",
    "                # Get all instances of this image on the page\n",
    "                rects = page.get_image_rects(xref)\n",
    "                if rects:\n",
    "                    for rect in rects:\n",
    "                        # Check minimum size\n",
    "                        width = rect.width\n",
    "                        height = rect.height\n",
    "                        if width >= min_image_size and height >= min_image_size:\n",
    "                            image_bboxes.append({\n",
    "                                'rect': rect,\n",
    "                                'xref': xref,\n",
    "                                'width': width,\n",
    "                                'height': height\n",
    "                            })\n",
    "\n",
    "            if not image_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Group nearby images\n",
    "            rects_only = [bbox['rect'] for bbox in image_bboxes]\n",
    "            groups = self._group_nearby_rectangles(rects_only, proximity_threshold)\n",
    "\n",
    "            # Process each group\n",
    "            for group_idx, group in enumerate(groups):\n",
    "                if len(group) == 1:\n",
    "                    # Single image - extract normally\n",
    "                    bbox = image_bboxes[group[0]]\n",
    "                    try:\n",
    "                        base_image = doc.extract_image(bbox['xref'])\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        pil_image = PILImage.open(io.BytesIO(image_bytes))\n",
    "\n",
    "                        # Save image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_img{len(images)+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        if pil_image.mode == 'RGBA':\n",
    "                            pil_image = pil_image.convert('RGB')\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': False,\n",
    "                            'bbox': bbox['rect']\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ⚠️  Failed to extract single image on page {page_num+1}: {e}\")\n",
    "\n",
    "                else:\n",
    "                    # Multiple images grouped together - capture as screenshot\n",
    "                    # Calculate bounding box that encompasses all images in group\n",
    "                    union_rect = image_bboxes[group[0]]['rect']\n",
    "                    for idx in group[1:]:\n",
    "                        union_rect = union_rect | image_bboxes[idx]['rect']  # Union of rectangles\n",
    "\n",
    "                    # Add some padding\n",
    "                    padding = 5\n",
    "                    union_rect = fitz.Rect(\n",
    "                        max(0, union_rect.x0 - padding),\n",
    "                        max(0, union_rect.y0 - padding),\n",
    "                        min(page.rect.width, union_rect.x1 + padding),\n",
    "                        min(page.rect.height, union_rect.y1 + padding)\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        # Render this region as an image\n",
    "                        mat = fitz.Matrix(2, 2)  # 2x zoom for better quality\n",
    "                        pix = page.get_pixmap(matrix=mat, clip=union_rect)\n",
    "\n",
    "                        # Convert to PIL Image\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        pil_image = PILImage.open(io.BytesIO(img_data))\n",
    "\n",
    "                        # Save composite image\n",
    "                        image_filename = f\"doc{document_id}_page{page_num+1}_composite{group_idx+1}.png\"\n",
    "                        image_path = os.path.join(self.config.images_dir, image_filename)\n",
    "\n",
    "                        pil_image.save(image_path, 'PNG')\n",
    "\n",
    "                        images.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'image_path': image_path,\n",
    "                            'image_index': len(images),\n",
    "                            'is_composite': True,\n",
    "                            'num_components': len(group),\n",
    "                            'bbox': union_rect\n",
    "                        })\n",
    "\n",
    "                        print(f\"    📊 Grouped {len(group)} images into composite on page {page_num+1}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ⚠️  Failed to create composite image on page {page_num+1}: {e}\")\n",
    "\n",
    "        doc.close()\n",
    "        return images\n",
    "    \n",
    "    def analyze_images(\n",
    "        self, \n",
    "        images: List[Dict],\n",
    "        document_id: int,\n",
    "        db_session\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Analyze images with vision model and save to database\"\"\"\n",
    "        image_ids = []\n",
    "        \n",
    "        for idx, img_info in enumerate(images):\n",
    "            print(f\"    Analyzing image {idx+1} on page {img_info['page_number']}...\", end=' ')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Analyze with vision model\n",
    "            analysis = self.ollama.analyze_image(img_info['image_path'])\n",
    "            \n",
    "            # Save to database with UTF-8 sanitization\n",
    "            image_record = Image(\n",
    "                document_id=document_id,\n",
    "                page_number=img_info['page_number'],\n",
    "                image_path=img_info['image_path'],\n",
    "                description=self._sanitize_utf8(analysis['description']),\n",
    "                image_type=self._sanitize_utf8(analysis['type']),\n",
    "                ocr_text=self._sanitize_utf8(analysis['ocr_text'])\n",
    "            )\n",
    "            db_session.add(image_record)\n",
    "            db_session.flush()\n",
    "            \n",
    "            image_ids.append(image_record.id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ ({elapsed:.1f}s)\")\n",
    "        \n",
    "        db_session.commit()\n",
    "        return image_ids\n",
    "    \n",
    "    def enrich_chunks_with_images(\n",
    "        self,\n",
    "        chunks: List[Dict],\n",
    "        images_data: List[Dict],\n",
    "        db_session\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Add image context (description + OCR text) to relevant chunks for better search accuracy\"\"\"\n",
    "        \n",
    "        enriched_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_copy = chunk.copy()\n",
    "            \n",
    "            # Find images that might be relevant to this chunk\n",
    "            # Simple heuristic: chunks that mention visual content keywords\n",
    "            relevant_images = []\n",
    "            \n",
    "            for img in images_data:\n",
    "                if any(keyword in chunk['text'].lower() for keyword in \n",
    "                       ['figure', 'image', 'diagram', 'chart', 'screenshot', 'see below', 'shown in']):\n",
    "                    relevant_images.append(img)\n",
    "            \n",
    "            if relevant_images:\n",
    "                # Build comprehensive image context including OCR text\n",
    "                image_context = \"\\n\\n[Images in this section]:\\n\"\n",
    "                image_metadata = []\n",
    "                \n",
    "                for img in relevant_images:\n",
    "                    # Add type and description\n",
    "                    image_context += f\"- {img['type'].capitalize()}: {img['description']}\\n\"\n",
    "                    \n",
    "                    # CRITICAL: Add OCR text if available (makes text in images searchable!)\n",
    "                    if img.get('ocr_text') and img['ocr_text'].strip():\n",
    "                        image_context += f\"  Text visible in image: {img['ocr_text']}\\n\"\n",
    "                    \n",
    "                    image_metadata.append({\n",
    "                        'path': img['image_path'],\n",
    "                        'description': img['description'],\n",
    "                        'type': img['type'],\n",
    "                        'ocr_text': img.get('ocr_text', '')\n",
    "                    })\n",
    "                \n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'] + image_context)\n",
    "                chunk_copy['has_images'] = True\n",
    "                chunk_copy['image_paths'] = [img['image_path'] for img in relevant_images]\n",
    "                chunk_copy['image_metadata'] = image_metadata\n",
    "            else:\n",
    "                chunk_copy['text'] = self._sanitize_utf8(chunk['text'])\n",
    "                chunk_copy['has_images'] = False\n",
    "            \n",
    "            enriched_chunks.append(chunk_copy)\n",
    "        \n",
    "        return enriched_chunks\n",
    "    \n",
    "    def process_document(\n",
    "        self, \n",
    "        pdf_path: str,\n",
    "        db_session\n",
    "    ) -> Tuple[List[Dict], int]:\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Convert to markdown\n",
    "        print(\"\\n[Step 1/5] Converting PDF to Markdown...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        markdown_text = self.pdf_to_markdown(pdf_path)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        print(f\"  • Extracted {len(markdown_text):,} characters\")\n",
    "        \n",
    "        # Create document record\n",
    "        doc = Document(\n",
    "            filename=os.path.basename(pdf_path),\n",
    "            status='processing'\n",
    "        )\n",
    "        db_session.add(doc)\n",
    "        db_session.commit()\n",
    "        \n",
    "        # Step 2: Extract and analyze images\n",
    "        print(\"\\n[Step 2/5] Extracting and analyzing images...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        images = self.extract_images_from_pdf(pdf_path, doc.id)\n",
    "        \n",
    "        if images:\n",
    "            image_ids = self.analyze_images(images, doc.id, db_session)\n",
    "            \n",
    "            # Get image data for enrichment\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_record = db_session.query(Image).filter_by(id=img_id).first()\n",
    "                if img_record:\n",
    "                    images_data.append({\n",
    "                        'image_path': img_record.image_path,\n",
    "                        'description': img_record.description,\n",
    "                        'type': img_record.image_type,\n",
    "                        'ocr_text': img_record.ocr_text\n",
    "                    })\n",
    "        else:\n",
    "            images_data = []\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ Completed in {elapsed:.2f}s\")\n",
    "        print(f\"  • Extracted {len(images)} images\")\n",
    "        if images:\n",
    "            print(f\"  • Vision analysis: ✓\")\n",
    "        \n",
    "        # Step 3: Markdown-aware semantic chunking\n",
    "        print(\"\\n[Step 3/5] Markdown-aware semantic chunking...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        doc_context = f\"Document: {os.path.basename(pdf_path)}\\n\\n{markdown_text[:500]}\"\n",
    "        chunks = self.chunker.chunk_markdown(markdown_text, doc_context)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        print(f\"  • Created {len(chunks)} semantic chunks\")\n",
    "        \n",
    "        # Step 4: Enrich chunks with image context (INCLUDING OCR TEXT!)\n",
    "        print(\"\\n[Step 4/5] Enriching chunks with image context...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        if images_data:\n",
    "            chunks = self.enrich_chunks_with_images(chunks, images_data, db_session)\n",
    "            chunks_with_images = sum(1 for c in chunks if c.get('has_images', False))\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ {elapsed:.2f}s\")\n",
    "            print(f\"  • {chunks_with_images} chunks enriched with image context + OCR text\")\n",
    "        else:\n",
    "            # Still sanitize even if no images\n",
    "            for chunk in chunks:\n",
    "                chunk['text'] = self._sanitize_utf8(chunk['text'])\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"✓ {elapsed:.2f}s\")\n",
    "            print(f\"  • No images to enrich\")\n",
    "        \n",
    "        # Step 5: Save to database\n",
    "        print(\"\\n[Step 5/5] Saving chunks to database...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_record = Chunk(\n",
    "                document_id=doc.id,\n",
    "                chunk_index=idx,\n",
    "                text=self._sanitize_utf8(chunk['text']),  # Sanitize before saving\n",
    "                heading_path=chunk.get('heading_path', ''),\n",
    "                token_count=chunk.get('token_count', 0),\n",
    "                has_images=chunk.get('has_images', False),\n",
    "                chunk_metadata=json.dumps({\n",
    "                    k: v for k, v in chunk.items() \n",
    "                    if k not in ['text', 'heading_path', 'token_count', 'has_images']\n",
    "                })\n",
    "            )\n",
    "            db_session.add(chunk_record)\n",
    "        \n",
    "        doc.status = 'indexed'\n",
    "        db_session.commit()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "        \n",
    "        return chunks, doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08725b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JINA COLBERT V2 RETRIEVER (NO RAGATOUILLE!)\n",
    "# ============================================================================\n",
    "\n",
    "class JinaColBERTRetriever:\n",
    "    \"\"\"Direct implementation of Jina ColBERT v2 (no RAGatouille dependency)\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.model = SentenceTransformer(\n",
    "            config.embedding_model,\n",
    "            trust_remote_code=True,\n",
    "            device=config.device\n",
    "        )\n",
    "        # Set max sequence length to avoid truncation warnings\n",
    "        self.model.max_seq_length = 512\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Index corpus with ColBERT embeddings\"\"\"\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        print(f\"  Encoding {len(corpus)} documents...\")\n",
    "        \n",
    "        # Encode corpus (this gives us token-level embeddings)\n",
    "        # Truncate long sequences to avoid errors\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            corpus,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Save to disk\n",
    "        os.makedirs(self.config.colbert_index_path, exist_ok=True)\n",
    "        torch.save({\n",
    "            'embeddings': self.corpus_embeddings,\n",
    "            'corpus': corpus\n",
    "        }, os.path.join(self.config.colbert_index_path, 'index.pt'))\n",
    "    \n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.config.colbert_index_path, 'index.pt')\n",
    "        data = torch.load(index_file, map_location=self.config.device)\n",
    "        self.corpus_embeddings = data['embeddings']\n",
    "        self.corpus = data['corpus']\n",
    "    \n",
    "    def search(self, query: str, k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using MaxSim scoring\"\"\"\n",
    "        if not self.corpus or len(self.corpus) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Handle single item corpus\n",
    "        if len(self.corpus) == 1:\n",
    "            return [{\n",
    "                'document_id': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'text': self.corpus[0]\n",
    "            }]\n",
    "        \n",
    "        # Get top-k\n",
    "        k = min(k, len(scores))\n",
    "        top_k_indices = torch.topk(scores, k=k).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append({\n",
    "                'document_id': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'text': self.corpus[idx] if self.corpus else None\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank documents with more accurate scoring\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Encode query and documents\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        doc_embeddings = self.model.encode(\n",
    "            documents, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=8  # Smaller batch size for stability\n",
    "        )\n",
    "        \n",
    "        # Compute MaxSim scores\n",
    "        scores = self._maxsim_score(query_embedding, doc_embeddings)\n",
    "        \n",
    "        # Handle single document\n",
    "        if len(documents) == 1:\n",
    "            return [{\n",
    "                'result_index': 0,\n",
    "                'score': float(scores.item() if scores.dim() == 0 else scores[0]),\n",
    "                'rank': 1,\n",
    "                'text': documents[0]\n",
    "            }]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(sorted_indices[:k]):\n",
    "            results.append({\n",
    "                'result_index': int(idx),\n",
    "                'score': float(scores[idx]),\n",
    "                'rank': rank + 1,\n",
    "                'text': documents[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _maxsim_score(\n",
    "        self, \n",
    "        query_embedding: torch.Tensor, \n",
    "        doc_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute MaxSim score between query and documents\n",
    "        \n",
    "        MaxSim: For each query token, find max similarity with all doc tokens,\n",
    "        then average across query tokens\n",
    "        \"\"\"\n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 1:\n",
    "            query_embedding = query_embedding.unsqueeze(0)\n",
    "        if doc_embeddings.dim() == 1:\n",
    "            doc_embeddings = doc_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # For 2D embeddings (single vector per doc), compute cosine similarity directly\n",
    "        if query_embedding.dim() == 2 and doc_embeddings.dim() == 2:\n",
    "            # Normalize embeddings\n",
    "            query_norm = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            doc_norm = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            scores = torch.mm(query_norm, doc_norm.t())\n",
    "            \n",
    "            # Return as 1D tensor\n",
    "            return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()\n",
    "        \n",
    "        # For 3D embeddings (token-level), use mean pooling\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_vec = query_embedding.mean(dim=1)\n",
    "        else:\n",
    "            query_vec = query_embedding\n",
    "            \n",
    "        if doc_embeddings.dim() == 3:\n",
    "            doc_vec = doc_embeddings.mean(dim=1)\n",
    "        else:\n",
    "            doc_vec = doc_embeddings\n",
    "        \n",
    "        # Normalize\n",
    "        query_vec = torch.nn.functional.normalize(query_vec, p=2, dim=-1)\n",
    "        doc_vec = torch.nn.functional.normalize(doc_vec, p=2, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        if query_vec.dim() == 1:\n",
    "            query_vec = query_vec.unsqueeze(0)\n",
    "        if doc_vec.dim() == 1:\n",
    "            doc_vec = doc_vec.unsqueeze(0)\n",
    "            \n",
    "        scores = torch.mm(query_vec, doc_vec.t())\n",
    "        \n",
    "        # Return as 1D tensor\n",
    "        return scores.squeeze(0) if scores.size(0) == 1 else scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DUAL INDEXER (BM25s + Jina ColBERT)\n",
    "# ============================================================================\n",
    "\n",
    "class DualIndexer:\n",
    "    \"\"\"Manages BM25s and Jina ColBERT v2 indexes\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.bm25_retriever = None\n",
    "        self.colbert_retriever = JinaColBERTRetriever(config)\n",
    "    \n",
    "    def build_bm25_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build BM25s index\"\"\"\n",
    "        print(\"\\n[BM25s] Building lexical search index...\", end=' ')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create stemmer\n",
    "        stemmer = Stemmer.Stemmer(\"english\")\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        corpus_tokens = bm25s.tokenize(\n",
    "            corpus, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=stemmer\n",
    "        )\n",
    "        \n",
    "        self.bm25_retriever = bm25s.BM25()\n",
    "        self.bm25_retriever.index(corpus_tokens)\n",
    "        \n",
    "        os.makedirs(self.config.bm25_index_path, exist_ok=True)\n",
    "        self.bm25_retriever.save(self.config.bm25_index_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ {elapsed:.2f}s\")\n",
    "    \n",
    "    def build_colbert_index(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build Jina ColBERT v2 index\"\"\"\n",
    "        print(\"\\n[ColBERT] Building semantic search index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.colbert_retriever.index(corpus)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  ✓ {elapsed:.2f}s\")\n",
    "    \n",
    "    def load_indexes(self) -> None:\n",
    "        \"\"\"Load indexes from disk\"\"\"\n",
    "        self.bm25_retriever = bm25s.BM25.load(self.config.bm25_index_path)\n",
    "        self.colbert_retriever.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506c9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID RETRIEVER WITH RRF AND RERANKING\n",
    "# ============================================================================\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Three-stage retrieval: BM25s + ColBERT + ColBERT Reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, indexer: DualIndexer, db_session, corpus_to_chunk_id: List[int] = None):\n",
    "        self.config = config\n",
    "        self.indexer = indexer\n",
    "        self.db_session = db_session\n",
    "        self.stemmer = Stemmer.Stemmer(\"english\")\n",
    "        # CRITICAL: Mapping from corpus index to database chunk ID\n",
    "        self.corpus_to_chunk_id = corpus_to_chunk_id or []\n",
    "    \n",
    "    def retrieve(self, query: str, top_k_final: int = None) -> List[Dict]:\n",
    "        \"\"\"Three-stage hybrid retrieval with detailed scoring\"\"\"\n",
    "        if top_k_final is None:\n",
    "            top_k_final = self.config.final_top_k\n",
    "        \n",
    "        print(f\"\\n🔍 Retrieving relevant chunks...\")\n",
    "        \n",
    "        # Get corpus size to adjust k values\n",
    "        corpus_size = len(self.indexer.colbert_retriever.corpus) if self.indexer.colbert_retriever.corpus else 0\n",
    "        \n",
    "        # Adjust k values based on corpus size\n",
    "        bm25_k = min(self.config.bm25_top_k, corpus_size) if corpus_size > 0 else self.config.bm25_top_k\n",
    "        colbert_k = min(self.config.colbert_top_k, corpus_size) if corpus_size > 0 else self.config.colbert_top_k\n",
    "        \n",
    "        print(f\"   • Corpus size: {corpus_size}, using k={bm25_k} for retrieval\")\n",
    "        \n",
    "        # Stage 1: BM25s\n",
    "        start = time.time()\n",
    "        bm25_results = self._bm25_search(query, k=bm25_k)\n",
    "        bm25_time = time.time() - start\n",
    "        print(f\"   • BM25s: {bm25_time:.3f}s ({len(bm25_results)} results)\")\n",
    "        \n",
    "        # Stage 2: ColBERT\n",
    "        start = time.time()\n",
    "        colbert_results = self._colbert_search(query, k=colbert_k)\n",
    "        colbert_time = time.time() - start\n",
    "        print(f\"   • ColBERT: {colbert_time:.3f}s ({len(colbert_results)} results)\")\n",
    "        \n",
    "        # Fusion\n",
    "        start = time.time()\n",
    "        fused_results = self._reciprocal_rank_fusion(bm25_results, colbert_results)\n",
    "        candidates = fused_results[:min(50, len(fused_results))]\n",
    "        fusion_time = time.time() - start\n",
    "        print(f\"   • Fusion: {fusion_time:.3f}s ({len(candidates)} candidates)\")\n",
    "        \n",
    "        # Fetch chunks - USING THE MAPPING!\n",
    "        start = time.time()\n",
    "        candidate_corpus_indices = [r['corpus_index'] for r in candidates]\n",
    "        candidate_chunks = self._fetch_chunks_from_db(candidate_corpus_indices)\n",
    "        \n",
    "        # PRESERVE INTERMEDIATE SCORES\n",
    "        # Map corpus_index to intermediate scores\n",
    "        score_map = {}\n",
    "        for bm25_result in bm25_results:\n",
    "            idx = bm25_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['bm25_score'] = bm25_result['score']\n",
    "        \n",
    "        for colbert_result in colbert_results:\n",
    "            idx = colbert_result['corpus_index']\n",
    "            if idx not in score_map:\n",
    "                score_map[idx] = {}\n",
    "            score_map[idx]['colbert_score'] = colbert_result['score']\n",
    "        \n",
    "        for fused_result in candidates:\n",
    "            idx = fused_result['corpus_index']\n",
    "            if idx in score_map:\n",
    "                score_map[idx]['rrf_score'] = fused_result['rrf_score']\n",
    "        \n",
    "        # Add intermediate scores to chunks\n",
    "        for i, chunk in enumerate(candidate_chunks):\n",
    "            corpus_idx = candidate_corpus_indices[i]\n",
    "            if corpus_idx in score_map:\n",
    "                chunk['intermediate_scores'] = score_map[corpus_idx]\n",
    "        \n",
    "        fetch_time = time.time() - start\n",
    "        print(f\"   • Fetch: {fetch_time:.3f}s ({len(candidate_chunks)} chunks)\")\n",
    "        \n",
    "        # Stage 3: Rerank\n",
    "        start = time.time()\n",
    "        final_k = min(top_k_final, len(candidate_chunks))\n",
    "        reranked_results = self._colbert_rerank(query, candidate_chunks, top_k=final_k)\n",
    "        rerank_time = time.time() - start\n",
    "        print(f\"   • Rerank: {rerank_time:.3f}s (top {len(reranked_results)})\")\n",
    "        \n",
    "        total_time = bm25_time + colbert_time + fusion_time + fetch_time + rerank_time\n",
    "        print(f\"   ✓ Total retrieval: {total_time:.3f}s\")\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 1: BM25s lexical search\"\"\"\n",
    "        query_tokens = bm25s.tokenize(\n",
    "            query, \n",
    "            stopwords=\"en\",\n",
    "            stemmer=self.stemmer\n",
    "        )\n",
    "        \n",
    "        results, scores = self.indexer.bm25_retriever.retrieve(query_tokens, k=k)\n",
    "        \n",
    "        return [\n",
    "            {'corpus_index': int(results[0][i]), 'score': float(scores[0][i]), 'source': 'bm25'}\n",
    "            for i in range(len(results[0]))\n",
    "        ]\n",
    "    \n",
    "    def _colbert_search(self, query: str, k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 2: ColBERT semantic search\"\"\"\n",
    "        results = self.indexer.colbert_retriever.search(query=query, k=k)\n",
    "        return [\n",
    "            {'corpus_index': r['document_id'], 'score': r['score'], 'source': 'colbert'}\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def _reciprocal_rank_fusion(\n",
    "        self, \n",
    "        bm25_results: List[Dict], \n",
    "        colbert_results: List[Dict],\n",
    "        k: int = 60\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"RRF fusion\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for rank, result in enumerate(bm25_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        for rank, result in enumerate(colbert_results, 1):\n",
    "            corpus_idx = result['corpus_index']\n",
    "            scores[corpus_idx] = scores.get(corpus_idx, 0) + (1 / (k + rank))\n",
    "        \n",
    "        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [{'corpus_index': idx, 'rrf_score': score} for idx, score in sorted_results]\n",
    "    \n",
    "    def _fetch_chunks_from_db(self, corpus_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Fetch chunks from database using corpus index -> chunk ID mapping\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for corpus_idx in corpus_indices:\n",
    "            # Convert corpus index to database chunk ID\n",
    "            if corpus_idx < len(self.corpus_to_chunk_id):\n",
    "                chunk_id = self.corpus_to_chunk_id[corpus_idx]\n",
    "                \n",
    "                # Fetch from database using the actual chunk ID\n",
    "                chunk = self.db_session.query(Chunk).filter_by(id=chunk_id).first()\n",
    "                if chunk:\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk.id,\n",
    "                        'text': chunk.text,\n",
    "                        'document_id': chunk.document_id,\n",
    "                        'heading_path': chunk.heading_path,\n",
    "                        'has_images': chunk.has_images,\n",
    "                        'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Chunk ID {chunk_id} not found in database\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Corpus index {corpus_idx} out of range (max: {len(self.corpus_to_chunk_id)-1})\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _colbert_rerank(self, query: str, chunks: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Stage 3: ColBERT reranking with score preservation\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        reranked_results = self.indexer.colbert_retriever.rerank(query=query, documents=documents, k=top_k)\n",
    "        \n",
    "        final_results = []\n",
    "        for result in reranked_results:\n",
    "            original_chunk = chunks[result['result_index']]\n",
    "            intermediate_scores = original_chunk.get('intermediate_scores', {})\n",
    "            \n",
    "            final_results.append({\n",
    "                'chunk_id': original_chunk['chunk_id'],\n",
    "                'text': original_chunk['text'],\n",
    "                'document_id': original_chunk['document_id'],\n",
    "                'heading_path': original_chunk.get('heading_path', ''),\n",
    "                'has_images': original_chunk.get('has_images', False),\n",
    "                'metadata': original_chunk['metadata'],\n",
    "                'score': result['score'],  # Final ColBERT rerank score (cosine similarity)\n",
    "                'rank': result['rank'],\n",
    "                'bm25_score': intermediate_scores.get('bm25_score', 0.0),\n",
    "                'colbert_score': intermediate_scores.get('colbert_score', 0.0),\n",
    "                'rrf_score': intermediate_scores.get('rrf_score', 0.0)\n",
    "            })\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176a5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAG CHATBOT WITH STREAMING\n",
    "# ============================================================================\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Complete RAG chatbot with Ollama and streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, retriever: HybridRetriever, ollama_client: OllamaClient):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.ollama = ollama_client\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, query: str, stream: bool = True) -> Dict:\n",
    "        \"\"\"Process user query and generate response with streaming\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        retrieved_chunks = self.retriever.retrieve(query)\n",
    "        \n",
    "        # Build context\n",
    "        context = self._build_context(retrieved_chunks)\n",
    "        \n",
    "        # Generate response with streaming\n",
    "        if stream:\n",
    "            print(f\"\\n🤖 Generating response (streaming)...\\n\")\n",
    "        else:\n",
    "            print(f\"\\n🤖 Generating response...\", end=' ')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        })\n",
    "        \n",
    "        response = self.ollama.chat(\n",
    "            messages=self.conversation_history,\n",
    "            context=context,\n",
    "            stream=stream\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if not stream:\n",
    "            print(f\"✓ {elapsed:.1f}s\")\n",
    "        else:\n",
    "            print(f\"\\n⏱️  Response generated in {elapsed:.1f}s\")\n",
    "        \n",
    "        self.conversation_history.append({\n",
    "            'role': 'assistant',\n",
    "            'content': response\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'sources': self._format_sources(retrieved_chunks),\n",
    "            'retrieved_chunks': len(retrieved_chunks)\n",
    "        }\n",
    "    \n",
    "    def _build_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"Build context from retrieved chunks\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            heading = f\" ({chunk['heading_path']})\" if chunk.get('heading_path') else \"\"\n",
    "            \n",
    "            # Add image info if present\n",
    "            image_info = \"\"\n",
    "            if chunk.get('has_images') and chunk.get('metadata', {}).get('image_paths'):\n",
    "                num_images = len(chunk['metadata']['image_paths'])\n",
    "                image_info = f\" [Contains {num_images} image(s)]\"\n",
    "            \n",
    "            context_parts.append(f\"[Source {i}{heading}{image_info}]\\n{chunk['text']}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _format_sources(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format source citations with full text, image paths, and ALL scores\"\"\"\n",
    "        sources = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            source = {\n",
    "                'source_id': i + 1,\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'document_id': chunk['document_id'],\n",
    "                'heading': chunk.get('heading_path', ''),\n",
    "                'score': chunk['score'],  # Final ColBERT rerank score\n",
    "                'bm25_score': chunk.get('bm25_score', 0.0),\n",
    "                'colbert_score': chunk.get('colbert_score', 0.0),\n",
    "                'rrf_score': chunk.get('rrf_score', 0.0),\n",
    "                'has_images': chunk.get('has_images', False),\n",
    "                'text': chunk['text'],  # Include full text\n",
    "                'preview': chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "            }\n",
    "            \n",
    "            # Add image paths if available\n",
    "            if chunk.get('has_images') and chunk.get('metadata'):\n",
    "                image_paths = chunk['metadata'].get('image_paths', [])\n",
    "                source['image_paths'] = image_paths\n",
    "            \n",
    "            sources.append(source)\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"🗑️  Conversation history cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGApplication:\n",
    "    \"\"\"Main application orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Database setup\n",
    "        db_url = f\"sqlite:///{config.db_path}\"\n",
    "        self.engine = create_engine(db_url)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.db_session = Session()\n",
    "        \n",
    "        # Initialize Ollama client\n",
    "        self.ollama = OllamaClient(config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.processor = DocumentProcessor(config, self.ollama)\n",
    "        self.indexer = DualIndexer(config)\n",
    "        self.retriever = None\n",
    "        self.chatbot = None\n",
    "        \n",
    "        # CRITICAL: Store mapping between corpus index and chunk IDs\n",
    "        self.corpus_to_chunk_id = []  # Maps corpus index -> database chunk ID\n",
    "    \n",
    "    def check_ollama(self) -> bool:\n",
    "        \"\"\"Check if Ollama is running\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.config.ollama_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def index_documents(self, pdf_paths: List[str]) -> None:\n",
    "        \"\"\"Index PDF documents\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"❌ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            chunks, doc_id = self.processor.process_document(pdf_path, self.db_session)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Building Indexes\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Build corpus and mapping\n",
    "        # CRITICAL FIX: Store the mapping between corpus index and database chunk IDs\n",
    "        all_db_chunks = self.db_session.query(Chunk).order_by(Chunk.id).all()\n",
    "        corpus = []\n",
    "        self.corpus_to_chunk_id = []\n",
    "        \n",
    "        for chunk in all_db_chunks:\n",
    "            corpus.append(chunk.text)\n",
    "            self.corpus_to_chunk_id.append(chunk.id)\n",
    "        \n",
    "        print(f\"  • Corpus: {len(corpus)} chunks\")\n",
    "        print(f\"  • Chunk ID mapping: {len(self.corpus_to_chunk_id)} entries\")\n",
    "        \n",
    "        # Build indexes\n",
    "        self.indexer.build_bm25_index(corpus)\n",
    "        self.indexer.build_colbert_index(corpus)\n",
    "        \n",
    "        # Save the mapping to disk for later use\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        os.makedirs(os.path.dirname(mapping_path), exist_ok=True)\n",
    "        with open(mapping_path, 'wb') as f:\n",
    "            pickle.dump(self.corpus_to_chunk_id, f)\n",
    "        \n",
    "        print(f\"\\n✅ Document indexed successfully!\")\n",
    "    \n",
    "    def initialize_chatbot(self) -> None:\n",
    "        \"\"\"Initialize chatbot with existing indexes\"\"\"\n",
    "        \n",
    "        if not self.check_ollama():\n",
    "            print(\"❌ Ollama is not running!\")\n",
    "            print(\"Please start Ollama: ollama serve\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading indexes...\")\n",
    "        self.indexer.load_indexes()\n",
    "        \n",
    "        # Load the corpus-to-chunk-id mapping\n",
    "        import pickle\n",
    "        mapping_path = os.path.join(self.config.base_dir, \"indexes\", \"corpus_mapping.pkl\")\n",
    "        try:\n",
    "            with open(mapping_path, 'rb') as f:\n",
    "                self.corpus_to_chunk_id = pickle.load(f)\n",
    "            print(f\"  • Loaded {len(self.corpus_to_chunk_id)} chunk ID mappings\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"  ⚠️  Warning: No corpus mapping found. Please re-index your documents.\")\n",
    "            self.corpus_to_chunk_id = []\n",
    "        \n",
    "        self.retriever = HybridRetriever(self.config, self.indexer, self.db_session, self.corpus_to_chunk_id)\n",
    "        self.chatbot = RAGChatbot(self.config, self.retriever, self.ollama)\n",
    "        \n",
    "        print(\"✅ Chatbot initialized and ready!\")\n",
    "    \n",
    "    def chat(self, query: str) -> Dict:\n",
    "        \"\"\"Chat interface\"\"\"\n",
    "        if not self.chatbot:\n",
    "            raise RuntimeError(\"Chatbot not initialized. Call initialize_chatbot() first.\")\n",
    "        \n",
    "        return self.chatbot.chat(query)\n",
    "    \n",
    "    def _filter_relevant_images(self, query: str, image_paths: List[str], chunk_text: str) -> List[str]:\n",
    "        \"\"\"Filter images to only show those DIRECTLY relevant to the user's query - STRICT filtering\"\"\"\n",
    "        if not image_paths:\n",
    "            return []\n",
    "        \n",
    "        relevant_images = []\n",
    "        \n",
    "        # Extract meaningful query keywords (remove stop words)\n",
    "        stop_words = {'what', 'is', 'are', 'the', 'a', 'an', 'how', 'why', 'when', 'where', \n",
    "                      'can', 'could', 'would', 'should', 'do', 'does', 'did', 'of', 'in', 'on',\n",
    "                      'for', 'to', 'with', 'by', 'from', 'at', 'about', 'as', 'into', 'through',\n",
    "                      'diagram', 'chart', 'figure', 'image', 'screenshot', 'show', 'me', 'please'}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        query_words = [w for w in query_lower.split() if w not in stop_words and len(w) > 2]\n",
    "        \n",
    "        if not query_words:\n",
    "            return []  # No meaningful query words, don't show images\n",
    "        \n",
    "        # Get image metadata from database\n",
    "        for img_path in image_paths:\n",
    "            # Extract just the filename for DB lookup\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            \n",
    "            # Look up image in database to get description\n",
    "            img_record = self.db_session.query(Image).filter(\n",
    "                Image.image_path.like(f\"%{img_filename}\")\n",
    "            ).first()\n",
    "            \n",
    "            if img_record:\n",
    "                # Combine all image metadata\n",
    "                desc_lower = (img_record.description or \"\").lower()\n",
    "                img_type_lower = (img_record.image_type or \"\").lower()\n",
    "                ocr_lower = (img_record.ocr_text or \"\").lower()\n",
    "                \n",
    "                # Create searchable text from image\n",
    "                image_text = f\"{desc_lower} {img_type_lower} {ocr_lower}\"\n",
    "                image_words = [w for w in image_text.split() if w not in stop_words and len(w) > 2]\n",
    "                \n",
    "                # Calculate meaningful overlap\n",
    "                query_set = set(query_words)\n",
    "                image_set = set(image_words)\n",
    "                overlap = query_set.intersection(image_set)\n",
    "                \n",
    "                # STRICT CRITERIA: Need at least 3 meaningful word overlaps\n",
    "                # This ensures the image is actually about what the user asked\n",
    "                if len(overlap) >= 3:\n",
    "                    relevant_images.append(img_path)\n",
    "                    # print(f\"  DEBUG: Image matched with {len(overlap)} overlaps: {overlap}\")\n",
    "        \n",
    "        return relevant_images\n",
    "    \n",
    "    def _display_chunk_with_images(self, chunk_text: str, image_paths: List[str] = None) -> None:\n",
    "        \"\"\"Display chunk text and associated images\"\"\"\n",
    "        from IPython.display import display, Image as IPImage\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"{chunk_text}\\n\")\n",
    "        \n",
    "        # Display images if available\n",
    "        if image_paths:\n",
    "            print(f\"  📷 Relevant Images ({len(image_paths)}):\")\n",
    "            for img_path in image_paths:\n",
    "                if os.path.exists(img_path):\n",
    "                    try:\n",
    "                        display(IPImage(filename=img_path, width=400))\n",
    "                        print(f\"  └─ {os.path.basename(img_path)}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  └─ ⚠️ Could not display {os.path.basename(img_path)}: {e}\\n\")\n",
    "                else:\n",
    "                    print(f\"  └─ ⚠️ Image not found: {os.path.basename(img_path)}\\n\")\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RAG Chatbot - Interactive Mode\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Type your questions (or 'exit' to quit, 'clear' to clear history)\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit']:\n",
    "                    print(\"\\nGoodbye! 👋\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'clear':\n",
    "                    self.chatbot.clear_history()\n",
    "                    continue\n",
    "                \n",
    "                result = self.chat(user_input)\n",
    "                print(f\"\\nAssistant: {result['response']}\\n\")\n",
    "                \n",
    "                # Show retrieved chunks with ALL SCORES\n",
    "                if result['sources']:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"📊 Retrieved Chunks with Similarity Scores ({len(result['sources'])})\")\n",
    "                    print(f\"{'='*60}\\n\")\n",
    "                    \n",
    "                    for idx, src in enumerate(result['sources'], 1):\n",
    "                        print(f\"┌─ Chunk {idx} {'─'*50}\")\n",
    "                        \n",
    "                        # Show ALL retrieval scores\n",
    "                        print(f\"│ 🎯 Final Score (ColBERT Rerank): {src['score']:.4f}\")\n",
    "                        print(f\"│ 📈 Intermediate Scores:\")\n",
    "                        print(f\"│    • BM25 (lexical):      {src.get('bm25_score', 0.0):.4f}\")\n",
    "                        print(f\"│    • ColBERT (semantic):  {src.get('colbert_score', 0.0):.4f}\")\n",
    "                        print(f\"│    • RRF (fusion):        {src.get('rrf_score', 0.0):.4f}\")\n",
    "                        \n",
    "                        if src['heading']:\n",
    "                            print(f\"│ 📍 Section: {src['heading']}\")\n",
    "                        \n",
    "                        if src['has_images']:\n",
    "                            print(f\"│ 🖼️  Contains Images: Yes\")\n",
    "                        \n",
    "                        print(f\"│\")\n",
    "                        print(f\"│ 📄 Text:\")\n",
    "                        \n",
    "                        # Display chunk text (show first 300 chars as preview)\n",
    "                        chunk_text = src.get('text', src.get('preview', ''))\n",
    "                        \n",
    "                        # Show preview\n",
    "                        if len(chunk_text) > 300:\n",
    "                            print(f\"│ {chunk_text[:300]}...\")\n",
    "                            print(f\"│ [Truncated - {len(chunk_text)} total characters]\")\n",
    "                        else:\n",
    "                            print(f\"│ {chunk_text}\")\n",
    "                        \n",
    "                        # Filter and display only STRICTLY RELEVANT images\n",
    "                        if src['has_images'] and src.get('image_paths'):\n",
    "                            # Filter images based on query relevance with STRICT criteria\n",
    "                            relevant_images = self._filter_relevant_images(\n",
    "                                user_input, \n",
    "                                src['image_paths'], \n",
    "                                chunk_text\n",
    "                            )\n",
    "                            \n",
    "                            if relevant_images:\n",
    "                                print(f\"│\")\n",
    "                                print(f\"│ [Showing {len(relevant_images)}/{len(src['image_paths'])} images matching your query]\")\n",
    "                                self._display_chunk_with_images(\"\", relevant_images)\n",
    "                            else:\n",
    "                                print(f\"│\")\n",
    "                                print(f\"│ [This chunk has images, but none directly match your specific query]\")\n",
    "                        \n",
    "                        print(f\"└{'─'*60}\\n\")\n",
    "                    \n",
    "                    print()\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! 👋\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error: {e}\\n\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"Print database statistics\"\"\"\n",
    "        doc_count = self.db_session.query(Document).count()\n",
    "        chunk_count = self.db_session.query(Chunk).count()\n",
    "        image_count = self.db_session.query(Image).count()\n",
    "        \n",
    "        print(f\"\\n📊 Database Statistics:\")\n",
    "        print(f\"   • Documents: {doc_count}\")\n",
    "        print(f\"   • Chunks: {chunk_count}\")\n",
    "        print(f\"   • Images: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c75a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name jinaai/jina-colbert-v2. Creating a new one with mean pooling.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RAG Chatbot - Choose an option:\n",
      "1. Upload and index a PDF\n",
      "2. Start interactive chat\n",
      "3. Show database statistics\n",
      "4. Exit\n",
      "Loading indexes...\n",
      "  • Loaded 3 chunk ID mappings\n",
      "✅ Chatbot initialized and ready!\n",
      "\n",
      "============================================================\n",
      "RAG Chatbot - Interactive Mode\n",
      "============================================================\n",
      "Type your questions (or 'exit' to quit, 'clear' to clear history)\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 3, using k=3 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd79e4fd9a44b50b1f04b17c758edb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4d65882f8f449f94bf4a226deff620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290bed8176dd43bf868299e905214f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.089s (3 results)\n",
      "   • ColBERT: 0.452s (3 results)\n",
      "   • Fusion: 0.000s (3 candidates)\n",
      "   • Fetch: 0.004s (3 chunks)\n",
      "   • Rerank: 0.303s (top 3)\n",
      "   ✓ Total retrieval: 0.848s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "According to Source 1 (**What is Claude Skills** > How Claude Skills Can Accelerate Our AI Development Work), Claude Skills can accelerate your AI development work by:\n",
      "\n",
      "* Creating **reusable Skills** for common development scenarios, such as:\n",
      "\t+ \"AI-Systems-Standards\" Skill (Python coding conventions, preferred libraries and frameworks, testing and evaluation patterns, deployment standards)\n",
      "\t+ \"KAI-Project\" Skill (KAI's architecture and components, retrieval strategies, integration requirements with existing systems, performance benchmarks and evaluation metrics)\n",
      "\t+ \"GenAI-Best-Practices\" Skill (approach to prompt engineering, RAG implementation patterns, model evaluation frameworks, security and data privacy guidelines)\n",
      "* Reducing **time overhead** by eliminating the need for repetitive explanation of context, allowing engineers to spend more time on actual development work\n",
      "* Delivering **more immediately usable solutions**, as Claude can automatically load relevant skills and provide pre-constructed code snippets tailored to your specific setup\n",
      "* Providing **consistency** across the team, ensuring that all engineers get Claude's help based on the same standards and practices\n",
      "* Enhancing **knowledge preservation** by documenting institutional knowledge about AI system architecture in reusable Skills\n",
      "* Improving **code quality**, reducing bugs and technical debt through pre-constructed code snippets aligned with your company's best practices\n",
      "\n",
      "By leveraging Claude Skills, you can accelerate your AI development work, saving up to 60-150 hours of development time per quarter for a 5-person team.\n",
      "\n",
      "⏱️  Response generated in 15.9s\n",
      "\n",
      "Assistant: According to Source 1 (**What is Claude Skills** > How Claude Skills Can Accelerate Our AI Development Work), Claude Skills can accelerate your AI development work by:\n",
      "\n",
      "* Creating **reusable Skills** for common development scenarios, such as:\n",
      "\t+ \"AI-Systems-Standards\" Skill (Python coding conventions, preferred libraries and frameworks, testing and evaluation patterns, deployment standards)\n",
      "\t+ \"KAI-Project\" Skill (KAI's architecture and components, retrieval strategies, integration requirements with existing systems, performance benchmarks and evaluation metrics)\n",
      "\t+ \"GenAI-Best-Practices\" Skill (approach to prompt engineering, RAG implementation patterns, model evaluation frameworks, security and data privacy guidelines)\n",
      "* Reducing **time overhead** by eliminating the need for repetitive explanation of context, allowing engineers to spend more time on actual development work\n",
      "* Delivering **more immediately usable solutions**, as Claude can automatically load relevant skills and provide pre-constructed code snippets tailored to your specific setup\n",
      "* Providing **consistency** across the team, ensuring that all engineers get Claude's help based on the same standards and practices\n",
      "* Enhancing **knowledge preservation** by documenting institutional knowledge about AI system architecture in reusable Skills\n",
      "* Improving **code quality**, reducing bugs and technical debt through pre-constructed code snippets aligned with your company's best practices\n",
      "\n",
      "By leveraging Claude Skills, you can accelerate your AI development work, saving up to 60-150 hours of development time per quarter for a 5-person team.\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (3)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5968\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      2.1805\n",
      "│    • ColBERT (semantic):  0.5968\n",
      "│    • RRF (fusion):        0.0328\n",
      "│ 📍 Section: **What is Claude Skills** > How Claude Skills Can Accelerate Our AI Development Work\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## How Claude Skills Can Accelerate Our AI Development Work\n",
      "\n",
      "**The Current Situation:**\n",
      "\n",
      "\n",
      "When our AI engineering team uses Claude for development help, we typically spend time explaining:\n",
      "\n",
      "\n",
      "Our tech stack and architecture choices\n",
      "\n",
      "\n",
      "Company coding standards and ...\n",
      "│ [Truncated - 3575 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5855\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.8142\n",
      "│    • ColBERT (semantic):  0.5855\n",
      "│    • RRF (fusion):        0.0320\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **What is Claude Skills**\n",
      "\n",
      "Claude Skills are folders **containing instructions, scripts, and resources** t **hat Claude can load when needed to**\n",
      "\n",
      "**improve performance on specific tasks.** Think of them as **reusable knowledge packages** that transform Claude from\n",
      "\n",
      "\n",
      "a general-purpose assistant in...\n",
      "│ [Truncated - 2579 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5720\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.2068\n",
      "│    • ColBERT (semantic):  0.5720\n",
      "│    • RRF (fusion):        0.0320\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## Additional key highlights from the VentureBeat article about **Claude Skills:**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **What is Claude Skills** > Additional key highlights from the VentureBeat article about **Claude Skills:**]\n",
      "\n",
      "### **Additional Highlights: How Skills Make Claude Fast...\n",
      "│ [Truncated - 3990 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 3, using k=3 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d306921e58a49a09d5f28a341fe8b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a60211af8f433f9af89cec5a94a2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc17dbfe4a644d14bac76c8f313b3d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.013s (3 results)\n",
      "   • ColBERT: 0.392s (3 results)\n",
      "   • Fusion: 0.000s (3 candidates)\n",
      "   • Fetch: 0.001s (3 chunks)\n",
      "   • Rerank: 0.290s (top 3)\n",
      "   ✓ Total retrieval: 0.696s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "According to Source 1 (**What is Claude Skills** > Key Technical Architecture), the key technical architecture of Claude Skills involves:\n",
      "\n",
      "* **Progressive Disclosure**: At startup, Claude only loads the name and description of each skill (just metadata), then dynamically loads full details only when relevant to the task. This keeps things token-efficient while maintaining access to deep expertise.\n",
      "\n",
      "Source: Source 1\n",
      "\n",
      "⏱️  Response generated in 4.0s\n",
      "\n",
      "Assistant: According to Source 1 (**What is Claude Skills** > Key Technical Architecture), the key technical architecture of Claude Skills involves:\n",
      "\n",
      "* **Progressive Disclosure**: At startup, Claude only loads the name and description of each skill (just metadata), then dynamically loads full details only when relevant to the task. This keeps things token-efficient while maintaining access to deep expertise.\n",
      "\n",
      "Source: Source 1\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (3)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.6265\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.8761\n",
      "│    • ColBERT (semantic):  0.6265\n",
      "│    • RRF (fusion):        0.0328\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **What is Claude Skills**\n",
      "\n",
      "Claude Skills are folders **containing instructions, scripts, and resources** t **hat Claude can load when needed to**\n",
      "\n",
      "**improve performance on specific tasks.** Think of them as **reusable knowledge packages** that transform Claude from\n",
      "\n",
      "\n",
      "a general-purpose assistant in...\n",
      "│ [Truncated - 2579 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5912\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.0245\n",
      "│    • ColBERT (semantic):  0.5912\n",
      "│    • RRF (fusion):        0.0320\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## Additional key highlights from the VentureBeat article about **Claude Skills:**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **What is Claude Skills** > Additional key highlights from the VentureBeat article about **Claude Skills:**]\n",
      "\n",
      "### **Additional Highlights: How Skills Make Claude Fast...\n",
      "│ [Truncated - 3990 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.5667\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      1.2815\n",
      "│    • ColBERT (semantic):  0.5667\n",
      "│    • RRF (fusion):        0.0320\n",
      "│ 📍 Section: **What is Claude Skills** > How Claude Skills Can Accelerate Our AI Development Work\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## How Claude Skills Can Accelerate Our AI Development Work\n",
      "\n",
      "**The Current Situation:**\n",
      "\n",
      "\n",
      "When our AI engineering team uses Claude for development help, we typically spend time explaining:\n",
      "\n",
      "\n",
      "Our tech stack and architecture choices\n",
      "\n",
      "\n",
      "Company coding standards and ...\n",
      "│ [Truncated - 3575 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\n",
      "🔍 Retrieving relevant chunks...\n",
      "   • Corpus size: 3, using k=3 for retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c6f04d888749488ffb17309d47322d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dd9a3fd1fd471ab1c41c231a316857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c417aca35a2475ca9923a5d96d8279a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   • BM25s: 0.012s (3 results)\n",
      "   • ColBERT: 0.089s (3 results)\n",
      "   • Fusion: 0.000s (3 candidates)\n",
      "   • Fetch: 0.001s (3 chunks)\n",
      "   • Rerank: 0.301s (top 3)\n",
      "   ✓ Total retrieval: 0.402s\n",
      "\n",
      "🤖 Generating response (streaming)...\n",
      "\n",
      "I don't have that information in the provided documents.\n",
      "\n",
      "⏱️  Response generated in 3.1s\n",
      "\n",
      "Assistant: I don't have that information in the provided documents.\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Retrieved Chunks with Similarity Scores (3)\n",
      "============================================================\n",
      "\n",
      "┌─ Chunk 1 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2319\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.0000\n",
      "│    • ColBERT (semantic):  0.2319\n",
      "│    • RRF (fusion):        0.0323\n",
      "│ 📍 Section: **What is Claude Skills** > How Claude Skills Can Accelerate Our AI Development Work\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## How Claude Skills Can Accelerate Our AI Development Work\n",
      "\n",
      "**The Current Situation:**\n",
      "\n",
      "\n",
      "When our AI engineering team uses Claude for development help, we typically spend time explaining:\n",
      "\n",
      "\n",
      "Our tech stack and architecture choices\n",
      "\n",
      "\n",
      "Company coding standards and ...\n",
      "│ [Truncated - 3575 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 2 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2238\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.0000\n",
      "│    • ColBERT (semantic):  0.2238\n",
      "│    • RRF (fusion):        0.0323\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ [Context: **What is Claude Skills**]\n",
      "\n",
      "## Additional key highlights from the VentureBeat article about **Claude Skills:**\n",
      "\n",
      "\n",
      "\n",
      "[Context: **What is Claude Skills** > Additional key highlights from the VentureBeat article about **Claude Skills:**]\n",
      "\n",
      "### **Additional Highlights: How Skills Make Claude Fast...\n",
      "│ [Truncated - 3990 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "┌─ Chunk 3 ──────────────────────────────────────────────────\n",
      "│ 🎯 Final Score (ColBERT Rerank): 0.2226\n",
      "│ 📈 Intermediate Scores:\n",
      "│    • BM25 (lexical):      0.0000\n",
      "│    • ColBERT (semantic):  0.2226\n",
      "│    • RRF (fusion):        0.0323\n",
      "│ 📍 Section: **What is Claude Skills**\n",
      "│\n",
      "│ 📄 Text:\n",
      "│ # **What is Claude Skills**\n",
      "\n",
      "Claude Skills are folders **containing instructions, scripts, and resources** t **hat Claude can load when needed to**\n",
      "\n",
      "**improve performance on specific tasks.** Think of them as **reusable knowledge packages** that transform Claude from\n",
      "\n",
      "\n",
      "a general-purpose assistant in...\n",
      "│ [Truncated - 2579 total characters]\n",
      "└────────────────────────────────────────────────────────────\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize config with SEPARATE models for vision and chat\n",
    "# Vision: gemma3:4b (multimodal, for analyzing images)\n",
    "# Chat: gemma3:4b (FASTER - recommended for 16GB RAM Mac Mini M4)\n",
    "# Note: gpt-oss:20b is available but VERY slow. Use only if you need maximum quality.\n",
    "config = RAGConfig(chat_model='llama3.2:3b')  # Changed from gpt-oss:20b to gemma3:4b for better performance\n",
    "app = RAGApplication(config)\n",
    "\n",
    "# Check Ollama\n",
    "if not app.check_ollama():\n",
    "    print(\"❌ Ollama is not running!\")\n",
    "    print(\"\\nTo start Ollama:\")\n",
    "    print(\"  1. Open a terminal\")\n",
    "    print(\"  2. Run: ollama serve\")\n",
    "    print(\"  3. Keep that terminal open\")\n",
    "    print(\"\\nThen run this cell again.\")\n",
    "else:\n",
    "    # Simple menu with proper exit handling\n",
    "    exit_program = False\n",
    "    \n",
    "    while not exit_program:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAG Chatbot - Choose an option:\")\n",
    "        print(\"1. Upload and index a PDF\")\n",
    "        print(\"2. Start interactive chat\")\n",
    "        print(\"3. Show database statistics\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            file_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "            if os.path.exists(file_path):\n",
    "                app.index_documents([file_path])\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                \n",
    "        elif choice == '2':\n",
    "            app.initialize_chatbot()\n",
    "            app.interactive_chat()\n",
    "            # Back to main menu after chat exits\n",
    "            print(\"\\n[Returned to main menu]\")\n",
    "            \n",
    "        elif choice == '3':\n",
    "            app.print_stats()\n",
    "            \n",
    "        elif choice == '4':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Goodbye! 👋\")\n",
    "            print(\"=\"*50)\n",
    "            exit_program = True\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1-4.\")\n",
    "    \n",
    "    print(\"\\n✅ Program exited successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3caa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
