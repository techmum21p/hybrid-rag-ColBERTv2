# Local RAG Setup for Mac Mini M4 - Files Summary

## üéØ What Changed?

Based on your previous chat thread, you want a **local setup on Mac Mini M4 with Ollama** using:
- ‚úÖ **PyMuPDF4LLM** for PDF extraction (better than MarkItDown)
- ‚úÖ **PyMuPDF** for image extraction
- ‚úÖ **Gemma2 Vision (27B)** for multimodal understanding
- ‚úÖ **Gemma2 (2B)** for text-only responses

---

## üìÅ File Status

### ‚úÖ **KEEP AS-IS** (No changes needed - works for local setup):

1. **`markdown_chunking_strategy.py`** (19KB)
   - Markdown-aware semantic chunking
   - Works perfectly for local setup
   - No cloud dependencies

2. **`model_downloader.py`** (14KB)
   - Downloads ColBERT and tokenizer
   - Still useful for local setup
   - Caches models in `~/.cache/rag_models`

### üîÑ **UPDATED** (New local versions):

1. **`local_rag_app.py`** ‚≠ê **NEW** (58KB)
   - Complete local RAG implementation
   - Uses Ollama instead of Gemini
   - SQLite instead of Cloud SQL
   - Local file storage instead of GCS
   - Detailed timing metrics
   - Mac M4 optimized

2. **`requirements_local.txt`** ‚≠ê **NEW**
   - Simplified dependencies
   - No GCP packages
   - Only local processing libraries

### ‚ùå **IGNORE** (Cloud-specific - not needed for local setup):

1. ~~`rag_app_complete.py`~~ - GCP/Gemini version
2. ~~`requirements.txt`~~ - Cloud dependencies
3. ~~`README.md`~~ - Cloud setup docs
4. ~~`quickstart.py`~~ - Cloud setup helper
5. ~~All other .md files~~ - Cloud-specific docs

---

## üöÄ Quick Start (Local Setup)

### 1. Install Ollama

```bash
# Install Ollama (Mac)
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama server
ollama serve

# Pull Gemma2 models (in another terminal)
ollama pull gemma2:2b          # For text responses (fast, 2GB)
ollama pull gemma2:27b-vision  # For vision + text (slower, 16GB)
```

### 2. Install Python Dependencies

```bash
# Install dependencies
pip install -r requirements_local.txt

# Optional: Download models ahead of time
python model_downloader.py --download-all
```

### 3. Upload and Index a PDF

```bash
python local_rag_app.py --upload your_document.pdf
```

**You'll see:**
```
Processing: your_document.pdf
[Step 1/3] Converting PDF to Markdown...
  ‚úì Completed in 2.34s
  ‚Ä¢ Extracted 125,432 characters

[Step 2/3] Markdown-aware semantic chunking...
  ‚úì Completed in 0.18s
  ‚Ä¢ Created 145 semantic chunks
  ‚Ä¢ Average chunk size: 512 tokens

[Step 3/3] Saving chunks to database...
  ‚úì Completed in 0.05s

[BM25s] Building lexical search index...
  ‚úì Completed in 0.42s
  ‚Ä¢ Indexed 145 chunks

[ColBERT] Building semantic search index...
  ‚úì Completed in 8.23s
  ‚Ä¢ Indexed 145 chunks

‚úÖ Document indexed successfully!
```

### 4. Chat with Your Documents

```bash
# Interactive chat
python local_rag_app.py --chat

# Single query
python local_rag_app.py --query "How do I configure authentication?"
```

**You'll see timing breakdown:**
```
üîç Retrieving relevant chunks...
   ‚Ä¢ BM25s: 0.023s
   ‚Ä¢ ColBERT: 0.156s
   ‚Ä¢ Fusion: 0.001s
   ‚Ä¢ Fetch: 0.003s
   ‚Ä¢ Rerank: 0.089s
   ‚úì Total retrieval: 0.272s
   ‚Ä¢ Retrieved 10 chunks

ü§ñ Generating response with Ollama (llama3.2:3b)...
   ‚úì Generated in 1.847s

‚è±Ô∏è  Total query time: 2.119s
   ‚Ä¢ Retrieval: 0.272s (12.8%)
   ‚Ä¢ Generation: 1.847s (87.2%)
```

---

## üîß What's Different in Local Version?

### Architecture Comparison

| Component | Cloud Version | Local Version |
|-----------|---------------|---------------|
| **PDF Processing** | MarkItDown | **PyMuPDF4LLM** ‚≠ê |
| **Image Extraction** | None | **PyMuPDF** ‚≠ê |
| **Chunking** | Markdown-aware | Markdown-aware ‚úÖ |
| **Contextualization** | Gemini API ($$$) | None (not needed locally) |
| **Retrieval** | BM25s + ColBERT | BM25s + ColBERT ‚úÖ |
| **Reranking** | ColBERT | ColBERT ‚úÖ |
| **LLM Generation** | Gemini API ($$$) | **Gemma2 (Ollama, FREE!)** ‚≠ê |
| **Vision Support** | Limited | **Gemma2 Vision (27B)** ‚≠ê |
| **Database** | Cloud SQL (PostgreSQL) | **SQLite** ‚≠ê |
| **Storage** | Google Cloud Storage | **Local files** ‚≠ê |
| **Cost** | ~$0.0001-0.0003/query | **FREE!** ‚≠ê |

### Key Benefits of Local Setup

‚úÖ **Completely FREE** - No API costs
‚úÖ **Privacy** - All data stays on your Mac
‚úÖ **Fast** - No network latency
‚úÖ **Offline** - Works without internet
‚úÖ **M4 Optimized** - Takes advantage of Apple Silicon

---

## üìä Performance on Mac Mini M4

### Upload/Indexing (500-page PDF):
- PDF ‚Üí Markdown: ~3-5s
- Chunking: ~0.2-0.5s
- BM25s indexing: ~0.5-1s
- ColBERT indexing: ~10-15s
- **Total: ~15-20s**

### Query Performance:
- Retrieval (BM25s + ColBERT + Rerank): ~0.2-0.3s
- Generation (llama3.2:3b): ~1-3s
- **Total: ~1.5-3.5s per query**

---

## ü§î Which Models to Use?

### Recommended Ollama Models for Mac Mini M4:

**For Speed (Recommended):**
```bash
ollama pull llama3.2:1b   # Super fast, 1B parameters
ollama pull llama3.2:3b   # Fast, good quality, 3B parameters
```

**For Quality:**
```bash
ollama pull qwen2.5:7b    # Better quality, 7B parameters
ollama pull llama3.1:8b   # Good quality, 8B parameters
```

**For Best Quality (if you have 32GB+ RAM):**
```bash
ollama pull qwen2.5:14b   # High quality, 14B parameters
```

Change model in the app:
```bash
python local_rag_app.py --chat --model llama3.2:1b
python local_rag_app.py --chat --model qwen2.5:7b
```

---

## üí° Model Clarification (Same as Before)

**You asked:** "Why do we need an embedding model? We have ColBERT?"

**Answer: You're RIGHT!** We don't use a separate embedding model.

What `model_downloader.py` downloads:
1. **ColBERTv2** (~440MB) - Does ALL embeddings
2. **BERT tokenizer** (~440MB) - ONLY for counting tokens during chunking

The tokenizer is just for splitting text properly. ColBERT does everything else!

---

## üìù CLI Commands

```bash
# Upload and index a PDF
python local_rag_app.py --upload document.pdf

# Interactive chat
python local_rag_app.py --chat

# Single query with full metrics
python local_rag_app.py --query "What is the API endpoint?"

# Show database stats
python local_rag_app.py --stats

# Use different Ollama model
python local_rag_app.py --chat --model llama3.2:1b
```

---

## üóÇÔ∏è File Structure

```
your-project/
‚îú‚îÄ‚îÄ local_rag_app.py              # Main application ‚≠ê
‚îú‚îÄ‚îÄ markdown_chunking_strategy.py  # Chunking module (reusable)
‚îú‚îÄ‚îÄ model_downloader.py           # Download ColBERT/tokenizer
‚îú‚îÄ‚îÄ requirements_local.txt         # Dependencies ‚≠ê
‚îî‚îÄ‚îÄ rag_local/                    # Created automatically
    ‚îú‚îÄ‚îÄ rag_local.db              # SQLite database
    ‚îî‚îÄ‚îÄ indexes/                  # BM25s + ColBERT indexes
        ‚îú‚îÄ‚îÄ bm25s/
        ‚îî‚îÄ‚îÄ colbert/
```

---

## ‚úÖ Final Files You Need (Local Setup)

### Essential Files (3 files):

1. **`local_rag_app.py`** (58KB) - Main application
2. **`requirements_local.txt`** - Dependencies  
3. **`markdown_chunking_strategy.py`** (19KB) - Optional, if you want to customize chunking

### Optional:
4. **`model_downloader.py`** (14KB) - Pre-download models

---

## üéØ Summary

**What to use:**
- ‚úÖ `local_rag_app.py` - Your main app
- ‚úÖ `requirements_local.txt` - Install these
- ‚úÖ `markdown_chunking_strategy.py` - Already integrated in local_rag_app.py, keep for reference
- ‚úÖ `model_downloader.py` - Optional, for pre-downloading models

**What to ignore:**
- ‚ùå All the cloud-based files (rag_app_complete.py, etc.)
- ‚ùå Cloud documentation files

---

## üöÄ You're Ready!

1. Install Ollama: `ollama serve`
2. Pull a model: `ollama pull llama3.2:3b`
3. Install Python deps: `pip install -r requirements_local.txt`
4. Upload PDF: `python local_rag_app.py --upload your.pdf`
5. Chat: `python local_rag_app.py --chat`

Everything runs locally. FREE. Fast. Private. üéâ
